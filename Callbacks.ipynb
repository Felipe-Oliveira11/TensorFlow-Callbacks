{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Callbacks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNs4VS23hqN4Eknz8p7iM0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Felipe-Oliveira11/TensorFlow-Callbacks/blob/master/Callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt6xSe5w1uig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6d2e5be7-10b8-40d0-8df7-d4b285b7b296"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "import time \n",
        "import datetime\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM5UOy2S12E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjiXRVl-12bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ttkzuP813yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVpPMUX015lY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padronização \n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7lGzdXf1_Vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_value = 42"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WEUZ4Vv2HX6",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EqMaaNH2Fza",
        "colab_type": "text"
      },
      "source": [
        "### CallBacks\n",
        "\n",
        "Um retorno de chamada é uma ferramenta poderosa para personalizar o comportamento de um modelo Keras durante o treinamento, avaliação ou inferência.\n",
        "\n",
        "\n",
        "Os retornos de chamada são úteis para obter uma visão dos estados internos e das estatísticas do modelo durante o treinamento.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dNalWW2LpB",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Retornos de chamada personalizados subclassificando a classe de retorno de chamada.¶\n",
        "Esses retornos de chamada estão na classe base \"tf.keras.callbacks\". Ao subclassificar esses retornos de chamada, podemos executar determinadas funções quando o treinamento / lote / época foi iniciado ou encerrado.\n",
        "\n",
        "Para isso, podemos substituir a função de classes de retorno de chamada. O nome dessas funções é auto-explicar seu comportamento. Por exemplo, def on_train_begin (), isso significa o que fazer quando o treinamento começar. Vamos ver abaixo como substituir essas funções. Também podemos monitorar logs e executar determinadas ações, geralmente no início ou no final do treinamento / lote / épocas.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxZPc3ld2BMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz3kiVxb2Ok8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image shape\n",
        "image_rows = 28\n",
        "image_cols = 28\n",
        "image_shape = (image_rows, image_cols,1)\n",
        "\n",
        "\n",
        "# Reshape | (28,28,1)\n",
        "X_train = X_train.reshape(X_train.shape[0], *image_shape)\n",
        "X_test = X_test.reshape(X_test.shape[0], *image_shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NOLW7Pn2QfQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d22672ca-e5fb-452f-a029-8c0526acbbf9"
      },
      "source": [
        "#image shape\n",
        "X_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUp7E1qY2R7e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3a86982d-3c63-4148-d6c7-5f6e5ec2b828"
      },
      "source": [
        "# 1.criar a arquitetura\n",
        "\n",
        "def get_model():\n",
        "  # CNN \n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(image_shape)))\n",
        "  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME'))\n",
        "  model.add(Dropout(0.20))\n",
        "  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n",
        "  model.add(Dropout(0.30))\n",
        "  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n",
        "  model.add(MaxPool2D(2,2))\n",
        "  model.add(GlobalMaxPool2D())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # compile\n",
        "  model.compile(optimizer=Adam(0.01),\n",
        "                loss=SparseCategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model \n",
        "\n",
        "\n",
        "# instânciando CNN\n",
        "model = get_model()\n",
        "model.summary()  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 74,602\n",
            "Trainable params: 74,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHOZBzPu2UmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CustomCallback \n",
        "\n",
        "class CustomCallback(Callback):\n",
        "  def on_train_begin(self,logs=None):\n",
        "      print(\"Treino iniciado em, no tempo {}\".format(datetime.datetime.now().time()))\n",
        "  def on_train_end(self, logs=None):\n",
        "      print(\"Treino finalizado no tempo de {}\".format(datetime.datetime.now().time()))\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "      print('Treinamento: batch {} iniciou com {}'.format(batch, datetime.datetime.now().time()))\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "      print('Treinamento: batch {} terminou com {}'.format(batch, datetime.datetime.now().time()))\n",
        "\n",
        "\n",
        "custom_callback = CustomCallback()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VUUinIa2Wsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0a36588-5bfc-4185-f603-54868509a9b1"
      },
      "source": [
        "model = get_model()\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[custom_callback])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treino iniciado em, no tempo 12:03:27.473484\n",
            "Treinamento: batch 0 iniciou com 12:03:27.478311\n",
            "Treinamento: batch 0 terminou com 12:03:28.220783\n",
            "   1/1875 [..............................] - ETA: 0s - loss: 2.3017 - accuracy: 0.0938Treinamento: batch 1 iniciou com 12:03:28.222727\n",
            "Treinamento: batch 1 terminou com 12:03:28.361281\n",
            "   2/1875 [..............................] - ETA: 2:11 - loss: 2.6722 - accuracy: 0.1094Treinamento: batch 2 iniciou com 12:03:28.362618\n",
            "Treinamento: batch 2 terminou com 12:03:28.498739\n",
            "   3/1875 [..............................] - ETA: 2:53 - loss: 2.5485 - accuracy: 0.1146Treinamento: batch 3 iniciou com 12:03:28.500139\n",
            "Treinamento: batch 3 terminou com 12:03:28.641535\n",
            "   4/1875 [..............................] - ETA: 3:16 - loss: 2.4900 - accuracy: 0.1016Treinamento: batch 4 iniciou com 12:03:28.643231\n",
            "Treinamento: batch 4 terminou com 12:03:28.789147\n",
            "   5/1875 [..............................] - ETA: 3:32 - loss: 2.4513 - accuracy: 0.1187Treinamento: batch 5 iniciou com 12:03:28.791064\n",
            "Treinamento: batch 5 terminou com 12:03:28.936308\n",
            "   6/1875 [..............................] - ETA: 3:43 - loss: 2.4254 - accuracy: 0.1354Treinamento: batch 6 iniciou com 12:03:28.938358\n",
            "Treinamento: batch 6 terminou com 12:03:29.079519\n",
            "   7/1875 [..............................] - ETA: 3:49 - loss: 2.4073 - accuracy: 0.1384Treinamento: batch 7 iniciou com 12:03:29.081710\n",
            "Treinamento: batch 7 terminou com 12:03:29.230120\n",
            "   8/1875 [..............................] - ETA: 3:55 - loss: 2.3963 - accuracy: 0.1211Treinamento: batch 8 iniciou com 12:03:29.231876\n",
            "Treinamento: batch 8 terminou com 12:03:29.379028\n",
            "   9/1875 [..............................] - ETA: 4:00 - loss: 2.3870 - accuracy: 0.1146Treinamento: batch 9 iniciou com 12:03:29.380795\n",
            "Treinamento: batch 9 terminou com 12:03:29.526283\n",
            "  10/1875 [..............................] - ETA: 4:03 - loss: 2.3814 - accuracy: 0.1125Treinamento: batch 10 iniciou com 12:03:29.527965\n",
            "Treinamento: batch 10 terminou com 12:03:29.683450\n",
            "  11/1875 [..............................] - ETA: 4:07 - loss: 2.3737 - accuracy: 0.1108Treinamento: batch 11 iniciou com 12:03:29.685274\n",
            "Treinamento: batch 11 terminou com 12:03:29.828889\n",
            "  12/1875 [..............................] - ETA: 4:09 - loss: 2.3677 - accuracy: 0.1094Treinamento: batch 12 iniciou com 12:03:29.830709\n",
            "Treinamento: batch 12 terminou com 12:03:29.972310\n",
            "  13/1875 [..............................] - ETA: 4:10 - loss: 2.3641 - accuracy: 0.1058Treinamento: batch 13 iniciou com 12:03:29.974173\n",
            "Treinamento: batch 13 terminou com 12:03:30.128822\n",
            "  14/1875 [..............................] - ETA: 4:13 - loss: 2.3590 - accuracy: 0.1049Treinamento: batch 14 iniciou com 12:03:30.130497\n",
            "Treinamento: batch 14 terminou com 12:03:30.273210\n",
            "  15/1875 [..............................] - ETA: 4:14 - loss: 2.3556 - accuracy: 0.1063Treinamento: batch 15 iniciou com 12:03:30.275370\n",
            "Treinamento: batch 15 terminou com 12:03:30.423441\n",
            "  16/1875 [..............................] - ETA: 4:15 - loss: 2.3530 - accuracy: 0.1035Treinamento: batch 16 iniciou com 12:03:30.425390\n",
            "Treinamento: batch 16 terminou com 12:03:30.576291\n",
            "  17/1875 [..............................] - ETA: 4:17 - loss: 2.3483 - accuracy: 0.1048Treinamento: batch 17 iniciou com 12:03:30.580873\n",
            "Treinamento: batch 17 terminou com 12:03:30.746452\n",
            "  18/1875 [..............................] - ETA: 4:20 - loss: 2.3451 - accuracy: 0.1042Treinamento: batch 18 iniciou com 12:03:30.749130\n",
            "Treinamento: batch 18 terminou com 12:03:30.909523\n",
            "  19/1875 [..............................] - ETA: 4:22 - loss: 2.3398 - accuracy: 0.1069Treinamento: batch 19 iniciou com 12:03:30.911424\n",
            "Treinamento: batch 19 terminou com 12:03:31.056128\n",
            "  20/1875 [..............................] - ETA: 4:22 - loss: 2.3400 - accuracy: 0.1078Treinamento: batch 20 iniciou com 12:03:31.058002\n",
            "Treinamento: batch 20 terminou com 12:03:31.197838\n",
            "  21/1875 [..............................] - ETA: 4:22 - loss: 2.3350 - accuracy: 0.1071Treinamento: batch 21 iniciou com 12:03:31.199682\n",
            "Treinamento: batch 21 terminou com 12:03:31.345451\n",
            "  22/1875 [..............................] - ETA: 4:23 - loss: 2.3324 - accuracy: 0.1094Treinamento: batch 22 iniciou com 12:03:31.347361\n",
            "Treinamento: batch 22 terminou com 12:03:31.488675\n",
            "  23/1875 [..............................] - ETA: 4:23 - loss: 2.3269 - accuracy: 0.1087Treinamento: batch 23 iniciou com 12:03:31.490547\n",
            "Treinamento: batch 23 terminou com 12:03:31.644508\n",
            "  24/1875 [..............................] - ETA: 4:24 - loss: 2.3252 - accuracy: 0.1068Treinamento: batch 24 iniciou com 12:03:31.646250\n",
            "Treinamento: batch 24 terminou com 12:03:31.793605\n",
            "  25/1875 [..............................] - ETA: 4:24 - loss: 2.3215 - accuracy: 0.1063Treinamento: batch 25 iniciou com 12:03:31.795760\n",
            "Treinamento: batch 25 terminou com 12:03:31.944221\n",
            "  26/1875 [..............................] - ETA: 4:24 - loss: 2.3173 - accuracy: 0.1046Treinamento: batch 26 iniciou com 12:03:31.946619\n",
            "Treinamento: batch 26 terminou com 12:03:32.087543\n",
            "  27/1875 [..............................] - ETA: 4:24 - loss: 2.3133 - accuracy: 0.1019Treinamento: batch 27 iniciou com 12:03:32.089725\n",
            "Treinamento: batch 27 terminou com 12:03:32.243835\n",
            "  28/1875 [..............................] - ETA: 4:25 - loss: 2.3086 - accuracy: 0.0993Treinamento: batch 28 iniciou com 12:03:32.246241\n",
            "Treinamento: batch 28 terminou com 12:03:32.389310\n",
            "  29/1875 [..............................] - ETA: 4:25 - loss: 2.3045 - accuracy: 0.1002Treinamento: batch 29 iniciou com 12:03:32.391124\n",
            "Treinamento: batch 29 terminou com 12:03:32.535814\n",
            "  30/1875 [..............................] - ETA: 4:25 - loss: 2.2956 - accuracy: 0.1042Treinamento: batch 30 iniciou com 12:03:32.537562\n",
            "Treinamento: batch 30 terminou com 12:03:32.690020\n",
            "  31/1875 [..............................] - ETA: 4:25 - loss: 2.2845 - accuracy: 0.1069Treinamento: batch 31 iniciou com 12:03:32.691860\n",
            "Treinamento: batch 31 terminou com 12:03:32.831127\n",
            "  32/1875 [..............................] - ETA: 4:25 - loss: 2.2768 - accuracy: 0.1094Treinamento: batch 32 iniciou com 12:03:32.832838\n",
            "Treinamento: batch 32 terminou com 12:03:32.980622\n",
            "  33/1875 [..............................] - ETA: 4:25 - loss: 2.2737 - accuracy: 0.1098Treinamento: batch 33 iniciou com 12:03:32.982469\n",
            "Treinamento: batch 33 terminou com 12:03:33.123589\n",
            "  34/1875 [..............................] - ETA: 4:25 - loss: 2.2648 - accuracy: 0.1112Treinamento: batch 34 iniciou com 12:03:33.125321\n",
            "Treinamento: batch 34 terminou com 12:03:33.269464\n",
            "  35/1875 [..............................] - ETA: 4:25 - loss: 2.2579 - accuracy: 0.1116Treinamento: batch 35 iniciou com 12:03:33.271328\n",
            "Treinamento: batch 35 terminou com 12:03:33.413507\n",
            "  36/1875 [..............................] - ETA: 4:25 - loss: 2.2538 - accuracy: 0.1137Treinamento: batch 36 iniciou com 12:03:33.415247\n",
            "Treinamento: batch 36 terminou com 12:03:33.554688\n",
            "  37/1875 [..............................] - ETA: 4:24 - loss: 2.2496 - accuracy: 0.1166Treinamento: batch 37 iniciou com 12:03:33.556519\n",
            "Treinamento: batch 37 terminou com 12:03:33.713579\n",
            "  38/1875 [..............................] - ETA: 4:25 - loss: 2.2401 - accuracy: 0.1184Treinamento: batch 38 iniciou com 12:03:33.715268\n",
            "Treinamento: batch 38 terminou com 12:03:33.856608\n",
            "  39/1875 [..............................] - ETA: 4:25 - loss: 2.2381 - accuracy: 0.1210Treinamento: batch 39 iniciou com 12:03:33.858460\n",
            "Treinamento: batch 39 terminou com 12:03:34.006328\n",
            "  40/1875 [..............................] - ETA: 4:25 - loss: 2.2330 - accuracy: 0.1195Treinamento: batch 40 iniciou com 12:03:34.007990\n",
            "Treinamento: batch 40 terminou com 12:03:34.156049\n",
            "  41/1875 [..............................] - ETA: 4:25 - loss: 2.2251 - accuracy: 0.1212Treinamento: batch 41 iniciou com 12:03:34.157886\n",
            "Treinamento: batch 41 terminou com 12:03:34.303836\n",
            "  42/1875 [..............................] - ETA: 4:25 - loss: 2.2188 - accuracy: 0.1213Treinamento: batch 42 iniciou com 12:03:34.305935\n",
            "Treinamento: batch 42 terminou com 12:03:34.448361\n",
            "  43/1875 [..............................] - ETA: 4:25 - loss: 2.2096 - accuracy: 0.1250Treinamento: batch 43 iniciou com 12:03:34.450181\n",
            "Treinamento: batch 43 terminou com 12:03:34.594465\n",
            "  44/1875 [..............................] - ETA: 4:25 - loss: 2.2076 - accuracy: 0.1278Treinamento: batch 44 iniciou com 12:03:34.596590\n",
            "Treinamento: batch 44 terminou com 12:03:34.750666\n",
            "  45/1875 [..............................] - ETA: 4:25 - loss: 2.2032 - accuracy: 0.1285Treinamento: batch 45 iniciou com 12:03:34.752529\n",
            "Treinamento: batch 45 terminou com 12:03:34.894468\n",
            "  46/1875 [..............................] - ETA: 4:25 - loss: 2.2014 - accuracy: 0.1291Treinamento: batch 46 iniciou com 12:03:34.896140\n",
            "Treinamento: batch 46 terminou com 12:03:35.038165\n",
            "  47/1875 [..............................] - ETA: 4:25 - loss: 2.1946 - accuracy: 0.1316Treinamento: batch 47 iniciou com 12:03:35.040511\n",
            "Treinamento: batch 47 terminou com 12:03:35.184903\n",
            "  48/1875 [..............................] - ETA: 4:25 - loss: 2.1889 - accuracy: 0.1335Treinamento: batch 48 iniciou com 12:03:35.186619\n",
            "Treinamento: batch 48 terminou com 12:03:35.334387\n",
            "  49/1875 [..............................] - ETA: 4:25 - loss: 2.1920 - accuracy: 0.1339Treinamento: batch 49 iniciou com 12:03:35.336160\n",
            "Treinamento: batch 49 terminou com 12:03:35.480349\n",
            "  50/1875 [..............................] - ETA: 4:24 - loss: 2.1870 - accuracy: 0.1344Treinamento: batch 50 iniciou com 12:03:35.482159\n",
            "Treinamento: batch 50 terminou com 12:03:35.626781\n",
            "  51/1875 [..............................] - ETA: 4:24 - loss: 2.1829 - accuracy: 0.1354Treinamento: batch 51 iniciou com 12:03:35.630071\n",
            "Treinamento: batch 51 terminou com 12:03:35.790224\n",
            "  52/1875 [..............................] - ETA: 4:25 - loss: 2.1795 - accuracy: 0.1358Treinamento: batch 52 iniciou com 12:03:35.792317\n",
            "Treinamento: batch 52 terminou com 12:03:35.932217\n",
            "  53/1875 [..............................] - ETA: 4:25 - loss: 2.1750 - accuracy: 0.1386Treinamento: batch 53 iniciou com 12:03:35.933995\n",
            "Treinamento: batch 53 terminou com 12:03:36.075440\n",
            "  54/1875 [..............................] - ETA: 4:24 - loss: 2.1680 - accuracy: 0.1395Treinamento: batch 54 iniciou com 12:03:36.077243\n",
            "Treinamento: batch 54 terminou com 12:03:36.221946\n",
            "  55/1875 [..............................] - ETA: 4:24 - loss: 2.1620 - accuracy: 0.1420Treinamento: batch 55 iniciou com 12:03:36.223793\n",
            "Treinamento: batch 55 terminou com 12:03:36.369323\n",
            "  56/1875 [..............................] - ETA: 4:24 - loss: 2.1570 - accuracy: 0.1412Treinamento: batch 56 iniciou com 12:03:36.371039\n",
            "Treinamento: batch 56 terminou com 12:03:36.510992\n",
            "  57/1875 [..............................] - ETA: 4:24 - loss: 2.1541 - accuracy: 0.1420Treinamento: batch 57 iniciou com 12:03:36.512785\n",
            "Treinamento: batch 57 terminou com 12:03:36.657073\n",
            "  58/1875 [..............................] - ETA: 4:24 - loss: 2.1504 - accuracy: 0.1428Treinamento: batch 58 iniciou com 12:03:36.658766\n",
            "Treinamento: batch 58 terminou com 12:03:36.808344\n",
            "  59/1875 [..............................] - ETA: 4:24 - loss: 2.1461 - accuracy: 0.1430Treinamento: batch 59 iniciou com 12:03:36.810301\n",
            "Treinamento: batch 59 terminou com 12:03:36.959593\n",
            "  60/1875 [..............................] - ETA: 4:24 - loss: 2.1434 - accuracy: 0.1432Treinamento: batch 60 iniciou com 12:03:36.961389\n",
            "Treinamento: batch 60 terminou com 12:03:37.102329\n",
            "  61/1875 [..............................] - ETA: 4:24 - loss: 2.1402 - accuracy: 0.1440Treinamento: batch 61 iniciou com 12:03:37.104178\n",
            "Treinamento: batch 61 terminou com 12:03:37.250349\n",
            "  62/1875 [..............................] - ETA: 4:24 - loss: 2.1345 - accuracy: 0.1452Treinamento: batch 62 iniciou com 12:03:37.252030\n",
            "Treinamento: batch 62 terminou com 12:03:37.396099\n",
            "  63/1875 [>.............................] - ETA: 4:23 - loss: 2.1302 - accuracy: 0.1448Treinamento: batch 63 iniciou com 12:03:37.397833\n",
            "Treinamento: batch 63 terminou com 12:03:37.537025\n",
            "  64/1875 [>.............................] - ETA: 4:23 - loss: 2.1261 - accuracy: 0.1455Treinamento: batch 64 iniciou com 12:03:37.538847\n",
            "Treinamento: batch 64 terminou com 12:03:37.684390\n",
            "  65/1875 [>.............................] - ETA: 4:23 - loss: 2.1188 - accuracy: 0.1462Treinamento: batch 65 iniciou com 12:03:37.686259\n",
            "Treinamento: batch 65 terminou com 12:03:37.841668\n",
            "  66/1875 [>.............................] - ETA: 4:23 - loss: 2.1152 - accuracy: 0.1468Treinamento: batch 66 iniciou com 12:03:37.843383\n",
            "Treinamento: batch 66 terminou com 12:03:37.990425\n",
            "  67/1875 [>.............................] - ETA: 4:23 - loss: 2.1091 - accuracy: 0.1474Treinamento: batch 67 iniciou com 12:03:37.992341\n",
            "Treinamento: batch 67 terminou com 12:03:38.136209\n",
            "  68/1875 [>.............................] - ETA: 4:23 - loss: 2.1049 - accuracy: 0.1471Treinamento: batch 68 iniciou com 12:03:38.137924\n",
            "Treinamento: batch 68 terminou com 12:03:38.282748\n",
            "  69/1875 [>.............................] - ETA: 4:23 - loss: 2.1023 - accuracy: 0.1486Treinamento: batch 69 iniciou com 12:03:38.284692\n",
            "Treinamento: batch 69 terminou com 12:03:38.430654\n",
            "  70/1875 [>.............................] - ETA: 4:23 - loss: 2.1024 - accuracy: 0.1496Treinamento: batch 70 iniciou com 12:03:38.432306\n",
            "Treinamento: batch 70 terminou com 12:03:38.576015\n",
            "  71/1875 [>.............................] - ETA: 4:23 - loss: 2.0964 - accuracy: 0.1505Treinamento: batch 71 iniciou com 12:03:38.579962\n",
            "Treinamento: batch 71 terminou com 12:03:38.721483\n",
            "  72/1875 [>.............................] - ETA: 4:22 - loss: 2.0944 - accuracy: 0.1493Treinamento: batch 72 iniciou com 12:03:38.723535\n",
            "Treinamento: batch 72 terminou com 12:03:38.878169\n",
            "  73/1875 [>.............................] - ETA: 4:23 - loss: 2.0898 - accuracy: 0.1498Treinamento: batch 73 iniciou com 12:03:38.880165\n",
            "Treinamento: batch 73 terminou com 12:03:39.026829\n",
            "  74/1875 [>.............................] - ETA: 4:22 - loss: 2.0860 - accuracy: 0.1512Treinamento: batch 74 iniciou com 12:03:39.028550\n",
            "Treinamento: batch 74 terminou com 12:03:39.168962\n",
            "  75/1875 [>.............................] - ETA: 4:22 - loss: 2.0817 - accuracy: 0.1538Treinamento: batch 75 iniciou com 12:03:39.170764\n",
            "Treinamento: batch 75 terminou com 12:03:39.315164\n",
            "  76/1875 [>.............................] - ETA: 4:22 - loss: 2.0767 - accuracy: 0.1550Treinamento: batch 76 iniciou com 12:03:39.317109\n",
            "Treinamento: batch 76 terminou com 12:03:39.458580\n",
            "  77/1875 [>.............................] - ETA: 4:22 - loss: 2.0719 - accuracy: 0.1554Treinamento: batch 77 iniciou com 12:03:39.461374\n",
            "Treinamento: batch 77 terminou com 12:03:39.602935\n",
            "  78/1875 [>.............................] - ETA: 4:22 - loss: 2.0679 - accuracy: 0.1562Treinamento: batch 78 iniciou com 12:03:39.604784\n",
            "Treinamento: batch 78 terminou com 12:03:39.752906\n",
            "  79/1875 [>.............................] - ETA: 4:22 - loss: 2.0629 - accuracy: 0.1570Treinamento: batch 79 iniciou com 12:03:39.754560\n",
            "Treinamento: batch 79 terminou com 12:03:39.909810\n",
            "  80/1875 [>.............................] - ETA: 4:22 - loss: 2.0581 - accuracy: 0.1582Treinamento: batch 80 iniciou com 12:03:39.912348\n",
            "Treinamento: batch 80 terminou com 12:03:40.054591\n",
            "  81/1875 [>.............................] - ETA: 4:22 - loss: 2.0540 - accuracy: 0.1597Treinamento: batch 81 iniciou com 12:03:40.056675\n",
            "Treinamento: batch 81 terminou com 12:03:40.200145\n",
            "  82/1875 [>.............................] - ETA: 4:21 - loss: 2.0513 - accuracy: 0.1612Treinamento: batch 82 iniciou com 12:03:40.202369\n",
            "Treinamento: batch 82 terminou com 12:03:40.346107\n",
            "  83/1875 [>.............................] - ETA: 4:21 - loss: 2.0464 - accuracy: 0.1627Treinamento: batch 83 iniciou com 12:03:40.347770\n",
            "Treinamento: batch 83 terminou com 12:03:40.488032\n",
            "  84/1875 [>.............................] - ETA: 4:21 - loss: 2.0421 - accuracy: 0.1629Treinamento: batch 84 iniciou com 12:03:40.490005\n",
            "Treinamento: batch 84 terminou com 12:03:40.635040\n",
            "  85/1875 [>.............................] - ETA: 4:21 - loss: 2.0395 - accuracy: 0.1640Treinamento: batch 85 iniciou com 12:03:40.636862\n",
            "Treinamento: batch 85 terminou com 12:03:40.783963\n",
            "  86/1875 [>.............................] - ETA: 4:21 - loss: 2.0328 - accuracy: 0.1672Treinamento: batch 86 iniciou com 12:03:40.785735\n",
            "Treinamento: batch 86 terminou com 12:03:40.940915\n",
            "  87/1875 [>.............................] - ETA: 4:21 - loss: 2.0280 - accuracy: 0.1685Treinamento: batch 87 iniciou com 12:03:40.942600\n",
            "Treinamento: batch 87 terminou com 12:03:41.115412\n",
            "  88/1875 [>.............................] - ETA: 4:21 - loss: 2.0261 - accuracy: 0.1676Treinamento: batch 88 iniciou com 12:03:41.117054\n",
            "Treinamento: batch 88 terminou com 12:03:41.268826\n",
            "  89/1875 [>.............................] - ETA: 4:21 - loss: 2.0224 - accuracy: 0.1678Treinamento: batch 89 iniciou com 12:03:41.270747\n",
            "Treinamento: batch 89 terminou com 12:03:41.417569\n",
            "  90/1875 [>.............................] - ETA: 4:21 - loss: 2.0181 - accuracy: 0.1691Treinamento: batch 90 iniciou com 12:03:41.419425\n",
            "Treinamento: batch 90 terminou com 12:03:41.566043\n",
            "  91/1875 [>.............................] - ETA: 4:21 - loss: 2.0149 - accuracy: 0.1693Treinamento: batch 91 iniciou com 12:03:41.567869\n",
            "Treinamento: batch 91 terminou com 12:03:41.713415\n",
            "  92/1875 [>.............................] - ETA: 4:21 - loss: 2.0122 - accuracy: 0.1712Treinamento: batch 92 iniciou com 12:03:41.715088\n",
            "Treinamento: batch 92 terminou com 12:03:41.872842\n",
            "  93/1875 [>.............................] - ETA: 4:21 - loss: 2.0098 - accuracy: 0.1700Treinamento: batch 93 iniciou com 12:03:41.874577\n",
            "Treinamento: batch 93 terminou com 12:03:42.020571\n",
            "  94/1875 [>.............................] - ETA: 4:21 - loss: 2.0062 - accuracy: 0.1719Treinamento: batch 94 iniciou com 12:03:42.022408\n",
            "Treinamento: batch 94 terminou com 12:03:42.165596\n",
            "  95/1875 [>.............................] - ETA: 4:21 - loss: 2.0050 - accuracy: 0.1727Treinamento: batch 95 iniciou com 12:03:42.167293\n",
            "Treinamento: batch 95 terminou com 12:03:42.311050\n",
            "  96/1875 [>.............................] - ETA: 4:21 - loss: 2.0013 - accuracy: 0.1738Treinamento: batch 96 iniciou com 12:03:42.312745\n",
            "Treinamento: batch 96 terminou com 12:03:42.463700\n",
            "  97/1875 [>.............................] - ETA: 4:21 - loss: 1.9973 - accuracy: 0.1743Treinamento: batch 97 iniciou com 12:03:42.465359\n",
            "Treinamento: batch 97 terminou com 12:03:42.605717\n",
            "  98/1875 [>.............................] - ETA: 4:20 - loss: 1.9922 - accuracy: 0.1763Treinamento: batch 98 iniciou com 12:03:42.607418\n",
            "Treinamento: batch 98 terminou com 12:03:42.755215\n",
            "  99/1875 [>.............................] - ETA: 4:20 - loss: 1.9896 - accuracy: 0.1780Treinamento: batch 99 iniciou com 12:03:42.757351\n",
            "Treinamento: batch 99 terminou com 12:03:42.910812\n",
            " 100/1875 [>.............................] - ETA: 4:20 - loss: 1.9847 - accuracy: 0.1803Treinamento: batch 100 iniciou com 12:03:42.912530\n",
            "Treinamento: batch 100 terminou com 12:03:43.057340\n",
            " 101/1875 [>.............................] - ETA: 4:20 - loss: 1.9817 - accuracy: 0.1810Treinamento: batch 101 iniciou com 12:03:43.059106\n",
            "Treinamento: batch 101 terminou com 12:03:43.203614\n",
            " 102/1875 [>.............................] - ETA: 4:20 - loss: 1.9783 - accuracy: 0.1814Treinamento: batch 102 iniciou com 12:03:43.205406\n",
            "Treinamento: batch 102 terminou com 12:03:43.348373\n",
            " 103/1875 [>.............................] - ETA: 4:20 - loss: 1.9747 - accuracy: 0.1836Treinamento: batch 103 iniciou com 12:03:43.350150\n",
            "Treinamento: batch 103 terminou com 12:03:43.496095\n",
            " 104/1875 [>.............................] - ETA: 4:20 - loss: 1.9735 - accuracy: 0.1848Treinamento: batch 104 iniciou com 12:03:43.497809\n",
            "Treinamento: batch 104 terminou com 12:03:43.642710\n",
            " 105/1875 [>.............................] - ETA: 4:19 - loss: 1.9715 - accuracy: 0.1857Treinamento: batch 105 iniciou com 12:03:43.644543\n",
            "Treinamento: batch 105 terminou com 12:03:43.786875\n",
            " 106/1875 [>.............................] - ETA: 4:19 - loss: 1.9705 - accuracy: 0.1857Treinamento: batch 106 iniciou com 12:03:43.788890\n",
            "Treinamento: batch 106 terminou com 12:03:43.947071\n",
            " 107/1875 [>.............................] - ETA: 4:19 - loss: 1.9685 - accuracy: 0.1872Treinamento: batch 107 iniciou com 12:03:43.948735\n",
            "Treinamento: batch 107 terminou com 12:03:44.095153\n",
            " 108/1875 [>.............................] - ETA: 4:19 - loss: 1.9646 - accuracy: 0.1892Treinamento: batch 108 iniciou com 12:03:44.096887\n",
            "Treinamento: batch 108 terminou com 12:03:44.243782\n",
            " 109/1875 [>.............................] - ETA: 4:19 - loss: 1.9636 - accuracy: 0.1912Treinamento: batch 109 iniciou com 12:03:44.245570\n",
            "Treinamento: batch 109 terminou com 12:03:44.387427\n",
            " 110/1875 [>.............................] - ETA: 4:19 - loss: 1.9635 - accuracy: 0.1915Treinamento: batch 110 iniciou com 12:03:44.389269\n",
            "Treinamento: batch 110 terminou com 12:03:44.535732\n",
            " 111/1875 [>.............................] - ETA: 4:19 - loss: 1.9595 - accuracy: 0.1923Treinamento: batch 111 iniciou com 12:03:44.537443\n",
            "Treinamento: batch 111 terminou com 12:03:44.681354\n",
            " 112/1875 [>.............................] - ETA: 4:19 - loss: 1.9561 - accuracy: 0.1936Treinamento: batch 112 iniciou com 12:03:44.683269\n",
            "Treinamento: batch 112 terminou com 12:03:44.827427\n",
            " 113/1875 [>.............................] - ETA: 4:18 - loss: 1.9525 - accuracy: 0.1941Treinamento: batch 113 iniciou com 12:03:44.829163\n",
            "Treinamento: batch 113 terminou com 12:03:44.982309\n",
            " 114/1875 [>.............................] - ETA: 4:18 - loss: 1.9505 - accuracy: 0.1952Treinamento: batch 114 iniciou com 12:03:44.983998\n",
            "Treinamento: batch 114 terminou com 12:03:45.126421\n",
            " 115/1875 [>.............................] - ETA: 4:18 - loss: 1.9478 - accuracy: 0.1951Treinamento: batch 115 iniciou com 12:03:45.128123\n",
            "Treinamento: batch 115 terminou com 12:03:45.274594\n",
            " 116/1875 [>.............................] - ETA: 4:18 - loss: 1.9447 - accuracy: 0.1964Treinamento: batch 116 iniciou com 12:03:45.276287\n",
            "Treinamento: batch 116 terminou com 12:03:45.415201\n",
            " 117/1875 [>.............................] - ETA: 4:18 - loss: 1.9412 - accuracy: 0.1979Treinamento: batch 117 iniciou com 12:03:45.416888\n",
            "Treinamento: batch 117 terminou com 12:03:45.571762\n",
            " 118/1875 [>.............................] - ETA: 4:18 - loss: 1.9379 - accuracy: 0.1992Treinamento: batch 118 iniciou com 12:03:45.573495\n",
            "Treinamento: batch 118 terminou com 12:03:45.720944\n",
            " 119/1875 [>.............................] - ETA: 4:18 - loss: 1.9345 - accuracy: 0.2004Treinamento: batch 119 iniciou com 12:03:45.723420\n",
            "Treinamento: batch 119 terminou com 12:03:45.868059\n",
            " 120/1875 [>.............................] - ETA: 4:18 - loss: 1.9314 - accuracy: 0.2021Treinamento: batch 120 iniciou com 12:03:45.869807\n",
            "Treinamento: batch 120 terminou com 12:03:46.025397\n",
            " 121/1875 [>.............................] - ETA: 4:18 - loss: 1.9273 - accuracy: 0.2043Treinamento: batch 121 iniciou com 12:03:46.027108\n",
            "Treinamento: batch 121 terminou com 12:03:46.167499\n",
            " 122/1875 [>.............................] - ETA: 4:17 - loss: 1.9232 - accuracy: 0.2057Treinamento: batch 122 iniciou com 12:03:46.168966\n",
            "Treinamento: batch 122 terminou com 12:03:46.315176\n",
            " 123/1875 [>.............................] - ETA: 4:17 - loss: 1.9181 - accuracy: 0.2073Treinamento: batch 123 iniciou com 12:03:46.317177\n",
            "Treinamento: batch 123 terminou com 12:03:46.459333\n",
            " 124/1875 [>.............................] - ETA: 4:17 - loss: 1.9163 - accuracy: 0.2072Treinamento: batch 124 iniciou com 12:03:46.461232\n",
            "Treinamento: batch 124 terminou com 12:03:46.607202\n",
            " 125/1875 [=>............................] - ETA: 4:17 - loss: 1.9162 - accuracy: 0.2072Treinamento: batch 125 iniciou com 12:03:46.608983\n",
            "Treinamento: batch 125 terminou com 12:03:46.750862\n",
            " 126/1875 [=>............................] - ETA: 4:17 - loss: 1.9127 - accuracy: 0.2093Treinamento: batch 126 iniciou com 12:03:46.752560\n",
            "Treinamento: batch 126 terminou com 12:03:46.896136\n",
            " 127/1875 [=>............................] - ETA: 4:17 - loss: 1.9117 - accuracy: 0.2101Treinamento: batch 127 iniciou com 12:03:46.897889\n",
            "Treinamento: batch 127 terminou com 12:03:47.045713\n",
            " 128/1875 [=>............................] - ETA: 4:16 - loss: 1.9106 - accuracy: 0.2104Treinamento: batch 128 iniciou com 12:03:47.047283\n",
            "Treinamento: batch 128 terminou com 12:03:47.200228\n",
            " 129/1875 [=>............................] - ETA: 4:16 - loss: 1.9078 - accuracy: 0.2115Treinamento: batch 129 iniciou com 12:03:47.201971\n",
            "Treinamento: batch 129 terminou com 12:03:47.350308\n",
            " 130/1875 [=>............................] - ETA: 4:16 - loss: 1.9036 - accuracy: 0.2125Treinamento: batch 130 iniciou com 12:03:47.351915\n",
            "Treinamento: batch 130 terminou com 12:03:47.490817\n",
            " 131/1875 [=>............................] - ETA: 4:16 - loss: 1.9022 - accuracy: 0.2137Treinamento: batch 131 iniciou com 12:03:47.492560\n",
            "Treinamento: batch 131 terminou com 12:03:47.635944\n",
            " 132/1875 [=>............................] - ETA: 4:16 - loss: 1.8992 - accuracy: 0.2152Treinamento: batch 132 iniciou com 12:03:47.637584\n",
            "Treinamento: batch 132 terminou com 12:03:47.783504\n",
            " 133/1875 [=>............................] - ETA: 4:16 - loss: 1.8969 - accuracy: 0.2159Treinamento: batch 133 iniciou com 12:03:47.785267\n",
            "Treinamento: batch 133 terminou com 12:03:47.928043\n",
            " 134/1875 [=>............................] - ETA: 4:16 - loss: 1.8941 - accuracy: 0.2171Treinamento: batch 134 iniciou com 12:03:47.929609\n",
            "Treinamento: batch 134 terminou com 12:03:48.082358\n",
            " 135/1875 [=>............................] - ETA: 4:15 - loss: 1.8914 - accuracy: 0.2176Treinamento: batch 135 iniciou com 12:03:48.084082\n",
            "Treinamento: batch 135 terminou com 12:03:48.226067\n",
            " 136/1875 [=>............................] - ETA: 4:15 - loss: 1.8870 - accuracy: 0.2192Treinamento: batch 136 iniciou com 12:03:48.227819\n",
            "Treinamento: batch 136 terminou com 12:03:48.370611\n",
            " 137/1875 [=>............................] - ETA: 4:15 - loss: 1.8844 - accuracy: 0.2199Treinamento: batch 137 iniciou com 12:03:48.372349\n",
            "Treinamento: batch 137 terminou com 12:03:48.519036\n",
            " 138/1875 [=>............................] - ETA: 4:15 - loss: 1.8811 - accuracy: 0.2224Treinamento: batch 138 iniciou com 12:03:48.520898\n",
            "Treinamento: batch 138 terminou com 12:03:48.663442\n",
            " 139/1875 [=>............................] - ETA: 4:15 - loss: 1.8778 - accuracy: 0.2235Treinamento: batch 139 iniciou com 12:03:48.665148\n",
            "Treinamento: batch 139 terminou com 12:03:48.806867\n",
            " 140/1875 [=>............................] - ETA: 4:15 - loss: 1.8749 - accuracy: 0.2237Treinamento: batch 140 iniciou com 12:03:48.808553\n",
            "Treinamento: batch 140 terminou com 12:03:48.951977\n",
            " 141/1875 [=>............................] - ETA: 4:14 - loss: 1.8725 - accuracy: 0.2247Treinamento: batch 141 iniciou com 12:03:48.960142\n",
            "Treinamento: batch 141 terminou com 12:03:49.102890\n",
            " 142/1875 [=>............................] - ETA: 4:14 - loss: 1.8693 - accuracy: 0.2262Treinamento: batch 142 iniciou com 12:03:49.104677\n",
            "Treinamento: batch 142 terminou com 12:03:49.250060\n",
            " 143/1875 [=>............................] - ETA: 4:14 - loss: 1.8658 - accuracy: 0.2279Treinamento: batch 143 iniciou com 12:03:49.251811\n",
            "Treinamento: batch 143 terminou com 12:03:49.390621\n",
            " 144/1875 [=>............................] - ETA: 4:14 - loss: 1.8628 - accuracy: 0.2285Treinamento: batch 144 iniciou com 12:03:49.392364\n",
            "Treinamento: batch 144 terminou com 12:03:49.540685\n",
            " 145/1875 [=>............................] - ETA: 4:14 - loss: 1.8591 - accuracy: 0.2306Treinamento: batch 145 iniciou com 12:03:49.542414\n",
            "Treinamento: batch 145 terminou com 12:03:49.688451\n",
            " 146/1875 [=>............................] - ETA: 4:14 - loss: 1.8576 - accuracy: 0.2316Treinamento: batch 146 iniciou com 12:03:49.690179\n",
            "Treinamento: batch 146 terminou com 12:03:49.834837\n",
            " 147/1875 [=>............................] - ETA: 4:14 - loss: 1.8558 - accuracy: 0.2321Treinamento: batch 147 iniciou com 12:03:49.836449\n",
            "Treinamento: batch 147 terminou com 12:03:49.994361\n",
            " 148/1875 [=>............................] - ETA: 4:14 - loss: 1.8533 - accuracy: 0.2335Treinamento: batch 148 iniciou com 12:03:49.996136\n",
            "Treinamento: batch 148 terminou com 12:03:50.149710\n",
            " 149/1875 [=>............................] - ETA: 4:14 - loss: 1.8508 - accuracy: 0.2345Treinamento: batch 149 iniciou com 12:03:50.151715\n",
            "Treinamento: batch 149 terminou com 12:03:50.295189\n",
            " 150/1875 [=>............................] - ETA: 4:13 - loss: 1.8469 - accuracy: 0.2360Treinamento: batch 150 iniciou com 12:03:50.296853\n",
            "Treinamento: batch 150 terminou com 12:03:50.437406\n",
            " 151/1875 [=>............................] - ETA: 4:13 - loss: 1.8441 - accuracy: 0.2374Treinamento: batch 151 iniciou com 12:03:50.439044\n",
            "Treinamento: batch 151 terminou com 12:03:50.582443\n",
            " 152/1875 [=>............................] - ETA: 4:13 - loss: 1.8412 - accuracy: 0.2385Treinamento: batch 152 iniciou com 12:03:50.584245\n",
            "Treinamento: batch 152 terminou com 12:03:50.728782\n",
            " 153/1875 [=>............................] - ETA: 4:13 - loss: 1.8383 - accuracy: 0.2398Treinamento: batch 153 iniciou com 12:03:50.730441\n",
            "Treinamento: batch 153 terminou com 12:03:50.873263\n",
            " 154/1875 [=>............................] - ETA: 4:13 - loss: 1.8367 - accuracy: 0.2401Treinamento: batch 154 iniciou com 12:03:50.875157\n",
            "Treinamento: batch 154 terminou com 12:03:51.031325\n",
            " 155/1875 [=>............................] - ETA: 4:13 - loss: 1.8350 - accuracy: 0.2409Treinamento: batch 155 iniciou com 12:03:51.033070\n",
            "Treinamento: batch 155 terminou com 12:03:51.173662\n",
            " 156/1875 [=>............................] - ETA: 4:12 - loss: 1.8333 - accuracy: 0.2420Treinamento: batch 156 iniciou com 12:03:51.175513\n",
            "Treinamento: batch 156 terminou com 12:03:51.325229\n",
            " 157/1875 [=>............................] - ETA: 4:12 - loss: 1.8304 - accuracy: 0.2428Treinamento: batch 157 iniciou com 12:03:51.327030\n",
            "Treinamento: batch 157 terminou com 12:03:51.479367\n",
            " 158/1875 [=>............................] - ETA: 4:12 - loss: 1.8278 - accuracy: 0.2443Treinamento: batch 158 iniciou com 12:03:51.481785\n",
            "Treinamento: batch 158 terminou com 12:03:51.657802\n",
            " 159/1875 [=>............................] - ETA: 4:12 - loss: 1.8249 - accuracy: 0.2455Treinamento: batch 159 iniciou com 12:03:51.659512\n",
            "Treinamento: batch 159 terminou com 12:03:51.805033\n",
            " 160/1875 [=>............................] - ETA: 4:12 - loss: 1.8229 - accuracy: 0.2459Treinamento: batch 160 iniciou com 12:03:51.806856\n",
            "Treinamento: batch 160 terminou com 12:03:51.949318\n",
            " 161/1875 [=>............................] - ETA: 4:12 - loss: 1.8197 - accuracy: 0.2479Treinamento: batch 161 iniciou com 12:03:51.951036\n",
            "Treinamento: batch 161 terminou com 12:03:52.109594\n",
            " 162/1875 [=>............................] - ETA: 4:12 - loss: 1.8171 - accuracy: 0.2483Treinamento: batch 162 iniciou com 12:03:52.111956\n",
            "Treinamento: batch 162 terminou com 12:03:52.256252\n",
            " 163/1875 [=>............................] - ETA: 4:12 - loss: 1.8149 - accuracy: 0.2487Treinamento: batch 163 iniciou com 12:03:52.258060\n",
            "Treinamento: batch 163 terminou com 12:03:52.405459\n",
            " 164/1875 [=>............................] - ETA: 4:12 - loss: 1.8110 - accuracy: 0.2513Treinamento: batch 164 iniciou com 12:03:52.407202\n",
            "Treinamento: batch 164 terminou com 12:03:52.564705\n",
            " 165/1875 [=>............................] - ETA: 4:12 - loss: 1.8086 - accuracy: 0.2525Treinamento: batch 165 iniciou com 12:03:52.566540\n",
            "Treinamento: batch 165 terminou com 12:03:52.714353\n",
            " 166/1875 [=>............................] - ETA: 4:12 - loss: 1.8044 - accuracy: 0.2545Treinamento: batch 166 iniciou com 12:03:52.716074\n",
            "Treinamento: batch 166 terminou com 12:03:52.861905\n",
            " 167/1875 [=>............................] - ETA: 4:12 - loss: 1.8018 - accuracy: 0.2558Treinamento: batch 167 iniciou com 12:03:52.863751\n",
            "Treinamento: batch 167 terminou com 12:03:53.004214\n",
            " 168/1875 [=>............................] - ETA: 4:11 - loss: 1.8000 - accuracy: 0.2560Treinamento: batch 168 iniciou com 12:03:53.006396\n",
            "Treinamento: batch 168 terminou com 12:03:53.158150\n",
            " 169/1875 [=>............................] - ETA: 4:11 - loss: 1.7978 - accuracy: 0.2568Treinamento: batch 169 iniciou com 12:03:53.159923\n",
            "Treinamento: batch 169 terminou com 12:03:53.306829\n",
            " 170/1875 [=>............................] - ETA: 4:11 - loss: 1.7967 - accuracy: 0.2574Treinamento: batch 170 iniciou com 12:03:53.308560\n",
            "Treinamento: batch 170 terminou com 12:03:53.449503\n",
            " 171/1875 [=>............................] - ETA: 4:11 - loss: 1.7959 - accuracy: 0.2579Treinamento: batch 171 iniciou com 12:03:53.451242\n",
            "Treinamento: batch 171 terminou com 12:03:53.594126\n",
            " 172/1875 [=>............................] - ETA: 4:11 - loss: 1.7932 - accuracy: 0.2591Treinamento: batch 172 iniciou com 12:03:53.595884\n",
            "Treinamento: batch 172 terminou com 12:03:53.748424\n",
            " 173/1875 [=>............................] - ETA: 4:11 - loss: 1.7908 - accuracy: 0.2610Treinamento: batch 173 iniciou com 12:03:53.750063\n",
            "Treinamento: batch 173 terminou com 12:03:53.891111\n",
            " 174/1875 [=>............................] - ETA: 4:10 - loss: 1.7879 - accuracy: 0.2628Treinamento: batch 174 iniciou com 12:03:53.892975\n",
            "Treinamento: batch 174 terminou com 12:03:54.036141\n",
            " 175/1875 [=>............................] - ETA: 4:10 - loss: 1.7846 - accuracy: 0.2637Treinamento: batch 175 iniciou com 12:03:54.038124\n",
            "Treinamento: batch 175 terminou com 12:03:54.196552\n",
            " 176/1875 [=>............................] - ETA: 4:10 - loss: 1.7822 - accuracy: 0.2651Treinamento: batch 176 iniciou com 12:03:54.198340\n",
            "Treinamento: batch 176 terminou com 12:03:54.339348\n",
            " 177/1875 [=>............................] - ETA: 4:10 - loss: 1.7797 - accuracy: 0.2666Treinamento: batch 177 iniciou com 12:03:54.341383\n",
            "Treinamento: batch 177 terminou com 12:03:54.488527\n",
            " 178/1875 [=>............................] - ETA: 4:10 - loss: 1.7771 - accuracy: 0.2676Treinamento: batch 178 iniciou com 12:03:54.490393\n",
            "Treinamento: batch 178 terminou com 12:03:54.636876\n",
            " 179/1875 [=>............................] - ETA: 4:10 - loss: 1.7749 - accuracy: 0.2678Treinamento: batch 179 iniciou com 12:03:54.638678\n",
            "Treinamento: batch 179 terminou com 12:03:54.784475\n",
            " 180/1875 [=>............................] - ETA: 4:10 - loss: 1.7730 - accuracy: 0.2689Treinamento: batch 180 iniciou com 12:03:54.786285\n",
            "Treinamento: batch 180 terminou com 12:03:54.934590\n",
            " 181/1875 [=>............................] - ETA: 4:10 - loss: 1.7710 - accuracy: 0.2690Treinamento: batch 181 iniciou com 12:03:54.936447\n",
            "Treinamento: batch 181 terminou com 12:03:55.093977\n",
            " 182/1875 [=>............................] - ETA: 4:09 - loss: 1.7679 - accuracy: 0.2706Treinamento: batch 182 iniciou com 12:03:55.095714\n",
            "Treinamento: batch 182 terminou com 12:03:55.238302\n",
            " 183/1875 [=>............................] - ETA: 4:09 - loss: 1.7650 - accuracy: 0.2720Treinamento: batch 183 iniciou com 12:03:55.240068\n",
            "Treinamento: batch 183 terminou com 12:03:55.388473\n",
            " 184/1875 [=>............................] - ETA: 4:09 - loss: 1.7626 - accuracy: 0.2731Treinamento: batch 184 iniciou com 12:03:55.390583\n",
            "Treinamento: batch 184 terminou com 12:03:55.535576\n",
            " 185/1875 [=>............................] - ETA: 4:09 - loss: 1.7612 - accuracy: 0.2738Treinamento: batch 185 iniciou com 12:03:55.537391\n",
            "Treinamento: batch 185 terminou com 12:03:55.683999\n",
            " 186/1875 [=>............................] - ETA: 4:09 - loss: 1.7602 - accuracy: 0.2742Treinamento: batch 186 iniciou com 12:03:55.685777\n",
            "Treinamento: batch 186 terminou com 12:03:55.830229\n",
            " 187/1875 [=>............................] - ETA: 4:09 - loss: 1.7575 - accuracy: 0.2746Treinamento: batch 187 iniciou com 12:03:55.832611\n",
            "Treinamento: batch 187 terminou com 12:03:55.973008\n",
            " 188/1875 [==>...........................] - ETA: 4:09 - loss: 1.7562 - accuracy: 0.2753Treinamento: batch 188 iniciou com 12:03:55.974771\n",
            "Treinamento: batch 188 terminou com 12:03:56.130592\n",
            " 189/1875 [==>...........................] - ETA: 4:08 - loss: 1.7541 - accuracy: 0.2763Treinamento: batch 189 iniciou com 12:03:56.132365\n",
            "Treinamento: batch 189 terminou com 12:03:56.278930\n",
            " 190/1875 [==>...........................] - ETA: 4:08 - loss: 1.7522 - accuracy: 0.2770Treinamento: batch 190 iniciou com 12:03:56.280619\n",
            "Treinamento: batch 190 terminou com 12:03:56.424535\n",
            " 191/1875 [==>...........................] - ETA: 4:08 - loss: 1.7505 - accuracy: 0.2777Treinamento: batch 191 iniciou com 12:03:56.426303\n",
            "Treinamento: batch 191 terminou com 12:03:56.573752\n",
            " 192/1875 [==>...........................] - ETA: 4:08 - loss: 1.7491 - accuracy: 0.2780Treinamento: batch 192 iniciou com 12:03:56.575601\n",
            "Treinamento: batch 192 terminou com 12:03:56.723290\n",
            " 193/1875 [==>...........................] - ETA: 4:08 - loss: 1.7471 - accuracy: 0.2788Treinamento: batch 193 iniciou com 12:03:56.724990\n",
            "Treinamento: batch 193 terminou com 12:03:56.866231\n",
            " 194/1875 [==>...........................] - ETA: 4:08 - loss: 1.7442 - accuracy: 0.2806Treinamento: batch 194 iniciou com 12:03:56.867909\n",
            "Treinamento: batch 194 terminou com 12:03:57.013426\n",
            " 195/1875 [==>...........................] - ETA: 4:08 - loss: 1.7434 - accuracy: 0.2812Treinamento: batch 195 iniciou com 12:03:57.015114\n",
            "Treinamento: batch 195 terminou com 12:03:57.170556\n",
            " 196/1875 [==>...........................] - ETA: 4:07 - loss: 1.7440 - accuracy: 0.2816Treinamento: batch 196 iniciou com 12:03:57.172240\n",
            "Treinamento: batch 196 terminou com 12:03:57.323909\n",
            " 197/1875 [==>...........................] - ETA: 4:07 - loss: 1.7422 - accuracy: 0.2827Treinamento: batch 197 iniciou com 12:03:57.325692\n",
            "Treinamento: batch 197 terminou com 12:03:57.472553\n",
            " 198/1875 [==>...........................] - ETA: 4:07 - loss: 1.7404 - accuracy: 0.2836Treinamento: batch 198 iniciou com 12:03:57.475604\n",
            "Treinamento: batch 198 terminou com 12:03:57.620685\n",
            " 199/1875 [==>...........................] - ETA: 4:07 - loss: 1.7400 - accuracy: 0.2836Treinamento: batch 199 iniciou com 12:03:57.622464\n",
            "Treinamento: batch 199 terminou com 12:03:57.779489\n",
            " 200/1875 [==>...........................] - ETA: 4:07 - loss: 1.7397 - accuracy: 0.2834Treinamento: batch 200 iniciou com 12:03:57.781221\n",
            "Treinamento: batch 200 terminou com 12:03:57.929568\n",
            " 201/1875 [==>...........................] - ETA: 4:07 - loss: 1.7374 - accuracy: 0.2842Treinamento: batch 201 iniciou com 12:03:57.931480\n",
            "Treinamento: batch 201 terminou com 12:03:58.075901\n",
            " 202/1875 [==>...........................] - ETA: 4:07 - loss: 1.7365 - accuracy: 0.2848Treinamento: batch 202 iniciou com 12:03:58.078237\n",
            "Treinamento: batch 202 terminou com 12:03:58.236150\n",
            " 203/1875 [==>...........................] - ETA: 4:07 - loss: 1.7347 - accuracy: 0.2857Treinamento: batch 203 iniciou com 12:03:58.237955\n",
            "Treinamento: batch 203 terminou com 12:03:58.381810\n",
            " 204/1875 [==>...........................] - ETA: 4:07 - loss: 1.7320 - accuracy: 0.2872Treinamento: batch 204 iniciou com 12:03:58.383424\n",
            "Treinamento: batch 204 terminou com 12:03:58.526099\n",
            " 205/1875 [==>...........................] - ETA: 4:06 - loss: 1.7295 - accuracy: 0.2883Treinamento: batch 205 iniciou com 12:03:58.527891\n",
            "Treinamento: batch 205 terminou com 12:03:58.676520\n",
            " 206/1875 [==>...........................] - ETA: 4:06 - loss: 1.7279 - accuracy: 0.2890Treinamento: batch 206 iniciou com 12:03:58.678563\n",
            "Treinamento: batch 206 terminou com 12:03:58.819534\n",
            " 207/1875 [==>...........................] - ETA: 4:06 - loss: 1.7259 - accuracy: 0.2897Treinamento: batch 207 iniciou com 12:03:58.821425\n",
            "Treinamento: batch 207 terminou com 12:03:58.964566\n",
            " 208/1875 [==>...........................] - ETA: 4:06 - loss: 1.7230 - accuracy: 0.2907Treinamento: batch 208 iniciou com 12:03:58.966333\n",
            "Treinamento: batch 208 terminou com 12:03:59.118661\n",
            " 209/1875 [==>...........................] - ETA: 4:06 - loss: 1.7208 - accuracy: 0.2920Treinamento: batch 209 iniciou com 12:03:59.120392\n",
            "Treinamento: batch 209 terminou com 12:03:59.273072\n",
            " 210/1875 [==>...........................] - ETA: 4:06 - loss: 1.7181 - accuracy: 0.2935Treinamento: batch 210 iniciou com 12:03:59.274886\n",
            "Treinamento: batch 210 terminou com 12:03:59.419975\n",
            " 211/1875 [==>...........................] - ETA: 4:06 - loss: 1.7163 - accuracy: 0.2938Treinamento: batch 211 iniciou com 12:03:59.421885\n",
            "Treinamento: batch 211 terminou com 12:03:59.563291\n",
            " 212/1875 [==>...........................] - ETA: 4:05 - loss: 1.7144 - accuracy: 0.2938Treinamento: batch 212 iniciou com 12:03:59.565054\n",
            "Treinamento: batch 212 terminou com 12:03:59.713530\n",
            " 213/1875 [==>...........................] - ETA: 4:05 - loss: 1.7125 - accuracy: 0.2942Treinamento: batch 213 iniciou com 12:03:59.715344\n",
            "Treinamento: batch 213 terminou com 12:03:59.858610\n",
            " 214/1875 [==>...........................] - ETA: 4:05 - loss: 1.7116 - accuracy: 0.2945Treinamento: batch 214 iniciou com 12:03:59.860802\n",
            "Treinamento: batch 214 terminou com 12:04:00.005692\n",
            " 215/1875 [==>...........................] - ETA: 4:05 - loss: 1.7091 - accuracy: 0.2955Treinamento: batch 215 iniciou com 12:04:00.008328\n",
            "Treinamento: batch 215 terminou com 12:04:00.156026\n",
            " 216/1875 [==>...........................] - ETA: 4:05 - loss: 1.7074 - accuracy: 0.2964Treinamento: batch 216 iniciou com 12:04:00.157690\n",
            "Treinamento: batch 216 terminou com 12:04:00.312106\n",
            " 217/1875 [==>...........................] - ETA: 4:05 - loss: 1.7071 - accuracy: 0.2967Treinamento: batch 217 iniciou com 12:04:00.315764\n",
            "Treinamento: batch 217 terminou com 12:04:00.461255\n",
            " 218/1875 [==>...........................] - ETA: 4:05 - loss: 1.7044 - accuracy: 0.2977Treinamento: batch 218 iniciou com 12:04:00.463347\n",
            "Treinamento: batch 218 terminou com 12:04:00.613562\n",
            " 219/1875 [==>...........................] - ETA: 4:04 - loss: 1.7031 - accuracy: 0.2988Treinamento: batch 219 iniciou com 12:04:00.619622\n",
            "Treinamento: batch 219 terminou com 12:04:00.773979\n",
            " 220/1875 [==>...........................] - ETA: 4:04 - loss: 1.7008 - accuracy: 0.2994Treinamento: batch 220 iniciou com 12:04:00.776170\n",
            "Treinamento: batch 220 terminou com 12:04:00.919565\n",
            " 221/1875 [==>...........................] - ETA: 4:04 - loss: 1.6990 - accuracy: 0.2998Treinamento: batch 221 iniciou com 12:04:00.921313\n",
            "Treinamento: batch 221 terminou com 12:04:01.070354\n",
            " 222/1875 [==>...........................] - ETA: 4:04 - loss: 1.6970 - accuracy: 0.3003Treinamento: batch 222 iniciou com 12:04:01.072140\n",
            "Treinamento: batch 222 terminou com 12:04:01.236873\n",
            " 223/1875 [==>...........................] - ETA: 4:04 - loss: 1.6945 - accuracy: 0.3013Treinamento: batch 223 iniciou com 12:04:01.239049\n",
            "Treinamento: batch 223 terminou com 12:04:01.386724\n",
            " 224/1875 [==>...........................] - ETA: 4:04 - loss: 1.6930 - accuracy: 0.3018Treinamento: batch 224 iniciou com 12:04:01.388466\n",
            "Treinamento: batch 224 terminou com 12:04:01.536242\n",
            " 225/1875 [==>...........................] - ETA: 4:04 - loss: 1.6914 - accuracy: 0.3024Treinamento: batch 225 iniciou com 12:04:01.538142\n",
            "Treinamento: batch 225 terminou com 12:04:01.688050\n",
            " 226/1875 [==>...........................] - ETA: 4:04 - loss: 1.6892 - accuracy: 0.3035Treinamento: batch 226 iniciou com 12:04:01.690838\n",
            "Treinamento: batch 226 terminou com 12:04:01.832288\n",
            " 227/1875 [==>...........................] - ETA: 4:04 - loss: 1.6871 - accuracy: 0.3042Treinamento: batch 227 iniciou com 12:04:01.837559\n",
            "Treinamento: batch 227 terminou com 12:04:01.978529\n",
            " 228/1875 [==>...........................] - ETA: 4:03 - loss: 1.6851 - accuracy: 0.3050Treinamento: batch 228 iniciou com 12:04:01.979887\n",
            "Treinamento: batch 228 terminou com 12:04:02.127228\n",
            " 229/1875 [==>...........................] - ETA: 4:03 - loss: 1.6821 - accuracy: 0.3058Treinamento: batch 229 iniciou com 12:04:02.128495\n",
            "Treinamento: batch 229 terminou com 12:04:02.273664\n",
            " 230/1875 [==>...........................] - ETA: 4:03 - loss: 1.6808 - accuracy: 0.3063Treinamento: batch 230 iniciou com 12:04:02.275164\n",
            "Treinamento: batch 230 terminou com 12:04:02.413041\n",
            " 231/1875 [==>...........................] - ETA: 4:03 - loss: 1.6783 - accuracy: 0.3075Treinamento: batch 231 iniciou com 12:04:02.414547\n",
            "Treinamento: batch 231 terminou com 12:04:02.553152\n",
            " 232/1875 [==>...........................] - ETA: 4:03 - loss: 1.6772 - accuracy: 0.3078Treinamento: batch 232 iniciou com 12:04:02.554614\n",
            "Treinamento: batch 232 terminou com 12:04:02.688789\n",
            " 233/1875 [==>...........................] - ETA: 4:02 - loss: 1.6757 - accuracy: 0.3081Treinamento: batch 233 iniciou com 12:04:02.691325\n",
            "Treinamento: batch 233 terminou com 12:04:02.835312\n",
            " 234/1875 [==>...........................] - ETA: 4:02 - loss: 1.6734 - accuracy: 0.3089Treinamento: batch 234 iniciou com 12:04:02.837007\n",
            "Treinamento: batch 234 terminou com 12:04:02.990542\n",
            " 235/1875 [==>...........................] - ETA: 4:02 - loss: 1.6716 - accuracy: 0.3098Treinamento: batch 235 iniciou com 12:04:02.992416\n",
            "Treinamento: batch 235 terminou com 12:04:03.140070\n",
            " 236/1875 [==>...........................] - ETA: 4:02 - loss: 1.6697 - accuracy: 0.3105Treinamento: batch 236 iniciou com 12:04:03.142376\n",
            "Treinamento: batch 236 terminou com 12:04:03.291360\n",
            " 237/1875 [==>...........................] - ETA: 4:02 - loss: 1.6682 - accuracy: 0.3112Treinamento: batch 237 iniciou com 12:04:03.293086\n",
            "Treinamento: batch 237 terminou com 12:04:03.439010\n",
            " 238/1875 [==>...........................] - ETA: 4:02 - loss: 1.6659 - accuracy: 0.3124Treinamento: batch 238 iniciou com 12:04:03.440709\n",
            "Treinamento: batch 238 terminou com 12:04:03.586852\n",
            " 239/1875 [==>...........................] - ETA: 4:02 - loss: 1.6639 - accuracy: 0.3134Treinamento: batch 239 iniciou com 12:04:03.588555\n",
            "Treinamento: batch 239 terminou com 12:04:03.731332\n",
            " 240/1875 [==>...........................] - ETA: 4:01 - loss: 1.6620 - accuracy: 0.3143Treinamento: batch 240 iniciou com 12:04:03.733020\n",
            "Treinamento: batch 240 terminou com 12:04:03.870974\n",
            " 241/1875 [==>...........................] - ETA: 4:01 - loss: 1.6591 - accuracy: 0.3152Treinamento: batch 241 iniciou com 12:04:03.872532\n",
            "Treinamento: batch 241 terminou com 12:04:04.015543\n",
            " 242/1875 [==>...........................] - ETA: 4:01 - loss: 1.6586 - accuracy: 0.3155Treinamento: batch 242 iniciou com 12:04:04.017322\n",
            "Treinamento: batch 242 terminou com 12:04:04.161370\n",
            " 243/1875 [==>...........................] - ETA: 4:01 - loss: 1.6567 - accuracy: 0.3160Treinamento: batch 243 iniciou com 12:04:04.163090\n",
            "Treinamento: batch 243 terminou com 12:04:04.315191\n",
            " 244/1875 [==>...........................] - ETA: 4:01 - loss: 1.6544 - accuracy: 0.3171Treinamento: batch 244 iniciou com 12:04:04.316937\n",
            "Treinamento: batch 244 terminou com 12:04:04.468832\n",
            " 245/1875 [==>...........................] - ETA: 4:01 - loss: 1.6521 - accuracy: 0.3181Treinamento: batch 245 iniciou com 12:04:04.470678\n",
            "Treinamento: batch 245 terminou com 12:04:04.615411\n",
            " 246/1875 [==>...........................] - ETA: 4:01 - loss: 1.6503 - accuracy: 0.3186Treinamento: batch 246 iniciou com 12:04:04.617170\n",
            "Treinamento: batch 246 terminou com 12:04:04.761996\n",
            " 247/1875 [==>...........................] - ETA: 4:00 - loss: 1.6490 - accuracy: 0.3193Treinamento: batch 247 iniciou com 12:04:04.763744\n",
            "Treinamento: batch 247 terminou com 12:04:04.907480\n",
            " 248/1875 [==>...........................] - ETA: 4:00 - loss: 1.6494 - accuracy: 0.3192Treinamento: batch 248 iniciou com 12:04:04.909184\n",
            "Treinamento: batch 248 terminou com 12:04:05.053543\n",
            " 249/1875 [==>...........................] - ETA: 4:00 - loss: 1.6476 - accuracy: 0.3195Treinamento: batch 249 iniciou com 12:04:05.055550\n",
            "Treinamento: batch 249 terminou com 12:04:05.204485\n",
            " 250/1875 [===>..........................] - ETA: 4:00 - loss: 1.6463 - accuracy: 0.3198Treinamento: batch 250 iniciou com 12:04:05.206539\n",
            "Treinamento: batch 250 terminou com 12:04:05.359719\n",
            " 251/1875 [===>..........................] - ETA: 4:00 - loss: 1.6450 - accuracy: 0.3203Treinamento: batch 251 iniciou com 12:04:05.361516\n",
            "Treinamento: batch 251 terminou com 12:04:05.506332\n",
            " 252/1875 [===>..........................] - ETA: 4:00 - loss: 1.6422 - accuracy: 0.3216Treinamento: batch 252 iniciou com 12:04:05.508042\n",
            "Treinamento: batch 252 terminou com 12:04:05.656892\n",
            " 253/1875 [===>..........................] - ETA: 4:00 - loss: 1.6403 - accuracy: 0.3220Treinamento: batch 253 iniciou com 12:04:05.658449\n",
            "Treinamento: batch 253 terminou com 12:04:05.800803\n",
            " 254/1875 [===>..........................] - ETA: 3:59 - loss: 1.6380 - accuracy: 0.3231Treinamento: batch 254 iniciou com 12:04:05.802436\n",
            "Treinamento: batch 254 terminou com 12:04:05.944117\n",
            " 255/1875 [===>..........................] - ETA: 3:59 - loss: 1.6361 - accuracy: 0.3238Treinamento: batch 255 iniciou com 12:04:05.945895\n",
            "Treinamento: batch 255 terminou com 12:04:06.092117\n",
            " 256/1875 [===>..........................] - ETA: 3:59 - loss: 1.6348 - accuracy: 0.3243Treinamento: batch 256 iniciou com 12:04:06.093848\n",
            "Treinamento: batch 256 terminou com 12:04:06.232058\n",
            " 257/1875 [===>..........................] - ETA: 3:59 - loss: 1.6341 - accuracy: 0.3248Treinamento: batch 257 iniciou com 12:04:06.233851\n",
            "Treinamento: batch 257 terminou com 12:04:06.388247\n",
            " 258/1875 [===>..........................] - ETA: 3:59 - loss: 1.6317 - accuracy: 0.3255Treinamento: batch 258 iniciou com 12:04:06.390053\n",
            "Treinamento: batch 258 terminou com 12:04:06.536956\n",
            " 259/1875 [===>..........................] - ETA: 3:59 - loss: 1.6316 - accuracy: 0.3254Treinamento: batch 259 iniciou com 12:04:06.538657\n",
            "Treinamento: batch 259 terminou com 12:04:06.683850\n",
            " 260/1875 [===>..........................] - ETA: 3:58 - loss: 1.6298 - accuracy: 0.3261Treinamento: batch 260 iniciou com 12:04:06.685576\n",
            "Treinamento: batch 260 terminou com 12:04:06.828354\n",
            " 261/1875 [===>..........................] - ETA: 3:58 - loss: 1.6285 - accuracy: 0.3267Treinamento: batch 261 iniciou com 12:04:06.830521\n",
            "Treinamento: batch 261 terminou com 12:04:06.978450\n",
            " 262/1875 [===>..........................] - ETA: 3:58 - loss: 1.6275 - accuracy: 0.3273Treinamento: batch 262 iniciou com 12:04:06.980165\n",
            "Treinamento: batch 262 terminou com 12:04:07.127913\n",
            " 263/1875 [===>..........................] - ETA: 3:58 - loss: 1.6256 - accuracy: 0.3277Treinamento: batch 263 iniciou com 12:04:07.130246\n",
            "Treinamento: batch 263 terminou com 12:04:07.274031\n",
            " 264/1875 [===>..........................] - ETA: 3:58 - loss: 1.6251 - accuracy: 0.3282Treinamento: batch 264 iniciou com 12:04:07.275667\n",
            "Treinamento: batch 264 terminou com 12:04:07.433442\n",
            " 265/1875 [===>..........................] - ETA: 3:58 - loss: 1.6237 - accuracy: 0.3291Treinamento: batch 265 iniciou com 12:04:07.435187\n",
            "Treinamento: batch 265 terminou com 12:04:07.573232\n",
            " 266/1875 [===>..........................] - ETA: 3:58 - loss: 1.6227 - accuracy: 0.3299Treinamento: batch 266 iniciou com 12:04:07.574857\n",
            "Treinamento: batch 266 terminou com 12:04:07.715607\n",
            " 267/1875 [===>..........................] - ETA: 3:57 - loss: 1.6213 - accuracy: 0.3302Treinamento: batch 267 iniciou com 12:04:07.717449\n",
            "Treinamento: batch 267 terminou com 12:04:07.858099\n",
            " 268/1875 [===>..........................] - ETA: 3:57 - loss: 1.6189 - accuracy: 0.3309Treinamento: batch 268 iniciou com 12:04:07.859864\n",
            "Treinamento: batch 268 terminou com 12:04:07.999453\n",
            " 269/1875 [===>..........................] - ETA: 3:57 - loss: 1.6168 - accuracy: 0.3316Treinamento: batch 269 iniciou com 12:04:08.001817\n",
            "Treinamento: batch 269 terminou com 12:04:08.144731\n",
            " 270/1875 [===>..........................] - ETA: 3:57 - loss: 1.6152 - accuracy: 0.3323Treinamento: batch 270 iniciou com 12:04:08.146476\n",
            "Treinamento: batch 270 terminou com 12:04:08.288973\n",
            " 271/1875 [===>..........................] - ETA: 3:57 - loss: 1.6130 - accuracy: 0.3335Treinamento: batch 271 iniciou com 12:04:08.290668\n",
            "Treinamento: batch 271 terminou com 12:04:08.439530\n",
            " 272/1875 [===>..........................] - ETA: 3:57 - loss: 1.6120 - accuracy: 0.3340Treinamento: batch 272 iniciou com 12:04:08.441249\n",
            "Treinamento: batch 272 terminou com 12:04:08.584880\n",
            " 273/1875 [===>..........................] - ETA: 3:56 - loss: 1.6105 - accuracy: 0.3345Treinamento: batch 273 iniciou com 12:04:08.586518\n",
            "Treinamento: batch 273 terminou com 12:04:08.725384\n",
            " 274/1875 [===>..........................] - ETA: 3:56 - loss: 1.6093 - accuracy: 0.3351Treinamento: batch 274 iniciou com 12:04:08.727002\n",
            "Treinamento: batch 274 terminou com 12:04:08.870822\n",
            " 275/1875 [===>..........................] - ETA: 3:56 - loss: 1.6081 - accuracy: 0.3353Treinamento: batch 275 iniciou com 12:04:08.872470\n",
            "Treinamento: batch 275 terminou com 12:04:09.013521\n",
            " 276/1875 [===>..........................] - ETA: 3:56 - loss: 1.6069 - accuracy: 0.3359Treinamento: batch 276 iniciou com 12:04:09.015159\n",
            "Treinamento: batch 276 terminou com 12:04:09.159158\n",
            " 277/1875 [===>..........................] - ETA: 3:56 - loss: 1.6056 - accuracy: 0.3363Treinamento: batch 277 iniciou com 12:04:09.161012\n",
            "Treinamento: batch 277 terminou com 12:04:09.320461\n",
            " 278/1875 [===>..........................] - ETA: 3:56 - loss: 1.6054 - accuracy: 0.3366Treinamento: batch 278 iniciou com 12:04:09.323099\n",
            "Treinamento: batch 278 terminou com 12:04:09.468046\n",
            " 279/1875 [===>..........................] - ETA: 3:55 - loss: 1.6045 - accuracy: 0.3370Treinamento: batch 279 iniciou com 12:04:09.469806\n",
            "Treinamento: batch 279 terminou com 12:04:09.623064\n",
            " 280/1875 [===>..........................] - ETA: 3:55 - loss: 1.6036 - accuracy: 0.3375Treinamento: batch 280 iniciou com 12:04:09.625531\n",
            "Treinamento: batch 280 terminou com 12:04:09.770577\n",
            " 281/1875 [===>..........................] - ETA: 3:55 - loss: 1.6015 - accuracy: 0.3384Treinamento: batch 281 iniciou com 12:04:09.772313\n",
            "Treinamento: batch 281 terminou com 12:04:09.911838\n",
            " 282/1875 [===>..........................] - ETA: 3:55 - loss: 1.6008 - accuracy: 0.3390Treinamento: batch 282 iniciou com 12:04:09.913511\n",
            "Treinamento: batch 282 terminou com 12:04:10.057613\n",
            " 283/1875 [===>..........................] - ETA: 3:55 - loss: 1.5996 - accuracy: 0.3393Treinamento: batch 283 iniciou com 12:04:10.059393\n",
            "Treinamento: batch 283 terminou com 12:04:10.205549\n",
            " 284/1875 [===>..........................] - ETA: 3:55 - loss: 1.5985 - accuracy: 0.3397Treinamento: batch 284 iniciou com 12:04:10.207154\n",
            "Treinamento: batch 284 terminou com 12:04:10.352227\n",
            " 285/1875 [===>..........................] - ETA: 3:55 - loss: 1.5980 - accuracy: 0.3398Treinamento: batch 285 iniciou com 12:04:10.354596\n",
            "Treinamento: batch 285 terminou com 12:04:10.509005\n",
            " 286/1875 [===>..........................] - ETA: 3:54 - loss: 1.5966 - accuracy: 0.3401Treinamento: batch 286 iniciou com 12:04:10.510777\n",
            "Treinamento: batch 286 terminou com 12:04:10.655926\n",
            " 287/1875 [===>..........................] - ETA: 3:54 - loss: 1.5946 - accuracy: 0.3411Treinamento: batch 287 iniciou com 12:04:10.657741\n",
            "Treinamento: batch 287 terminou com 12:04:10.803851\n",
            " 288/1875 [===>..........................] - ETA: 3:54 - loss: 1.5942 - accuracy: 0.3417Treinamento: batch 288 iniciou com 12:04:10.805770\n",
            "Treinamento: batch 288 terminou com 12:04:10.957817\n",
            " 289/1875 [===>..........................] - ETA: 3:54 - loss: 1.5920 - accuracy: 0.3428Treinamento: batch 289 iniciou com 12:04:10.959473\n",
            "Treinamento: batch 289 terminou com 12:04:11.106089\n",
            " 290/1875 [===>..........................] - ETA: 3:54 - loss: 1.5914 - accuracy: 0.3430Treinamento: batch 290 iniciou com 12:04:11.108132\n",
            "Treinamento: batch 290 terminou com 12:04:11.250480\n",
            " 291/1875 [===>..........................] - ETA: 3:54 - loss: 1.5891 - accuracy: 0.3441Treinamento: batch 291 iniciou com 12:04:11.253282\n",
            "Treinamento: batch 291 terminou com 12:04:11.407372\n",
            " 292/1875 [===>..........................] - ETA: 3:54 - loss: 1.5872 - accuracy: 0.3448Treinamento: batch 292 iniciou com 12:04:11.409412\n",
            "Treinamento: batch 292 terminou com 12:04:11.551521\n",
            " 293/1875 [===>..........................] - ETA: 3:53 - loss: 1.5857 - accuracy: 0.3453Treinamento: batch 293 iniciou com 12:04:11.553597\n",
            "Treinamento: batch 293 terminou com 12:04:11.703974\n",
            " 294/1875 [===>..........................] - ETA: 3:53 - loss: 1.5845 - accuracy: 0.3460Treinamento: batch 294 iniciou com 12:04:11.705726\n",
            "Treinamento: batch 294 terminou com 12:04:11.852906\n",
            " 295/1875 [===>..........................] - ETA: 3:53 - loss: 1.5828 - accuracy: 0.3464Treinamento: batch 295 iniciou com 12:04:11.854702\n",
            "Treinamento: batch 295 terminou com 12:04:11.998590\n",
            " 296/1875 [===>..........................] - ETA: 3:53 - loss: 1.5809 - accuracy: 0.3472Treinamento: batch 296 iniciou com 12:04:12.001251\n",
            "Treinamento: batch 296 terminou com 12:04:12.149210\n",
            " 297/1875 [===>..........................] - ETA: 3:53 - loss: 1.5803 - accuracy: 0.3473Treinamento: batch 297 iniciou com 12:04:12.150917\n",
            "Treinamento: batch 297 terminou com 12:04:12.301710\n",
            " 298/1875 [===>..........................] - ETA: 3:53 - loss: 1.5801 - accuracy: 0.3468Treinamento: batch 298 iniciou com 12:04:12.303500\n",
            "Treinamento: batch 298 terminou com 12:04:12.483168\n",
            " 299/1875 [===>..........................] - ETA: 3:53 - loss: 1.5789 - accuracy: 0.3473Treinamento: batch 299 iniciou com 12:04:12.484929\n",
            "Treinamento: batch 299 terminou com 12:04:12.622992\n",
            " 300/1875 [===>..........................] - ETA: 3:53 - loss: 1.5774 - accuracy: 0.3481Treinamento: batch 300 iniciou com 12:04:12.624678\n",
            "Treinamento: batch 300 terminou com 12:04:12.771888\n",
            " 301/1875 [===>..........................] - ETA: 3:52 - loss: 1.5749 - accuracy: 0.3493Treinamento: batch 301 iniciou com 12:04:12.773760\n",
            "Treinamento: batch 301 terminou com 12:04:12.915702\n",
            " 302/1875 [===>..........................] - ETA: 3:52 - loss: 1.5753 - accuracy: 0.3490Treinamento: batch 302 iniciou com 12:04:12.917825\n",
            "Treinamento: batch 302 terminou com 12:04:13.062860\n",
            " 303/1875 [===>..........................] - ETA: 3:52 - loss: 1.5745 - accuracy: 0.3492Treinamento: batch 303 iniciou com 12:04:13.064523\n",
            "Treinamento: batch 303 terminou com 12:04:13.203441\n",
            " 304/1875 [===>..........................] - ETA: 3:52 - loss: 1.5730 - accuracy: 0.3496Treinamento: batch 304 iniciou com 12:04:13.205783\n",
            "Treinamento: batch 304 terminou com 12:04:13.348010\n",
            " 305/1875 [===>..........................] - ETA: 3:52 - loss: 1.5720 - accuracy: 0.3500Treinamento: batch 305 iniciou com 12:04:13.349700\n",
            "Treinamento: batch 305 terminou com 12:04:13.497991\n",
            " 306/1875 [===>..........................] - ETA: 3:52 - loss: 1.5716 - accuracy: 0.3501Treinamento: batch 306 iniciou com 12:04:13.499735\n",
            "Treinamento: batch 306 terminou com 12:04:13.646146\n",
            " 307/1875 [===>..........................] - ETA: 3:52 - loss: 1.5703 - accuracy: 0.3506Treinamento: batch 307 iniciou com 12:04:13.647829\n",
            "Treinamento: batch 307 terminou com 12:04:13.789803\n",
            " 308/1875 [===>..........................] - ETA: 3:51 - loss: 1.5688 - accuracy: 0.3515Treinamento: batch 308 iniciou com 12:04:13.791503\n",
            "Treinamento: batch 308 terminou com 12:04:13.938711\n",
            " 309/1875 [===>..........................] - ETA: 3:51 - loss: 1.5674 - accuracy: 0.3520Treinamento: batch 309 iniciou com 12:04:13.940612\n",
            "Treinamento: batch 309 terminou com 12:04:14.082082\n",
            " 310/1875 [===>..........................] - ETA: 3:51 - loss: 1.5668 - accuracy: 0.3521Treinamento: batch 310 iniciou com 12:04:14.083844\n",
            "Treinamento: batch 310 terminou com 12:04:14.232598\n",
            " 311/1875 [===>..........................] - ETA: 3:51 - loss: 1.5653 - accuracy: 0.3524Treinamento: batch 311 iniciou com 12:04:14.234360\n",
            "Treinamento: batch 311 terminou com 12:04:14.374122\n",
            " 312/1875 [===>..........................] - ETA: 3:51 - loss: 1.5642 - accuracy: 0.3529Treinamento: batch 312 iniciou com 12:04:14.375836\n",
            "Treinamento: batch 312 terminou com 12:04:14.528502\n",
            " 313/1875 [====>.........................] - ETA: 3:51 - loss: 1.5629 - accuracy: 0.3533Treinamento: batch 313 iniciou com 12:04:14.530221\n",
            "Treinamento: batch 313 terminou com 12:04:14.668000\n",
            " 314/1875 [====>.........................] - ETA: 3:50 - loss: 1.5613 - accuracy: 0.3540Treinamento: batch 314 iniciou com 12:04:14.669727\n",
            "Treinamento: batch 314 terminou com 12:04:14.811501\n",
            " 315/1875 [====>.........................] - ETA: 3:50 - loss: 1.5620 - accuracy: 0.3540Treinamento: batch 315 iniciou com 12:04:14.813102\n",
            "Treinamento: batch 315 terminou com 12:04:14.955399\n",
            " 316/1875 [====>.........................] - ETA: 3:50 - loss: 1.5615 - accuracy: 0.3543Treinamento: batch 316 iniciou com 12:04:14.957041\n",
            "Treinamento: batch 316 terminou com 12:04:15.102024\n",
            " 317/1875 [====>.........................] - ETA: 3:50 - loss: 1.5603 - accuracy: 0.3547Treinamento: batch 317 iniciou com 12:04:15.103772\n",
            "Treinamento: batch 317 terminou com 12:04:15.252511\n",
            " 318/1875 [====>.........................] - ETA: 3:50 - loss: 1.5594 - accuracy: 0.3550Treinamento: batch 318 iniciou com 12:04:15.254384\n",
            "Treinamento: batch 318 terminou com 12:04:15.399669\n",
            " 319/1875 [====>.........................] - ETA: 3:50 - loss: 1.5586 - accuracy: 0.3553Treinamento: batch 319 iniciou com 12:04:15.401407\n",
            "Treinamento: batch 319 terminou com 12:04:15.566958\n",
            " 320/1875 [====>.........................] - ETA: 3:50 - loss: 1.5576 - accuracy: 0.3556Treinamento: batch 320 iniciou com 12:04:15.568900\n",
            "Treinamento: batch 320 terminou com 12:04:15.710913\n",
            " 321/1875 [====>.........................] - ETA: 3:49 - loss: 1.5568 - accuracy: 0.3557Treinamento: batch 321 iniciou com 12:04:15.712702\n",
            "Treinamento: batch 321 terminou com 12:04:15.852288\n",
            " 322/1875 [====>.........................] - ETA: 3:49 - loss: 1.5563 - accuracy: 0.3559Treinamento: batch 322 iniciou com 12:04:15.854227\n",
            "Treinamento: batch 322 terminou com 12:04:15.998387\n",
            " 323/1875 [====>.........................] - ETA: 3:49 - loss: 1.5559 - accuracy: 0.3557Treinamento: batch 323 iniciou com 12:04:16.000118\n",
            "Treinamento: batch 323 terminou com 12:04:16.138972\n",
            " 324/1875 [====>.........................] - ETA: 3:49 - loss: 1.5556 - accuracy: 0.3559Treinamento: batch 324 iniciou com 12:04:16.140807\n",
            "Treinamento: batch 324 terminou com 12:04:16.289047\n",
            " 325/1875 [====>.........................] - ETA: 3:49 - loss: 1.5545 - accuracy: 0.3560Treinamento: batch 325 iniciou com 12:04:16.290765\n",
            "Treinamento: batch 325 terminou com 12:04:16.435240\n",
            " 326/1875 [====>.........................] - ETA: 3:49 - loss: 1.5536 - accuracy: 0.3565Treinamento: batch 326 iniciou com 12:04:16.437034\n",
            "Treinamento: batch 326 terminou com 12:04:16.585470\n",
            " 327/1875 [====>.........................] - ETA: 3:48 - loss: 1.5527 - accuracy: 0.3567Treinamento: batch 327 iniciou com 12:04:16.587468\n",
            "Treinamento: batch 327 terminou com 12:04:16.730186\n",
            " 328/1875 [====>.........................] - ETA: 3:48 - loss: 1.5514 - accuracy: 0.3571Treinamento: batch 328 iniciou com 12:04:16.731955\n",
            "Treinamento: batch 328 terminou com 12:04:16.871329\n",
            " 329/1875 [====>.........................] - ETA: 3:48 - loss: 1.5513 - accuracy: 0.3571Treinamento: batch 329 iniciou com 12:04:16.873041\n",
            "Treinamento: batch 329 terminou com 12:04:17.013470\n",
            " 330/1875 [====>.........................] - ETA: 3:48 - loss: 1.5498 - accuracy: 0.3580Treinamento: batch 330 iniciou com 12:04:17.015226\n",
            "Treinamento: batch 330 terminou com 12:04:17.168886\n",
            " 331/1875 [====>.........................] - ETA: 3:48 - loss: 1.5485 - accuracy: 0.3590Treinamento: batch 331 iniciou com 12:04:17.170828\n",
            "Treinamento: batch 331 terminou com 12:04:17.312708\n",
            " 332/1875 [====>.........................] - ETA: 3:48 - loss: 1.5477 - accuracy: 0.3590Treinamento: batch 332 iniciou com 12:04:17.314457\n",
            "Treinamento: batch 332 terminou com 12:04:17.454877\n",
            " 333/1875 [====>.........................] - ETA: 3:47 - loss: 1.5462 - accuracy: 0.3599Treinamento: batch 333 iniciou com 12:04:17.456604\n",
            "Treinamento: batch 333 terminou com 12:04:17.607072\n",
            " 334/1875 [====>.........................] - ETA: 3:47 - loss: 1.5452 - accuracy: 0.3602Treinamento: batch 334 iniciou com 12:04:17.611395\n",
            "Treinamento: batch 334 terminou com 12:04:17.753486\n",
            " 335/1875 [====>.........................] - ETA: 3:47 - loss: 1.5447 - accuracy: 0.3606Treinamento: batch 335 iniciou com 12:04:17.755426\n",
            "Treinamento: batch 335 terminou com 12:04:17.895141\n",
            " 336/1875 [====>.........................] - ETA: 3:47 - loss: 1.5431 - accuracy: 0.3613Treinamento: batch 336 iniciou com 12:04:17.896740\n",
            "Treinamento: batch 336 terminou com 12:04:18.044964\n",
            " 337/1875 [====>.........................] - ETA: 3:47 - loss: 1.5413 - accuracy: 0.3622Treinamento: batch 337 iniciou com 12:04:18.047032\n",
            "Treinamento: batch 337 terminou com 12:04:18.183997\n",
            " 338/1875 [====>.........................] - ETA: 3:47 - loss: 1.5407 - accuracy: 0.3624Treinamento: batch 338 iniciou com 12:04:18.186293\n",
            "Treinamento: batch 338 terminou com 12:04:18.336028\n",
            " 339/1875 [====>.........................] - ETA: 3:47 - loss: 1.5402 - accuracy: 0.3628Treinamento: batch 339 iniciou com 12:04:18.337790\n",
            "Treinamento: batch 339 terminou com 12:04:18.479893\n",
            " 340/1875 [====>.........................] - ETA: 3:46 - loss: 1.5390 - accuracy: 0.3635Treinamento: batch 340 iniciou com 12:04:18.481622\n",
            "Treinamento: batch 340 terminou com 12:04:18.635527\n",
            " 341/1875 [====>.........................] - ETA: 3:46 - loss: 1.5381 - accuracy: 0.3640Treinamento: batch 341 iniciou com 12:04:18.637257\n",
            "Treinamento: batch 341 terminou com 12:04:18.781730\n",
            " 342/1875 [====>.........................] - ETA: 3:46 - loss: 1.5368 - accuracy: 0.3645Treinamento: batch 342 iniciou com 12:04:18.783450\n",
            "Treinamento: batch 342 terminou com 12:04:18.921995\n",
            " 343/1875 [====>.........................] - ETA: 3:46 - loss: 1.5358 - accuracy: 0.3649Treinamento: batch 343 iniciou com 12:04:18.923718\n",
            "Treinamento: batch 343 terminou com 12:04:19.073255\n",
            " 344/1875 [====>.........................] - ETA: 3:46 - loss: 1.5345 - accuracy: 0.3656Treinamento: batch 344 iniciou com 12:04:19.075127\n",
            "Treinamento: batch 344 terminou com 12:04:19.219556\n",
            " 345/1875 [====>.........................] - ETA: 3:46 - loss: 1.5340 - accuracy: 0.3656Treinamento: batch 345 iniciou com 12:04:19.221962\n",
            "Treinamento: batch 345 terminou com 12:04:19.365265\n",
            " 346/1875 [====>.........................] - ETA: 3:46 - loss: 1.5333 - accuracy: 0.3658Treinamento: batch 346 iniciou com 12:04:19.367003\n",
            "Treinamento: batch 346 terminou com 12:04:19.509145\n",
            " 347/1875 [====>.........................] - ETA: 3:45 - loss: 1.5324 - accuracy: 0.3663Treinamento: batch 347 iniciou com 12:04:19.510845\n",
            "Treinamento: batch 347 terminou com 12:04:19.666695\n",
            " 348/1875 [====>.........................] - ETA: 3:45 - loss: 1.5320 - accuracy: 0.3666Treinamento: batch 348 iniciou com 12:04:19.668414\n",
            "Treinamento: batch 348 terminou com 12:04:19.811382\n",
            " 349/1875 [====>.........................] - ETA: 3:45 - loss: 1.5309 - accuracy: 0.3672Treinamento: batch 349 iniciou com 12:04:19.813103\n",
            "Treinamento: batch 349 terminou com 12:04:19.953739\n",
            " 350/1875 [====>.........................] - ETA: 3:45 - loss: 1.5295 - accuracy: 0.3679Treinamento: batch 350 iniciou com 12:04:19.955474\n",
            "Treinamento: batch 350 terminou com 12:04:20.101394\n",
            " 351/1875 [====>.........................] - ETA: 3:45 - loss: 1.5288 - accuracy: 0.3681Treinamento: batch 351 iniciou com 12:04:20.103145\n",
            "Treinamento: batch 351 terminou com 12:04:20.244793\n",
            " 352/1875 [====>.........................] - ETA: 3:45 - loss: 1.5283 - accuracy: 0.3684Treinamento: batch 352 iniciou com 12:04:20.246854\n",
            "Treinamento: batch 352 terminou com 12:04:20.393559\n",
            " 353/1875 [====>.........................] - ETA: 3:44 - loss: 1.5264 - accuracy: 0.3693Treinamento: batch 353 iniciou com 12:04:20.395247\n",
            "Treinamento: batch 353 terminou com 12:04:20.541493\n",
            " 354/1875 [====>.........................] - ETA: 3:44 - loss: 1.5248 - accuracy: 0.3700Treinamento: batch 354 iniciou com 12:04:20.543440\n",
            "Treinamento: batch 354 terminou com 12:04:20.693553\n",
            " 355/1875 [====>.........................] - ETA: 3:44 - loss: 1.5240 - accuracy: 0.3702Treinamento: batch 355 iniciou com 12:04:20.695299\n",
            "Treinamento: batch 355 terminou com 12:04:20.842804\n",
            " 356/1875 [====>.........................] - ETA: 3:44 - loss: 1.5237 - accuracy: 0.3705Treinamento: batch 356 iniciou com 12:04:20.844813\n",
            "Treinamento: batch 356 terminou com 12:04:20.985334\n",
            " 357/1875 [====>.........................] - ETA: 3:44 - loss: 1.5220 - accuracy: 0.3712Treinamento: batch 357 iniciou com 12:04:20.987847\n",
            "Treinamento: batch 357 terminou com 12:04:21.135859\n",
            " 358/1875 [====>.........................] - ETA: 3:44 - loss: 1.5213 - accuracy: 0.3715Treinamento: batch 358 iniciou com 12:04:21.138010\n",
            "Treinamento: batch 358 terminou com 12:04:21.284204\n",
            " 359/1875 [====>.........................] - ETA: 3:44 - loss: 1.5202 - accuracy: 0.3720Treinamento: batch 359 iniciou com 12:04:21.285989\n",
            "Treinamento: batch 359 terminou com 12:04:21.426058\n",
            " 360/1875 [====>.........................] - ETA: 3:43 - loss: 1.5193 - accuracy: 0.3724Treinamento: batch 360 iniciou com 12:04:21.427732\n",
            "Treinamento: batch 360 terminou com 12:04:21.575896\n",
            " 361/1875 [====>.........................] - ETA: 3:43 - loss: 1.5174 - accuracy: 0.3734Treinamento: batch 361 iniciou com 12:04:21.577555\n",
            "Treinamento: batch 361 terminou com 12:04:21.726995\n",
            " 362/1875 [====>.........................] - ETA: 3:43 - loss: 1.5167 - accuracy: 0.3736Treinamento: batch 362 iniciou com 12:04:21.729622\n",
            "Treinamento: batch 362 terminou com 12:04:21.872918\n",
            " 363/1875 [====>.........................] - ETA: 3:43 - loss: 1.5159 - accuracy: 0.3740Treinamento: batch 363 iniciou com 12:04:21.874728\n",
            "Treinamento: batch 363 terminou com 12:04:22.020559\n",
            " 364/1875 [====>.........................] - ETA: 3:43 - loss: 1.5152 - accuracy: 0.3742Treinamento: batch 364 iniciou com 12:04:22.023127\n",
            "Treinamento: batch 364 terminou com 12:04:22.165974\n",
            " 365/1875 [====>.........................] - ETA: 3:43 - loss: 1.5153 - accuracy: 0.3742Treinamento: batch 365 iniciou com 12:04:22.167795\n",
            "Treinamento: batch 365 terminou com 12:04:22.327438\n",
            " 366/1875 [====>.........................] - ETA: 3:43 - loss: 1.5149 - accuracy: 0.3745Treinamento: batch 366 iniciou com 12:04:22.329178\n",
            "Treinamento: batch 366 terminou com 12:04:22.474519\n",
            " 367/1875 [====>.........................] - ETA: 3:42 - loss: 1.5134 - accuracy: 0.3751Treinamento: batch 367 iniciou com 12:04:22.476304\n",
            "Treinamento: batch 367 terminou com 12:04:22.618120\n",
            " 368/1875 [====>.........................] - ETA: 3:42 - loss: 1.5130 - accuracy: 0.3750Treinamento: batch 368 iniciou com 12:04:22.619823\n",
            "Treinamento: batch 368 terminou com 12:04:22.774954\n",
            " 369/1875 [====>.........................] - ETA: 3:42 - loss: 1.5119 - accuracy: 0.3752Treinamento: batch 369 iniciou com 12:04:22.776778\n",
            "Treinamento: batch 369 terminou com 12:04:22.948843\n",
            " 370/1875 [====>.........................] - ETA: 3:42 - loss: 1.5112 - accuracy: 0.3757Treinamento: batch 370 iniciou com 12:04:22.953343\n",
            "Treinamento: batch 370 terminou com 12:04:23.094560\n",
            " 371/1875 [====>.........................] - ETA: 3:42 - loss: 1.5099 - accuracy: 0.3763Treinamento: batch 371 iniciou com 12:04:23.096323\n",
            "Treinamento: batch 371 terminou com 12:04:23.239084\n",
            " 372/1875 [====>.........................] - ETA: 3:42 - loss: 1.5091 - accuracy: 0.3763Treinamento: batch 372 iniciou com 12:04:23.241697\n",
            "Treinamento: batch 372 terminou com 12:04:23.393592\n",
            " 373/1875 [====>.........................] - ETA: 3:42 - loss: 1.5081 - accuracy: 0.3768Treinamento: batch 373 iniciou com 12:04:23.395754\n",
            "Treinamento: batch 373 terminou com 12:04:23.542973\n",
            " 374/1875 [====>.........................] - ETA: 3:42 - loss: 1.5070 - accuracy: 0.3771Treinamento: batch 374 iniciou com 12:04:23.544704\n",
            "Treinamento: batch 374 terminou com 12:04:23.701780\n",
            " 375/1875 [=====>........................] - ETA: 3:41 - loss: 1.5056 - accuracy: 0.3775Treinamento: batch 375 iniciou com 12:04:23.705720\n",
            "Treinamento: batch 375 terminou com 12:04:23.855589\n",
            " 376/1875 [=====>........................] - ETA: 3:41 - loss: 1.5048 - accuracy: 0.3782Treinamento: batch 376 iniciou com 12:04:23.857200\n",
            "Treinamento: batch 376 terminou com 12:04:23.996698\n",
            " 377/1875 [=====>........................] - ETA: 3:41 - loss: 1.5042 - accuracy: 0.3786Treinamento: batch 377 iniciou com 12:04:23.998817\n",
            "Treinamento: batch 377 terminou com 12:04:24.139374\n",
            " 378/1875 [=====>........................] - ETA: 3:41 - loss: 1.5028 - accuracy: 0.3793Treinamento: batch 378 iniciou com 12:04:24.141179\n",
            "Treinamento: batch 378 terminou com 12:04:24.290427\n",
            " 379/1875 [=====>........................] - ETA: 3:41 - loss: 1.5019 - accuracy: 0.3799Treinamento: batch 379 iniciou com 12:04:24.292213\n",
            "Treinamento: batch 379 terminou com 12:04:24.441031\n",
            " 380/1875 [=====>........................] - ETA: 3:41 - loss: 1.5010 - accuracy: 0.3803Treinamento: batch 380 iniciou com 12:04:24.443013\n",
            "Treinamento: batch 380 terminou com 12:04:24.586792\n",
            " 381/1875 [=====>........................] - ETA: 3:41 - loss: 1.5002 - accuracy: 0.3806Treinamento: batch 381 iniciou com 12:04:24.588570\n",
            "Treinamento: batch 381 terminou com 12:04:24.754288\n",
            " 382/1875 [=====>........................] - ETA: 3:40 - loss: 1.4987 - accuracy: 0.3814Treinamento: batch 382 iniciou com 12:04:24.756076\n",
            "Treinamento: batch 382 terminou com 12:04:24.897715\n",
            " 383/1875 [=====>........................] - ETA: 3:40 - loss: 1.4976 - accuracy: 0.3819Treinamento: batch 383 iniciou com 12:04:24.900117\n",
            "Treinamento: batch 383 terminou com 12:04:25.044608\n",
            " 384/1875 [=====>........................] - ETA: 3:40 - loss: 1.4971 - accuracy: 0.3822Treinamento: batch 384 iniciou com 12:04:25.046428\n",
            "Treinamento: batch 384 terminou com 12:04:25.186796\n",
            " 385/1875 [=====>........................] - ETA: 3:40 - loss: 1.4964 - accuracy: 0.3825Treinamento: batch 385 iniciou com 12:04:25.188543\n",
            "Treinamento: batch 385 terminou com 12:04:25.331993\n",
            " 386/1875 [=====>........................] - ETA: 3:40 - loss: 1.4952 - accuracy: 0.3830Treinamento: batch 386 iniciou com 12:04:25.333719\n",
            "Treinamento: batch 386 terminou com 12:04:25.479245\n",
            " 387/1875 [=====>........................] - ETA: 3:40 - loss: 1.4945 - accuracy: 0.3833Treinamento: batch 387 iniciou com 12:04:25.480868\n",
            "Treinamento: batch 387 terminou com 12:04:25.625367\n",
            " 388/1875 [=====>........................] - ETA: 3:40 - loss: 1.4932 - accuracy: 0.3839Treinamento: batch 388 iniciou com 12:04:25.627121\n",
            "Treinamento: batch 388 terminou com 12:04:25.784869\n",
            " 389/1875 [=====>........................] - ETA: 3:39 - loss: 1.4921 - accuracy: 0.3843Treinamento: batch 389 iniciou com 12:04:25.786583\n",
            "Treinamento: batch 389 terminou com 12:04:25.927971\n",
            " 390/1875 [=====>........................] - ETA: 3:39 - loss: 1.4913 - accuracy: 0.3850Treinamento: batch 390 iniciou com 12:04:25.929702\n",
            "Treinamento: batch 390 terminou com 12:04:26.082234\n",
            " 391/1875 [=====>........................] - ETA: 3:39 - loss: 1.4908 - accuracy: 0.3857Treinamento: batch 391 iniciou com 12:04:26.084076\n",
            "Treinamento: batch 391 terminou com 12:04:26.233830\n",
            " 392/1875 [=====>........................] - ETA: 3:39 - loss: 1.4896 - accuracy: 0.3861Treinamento: batch 392 iniciou com 12:04:26.235482\n",
            "Treinamento: batch 392 terminou com 12:04:26.377068\n",
            " 393/1875 [=====>........................] - ETA: 3:39 - loss: 1.4885 - accuracy: 0.3868Treinamento: batch 393 iniciou com 12:04:26.378785\n",
            "Treinamento: batch 393 terminou com 12:04:26.527599\n",
            " 394/1875 [=====>........................] - ETA: 3:39 - loss: 1.4893 - accuracy: 0.3868Treinamento: batch 394 iniciou com 12:04:26.529314\n",
            "Treinamento: batch 394 terminou com 12:04:26.674482\n",
            " 395/1875 [=====>........................] - ETA: 3:39 - loss: 1.4883 - accuracy: 0.3873Treinamento: batch 395 iniciou com 12:04:26.676502\n",
            "Treinamento: batch 395 terminou com 12:04:26.828906\n",
            " 396/1875 [=====>........................] - ETA: 3:38 - loss: 1.4873 - accuracy: 0.3877Treinamento: batch 396 iniciou com 12:04:26.830672\n",
            "Treinamento: batch 396 terminou com 12:04:26.970292\n",
            " 397/1875 [=====>........................] - ETA: 3:38 - loss: 1.4860 - accuracy: 0.3883Treinamento: batch 397 iniciou com 12:04:26.973127\n",
            "Treinamento: batch 397 terminou com 12:04:27.116493\n",
            " 398/1875 [=====>........................] - ETA: 3:38 - loss: 1.4852 - accuracy: 0.3887Treinamento: batch 398 iniciou com 12:04:27.118651\n",
            "Treinamento: batch 398 terminou com 12:04:27.262533\n",
            " 399/1875 [=====>........................] - ETA: 3:38 - loss: 1.4845 - accuracy: 0.3891Treinamento: batch 399 iniciou com 12:04:27.264294\n",
            "Treinamento: batch 399 terminou com 12:04:27.414916\n",
            " 400/1875 [=====>........................] - ETA: 3:38 - loss: 1.4836 - accuracy: 0.3895Treinamento: batch 400 iniciou com 12:04:27.417417\n",
            "Treinamento: batch 400 terminou com 12:04:27.570624\n",
            " 401/1875 [=====>........................] - ETA: 3:38 - loss: 1.4824 - accuracy: 0.3903Treinamento: batch 401 iniciou com 12:04:27.572421\n",
            "Treinamento: batch 401 terminou com 12:04:27.712759\n",
            " 402/1875 [=====>........................] - ETA: 3:37 - loss: 1.4817 - accuracy: 0.3907Treinamento: batch 402 iniciou com 12:04:27.714501\n",
            "Treinamento: batch 402 terminou com 12:04:27.869874\n",
            " 403/1875 [=====>........................] - ETA: 3:37 - loss: 1.4807 - accuracy: 0.3910Treinamento: batch 403 iniciou com 12:04:27.871583\n",
            "Treinamento: batch 403 terminou com 12:04:28.009876\n",
            " 404/1875 [=====>........................] - ETA: 3:37 - loss: 1.4797 - accuracy: 0.3917Treinamento: batch 404 iniciou com 12:04:28.015924\n",
            "Treinamento: batch 404 terminou com 12:04:28.161559\n",
            " 405/1875 [=====>........................] - ETA: 3:37 - loss: 1.4786 - accuracy: 0.3921Treinamento: batch 405 iniciou com 12:04:28.163260\n",
            "Treinamento: batch 405 terminou com 12:04:28.307524\n",
            " 406/1875 [=====>........................] - ETA: 3:37 - loss: 1.4775 - accuracy: 0.3927Treinamento: batch 406 iniciou com 12:04:28.309201\n",
            "Treinamento: batch 406 terminou com 12:04:28.457966\n",
            " 407/1875 [=====>........................] - ETA: 3:37 - loss: 1.4771 - accuracy: 0.3932Treinamento: batch 407 iniciou com 12:04:28.459735\n",
            "Treinamento: batch 407 terminou com 12:04:28.604107\n",
            " 408/1875 [=====>........................] - ETA: 3:37 - loss: 1.4764 - accuracy: 0.3935Treinamento: batch 408 iniciou com 12:04:28.605831\n",
            "Treinamento: batch 408 terminou com 12:04:28.754137\n",
            " 409/1875 [=====>........................] - ETA: 3:36 - loss: 1.4756 - accuracy: 0.3941Treinamento: batch 409 iniciou com 12:04:28.755910\n",
            "Treinamento: batch 409 terminou com 12:04:28.903381\n",
            " 410/1875 [=====>........................] - ETA: 3:36 - loss: 1.4751 - accuracy: 0.3944Treinamento: batch 410 iniciou com 12:04:28.905067\n",
            "Treinamento: batch 410 terminou com 12:04:29.049324\n",
            " 411/1875 [=====>........................] - ETA: 3:36 - loss: 1.4751 - accuracy: 0.3945Treinamento: batch 411 iniciou com 12:04:29.051084\n",
            "Treinamento: batch 411 terminou com 12:04:29.196041\n",
            " 412/1875 [=====>........................] - ETA: 3:36 - loss: 1.4737 - accuracy: 0.3953Treinamento: batch 412 iniciou com 12:04:29.197743\n",
            "Treinamento: batch 412 terminou com 12:04:29.342109\n",
            " 413/1875 [=====>........................] - ETA: 3:36 - loss: 1.4727 - accuracy: 0.3958Treinamento: batch 413 iniciou com 12:04:29.343864\n",
            "Treinamento: batch 413 terminou com 12:04:29.492432\n",
            " 414/1875 [=====>........................] - ETA: 3:36 - loss: 1.4720 - accuracy: 0.3963Treinamento: batch 414 iniciou com 12:04:29.494173\n",
            "Treinamento: batch 414 terminou com 12:04:29.633815\n",
            " 415/1875 [=====>........................] - ETA: 3:36 - loss: 1.4707 - accuracy: 0.3968Treinamento: batch 415 iniciou com 12:04:29.637123\n",
            "Treinamento: batch 415 terminou com 12:04:29.778089\n",
            " 416/1875 [=====>........................] - ETA: 3:35 - loss: 1.4697 - accuracy: 0.3971Treinamento: batch 416 iniciou com 12:04:29.779784\n",
            "Treinamento: batch 416 terminou com 12:04:29.933287\n",
            " 417/1875 [=====>........................] - ETA: 3:35 - loss: 1.4687 - accuracy: 0.3975Treinamento: batch 417 iniciou com 12:04:29.935023\n",
            "Treinamento: batch 417 terminou com 12:04:30.078509\n",
            " 418/1875 [=====>........................] - ETA: 3:35 - loss: 1.4673 - accuracy: 0.3983Treinamento: batch 418 iniciou com 12:04:30.080490\n",
            "Treinamento: batch 418 terminou com 12:04:30.237175\n",
            " 419/1875 [=====>........................] - ETA: 3:35 - loss: 1.4662 - accuracy: 0.3987Treinamento: batch 419 iniciou com 12:04:30.238913\n",
            "Treinamento: batch 419 terminou com 12:04:30.384343\n",
            " 420/1875 [=====>........................] - ETA: 3:35 - loss: 1.4649 - accuracy: 0.3993Treinamento: batch 420 iniciou com 12:04:30.386427\n",
            "Treinamento: batch 420 terminou com 12:04:30.528558\n",
            " 421/1875 [=====>........................] - ETA: 3:35 - loss: 1.4633 - accuracy: 0.4001Treinamento: batch 421 iniciou com 12:04:30.530458\n",
            "Treinamento: batch 421 terminou com 12:04:30.674309\n",
            " 422/1875 [=====>........................] - ETA: 3:35 - loss: 1.4623 - accuracy: 0.4006Treinamento: batch 422 iniciou com 12:04:30.676114\n",
            "Treinamento: batch 422 terminou com 12:04:30.837208\n",
            " 423/1875 [=====>........................] - ETA: 3:34 - loss: 1.4611 - accuracy: 0.4009Treinamento: batch 423 iniciou com 12:04:30.838956\n",
            "Treinamento: batch 423 terminou com 12:04:30.985361\n",
            " 424/1875 [=====>........................] - ETA: 3:34 - loss: 1.4597 - accuracy: 0.4015Treinamento: batch 424 iniciou com 12:04:30.987106\n",
            "Treinamento: batch 424 terminou com 12:04:31.131938\n",
            " 425/1875 [=====>........................] - ETA: 3:34 - loss: 1.4588 - accuracy: 0.4022Treinamento: batch 425 iniciou com 12:04:31.133773\n",
            "Treinamento: batch 425 terminou com 12:04:31.278735\n",
            " 426/1875 [=====>........................] - ETA: 3:34 - loss: 1.4583 - accuracy: 0.4025Treinamento: batch 426 iniciou com 12:04:31.280423\n",
            "Treinamento: batch 426 terminou com 12:04:31.427659\n",
            " 427/1875 [=====>........................] - ETA: 3:34 - loss: 1.4581 - accuracy: 0.4024Treinamento: batch 427 iniciou com 12:04:31.429408\n",
            "Treinamento: batch 427 terminou com 12:04:31.577267\n",
            " 428/1875 [=====>........................] - ETA: 3:34 - loss: 1.4575 - accuracy: 0.4025Treinamento: batch 428 iniciou com 12:04:31.579514\n",
            "Treinamento: batch 428 terminou com 12:04:31.726294\n",
            " 429/1875 [=====>........................] - ETA: 3:34 - loss: 1.4569 - accuracy: 0.4028Treinamento: batch 429 iniciou com 12:04:31.728157\n",
            "Treinamento: batch 429 terminou com 12:04:31.881688\n",
            " 430/1875 [=====>........................] - ETA: 3:33 - loss: 1.4572 - accuracy: 0.4031Treinamento: batch 430 iniciou com 12:04:31.883421\n",
            "Treinamento: batch 430 terminou com 12:04:32.031094\n",
            " 431/1875 [=====>........................] - ETA: 3:33 - loss: 1.4564 - accuracy: 0.4033Treinamento: batch 431 iniciou com 12:04:32.032840\n",
            "Treinamento: batch 431 terminou com 12:04:32.177361\n",
            " 432/1875 [=====>........................] - ETA: 3:33 - loss: 1.4555 - accuracy: 0.4037Treinamento: batch 432 iniciou com 12:04:32.179193\n",
            "Treinamento: batch 432 terminou com 12:04:32.327464\n",
            " 433/1875 [=====>........................] - ETA: 3:33 - loss: 1.4554 - accuracy: 0.4039Treinamento: batch 433 iniciou com 12:04:32.329237\n",
            "Treinamento: batch 433 terminou com 12:04:32.469230\n",
            " 434/1875 [=====>........................] - ETA: 3:33 - loss: 1.4558 - accuracy: 0.4040Treinamento: batch 434 iniciou com 12:04:32.471341\n",
            "Treinamento: batch 434 terminou com 12:04:32.623247\n",
            " 435/1875 [=====>........................] - ETA: 3:33 - loss: 1.4554 - accuracy: 0.4040Treinamento: batch 435 iniciou com 12:04:32.625292\n",
            "Treinamento: batch 435 terminou com 12:04:32.771342\n",
            " 436/1875 [=====>........................] - ETA: 3:33 - loss: 1.4541 - accuracy: 0.4047Treinamento: batch 436 iniciou com 12:04:32.773080\n",
            "Treinamento: batch 436 terminou com 12:04:32.925447\n",
            " 437/1875 [=====>........................] - ETA: 3:32 - loss: 1.4539 - accuracy: 0.4051Treinamento: batch 437 iniciou com 12:04:32.927388\n",
            "Treinamento: batch 437 terminou com 12:04:33.081330\n",
            " 438/1875 [======>.......................] - ETA: 3:32 - loss: 1.4530 - accuracy: 0.4055Treinamento: batch 438 iniciou com 12:04:33.083093\n",
            "Treinamento: batch 438 terminou com 12:04:33.252267\n",
            " 439/1875 [======>.......................] - ETA: 3:32 - loss: 1.4526 - accuracy: 0.4055Treinamento: batch 439 iniciou com 12:04:33.254506\n",
            "Treinamento: batch 439 terminou com 12:04:33.395537\n",
            " 440/1875 [======>.......................] - ETA: 3:32 - loss: 1.4522 - accuracy: 0.4055Treinamento: batch 440 iniciou com 12:04:33.397378\n",
            "Treinamento: batch 440 terminou com 12:04:33.541049\n",
            " 441/1875 [======>.......................] - ETA: 3:32 - loss: 1.4517 - accuracy: 0.4057Treinamento: batch 441 iniciou com 12:04:33.542619\n",
            "Treinamento: batch 441 terminou com 12:04:33.687327\n",
            " 442/1875 [======>.......................] - ETA: 3:32 - loss: 1.4507 - accuracy: 0.4060Treinamento: batch 442 iniciou com 12:04:33.693045\n",
            "Treinamento: batch 442 terminou com 12:04:33.834599\n",
            " 443/1875 [======>.......................] - ETA: 3:32 - loss: 1.4506 - accuracy: 0.4058Treinamento: batch 443 iniciou com 12:04:33.836834\n",
            "Treinamento: batch 443 terminou com 12:04:33.993727\n",
            " 444/1875 [======>.......................] - ETA: 3:31 - loss: 1.4504 - accuracy: 0.4057Treinamento: batch 444 iniciou com 12:04:33.995518\n",
            "Treinamento: batch 444 terminou com 12:04:34.138873\n",
            " 445/1875 [======>.......................] - ETA: 3:31 - loss: 1.4492 - accuracy: 0.4062Treinamento: batch 445 iniciou com 12:04:34.140522\n",
            "Treinamento: batch 445 terminou com 12:04:34.289449\n",
            " 446/1875 [======>.......................] - ETA: 3:31 - loss: 1.4481 - accuracy: 0.4066Treinamento: batch 446 iniciou com 12:04:34.291222\n",
            "Treinamento: batch 446 terminou com 12:04:34.434062\n",
            " 447/1875 [======>.......................] - ETA: 3:31 - loss: 1.4473 - accuracy: 0.4071Treinamento: batch 447 iniciou com 12:04:34.435829\n",
            "Treinamento: batch 447 terminou com 12:04:34.575193\n",
            " 448/1875 [======>.......................] - ETA: 3:31 - loss: 1.4461 - accuracy: 0.4075Treinamento: batch 448 iniciou com 12:04:34.576863\n",
            "Treinamento: batch 448 terminou com 12:04:34.722877\n",
            " 449/1875 [======>.......................] - ETA: 3:31 - loss: 1.4452 - accuracy: 0.4080Treinamento: batch 449 iniciou com 12:04:34.724591\n",
            "Treinamento: batch 449 terminou com 12:04:34.866224\n",
            " 450/1875 [======>.......................] - ETA: 3:31 - loss: 1.4441 - accuracy: 0.4084Treinamento: batch 450 iniciou com 12:04:34.867955\n",
            "Treinamento: batch 450 terminou com 12:04:35.016068\n",
            " 451/1875 [======>.......................] - ETA: 3:30 - loss: 1.4439 - accuracy: 0.4086Treinamento: batch 451 iniciou com 12:04:35.017825\n",
            "Treinamento: batch 451 terminou com 12:04:35.164295\n",
            " 452/1875 [======>.......................] - ETA: 3:30 - loss: 1.4435 - accuracy: 0.4087Treinamento: batch 452 iniciou com 12:04:35.166055\n",
            "Treinamento: batch 452 terminou com 12:04:35.312937\n",
            " 453/1875 [======>.......................] - ETA: 3:30 - loss: 1.4427 - accuracy: 0.4088Treinamento: batch 453 iniciou com 12:04:35.314613\n",
            "Treinamento: batch 453 terminou com 12:04:35.455844\n",
            " 454/1875 [======>.......................] - ETA: 3:30 - loss: 1.4424 - accuracy: 0.4090Treinamento: batch 454 iniciou com 12:04:35.457495\n",
            "Treinamento: batch 454 terminou com 12:04:35.600871\n",
            " 455/1875 [======>.......................] - ETA: 3:30 - loss: 1.4418 - accuracy: 0.4095Treinamento: batch 455 iniciou com 12:04:35.602566\n",
            "Treinamento: batch 455 terminou com 12:04:35.743127\n",
            " 456/1875 [======>.......................] - ETA: 3:30 - loss: 1.4409 - accuracy: 0.4100Treinamento: batch 456 iniciou com 12:04:35.744788\n",
            "Treinamento: batch 456 terminou com 12:04:35.893445\n",
            " 457/1875 [======>.......................] - ETA: 3:29 - loss: 1.4401 - accuracy: 0.4105Treinamento: batch 457 iniciou com 12:04:35.895206\n",
            "Treinamento: batch 457 terminou com 12:04:36.049148\n",
            " 458/1875 [======>.......................] - ETA: 3:29 - loss: 1.4391 - accuracy: 0.4108Treinamento: batch 458 iniciou com 12:04:36.050968\n",
            "Treinamento: batch 458 terminou com 12:04:36.197129\n",
            " 459/1875 [======>.......................] - ETA: 3:29 - loss: 1.4392 - accuracy: 0.4110Treinamento: batch 459 iniciou com 12:04:36.198802\n",
            "Treinamento: batch 459 terminou com 12:04:36.339799\n",
            " 460/1875 [======>.......................] - ETA: 3:29 - loss: 1.4388 - accuracy: 0.4112Treinamento: batch 460 iniciou com 12:04:36.341490\n",
            "Treinamento: batch 460 terminou com 12:04:36.488957\n",
            " 461/1875 [======>.......................] - ETA: 3:29 - loss: 1.4379 - accuracy: 0.4114Treinamento: batch 461 iniciou com 12:04:36.490794\n",
            "Treinamento: batch 461 terminou com 12:04:36.630558\n",
            " 462/1875 [======>.......................] - ETA: 3:29 - loss: 1.4374 - accuracy: 0.4117Treinamento: batch 462 iniciou com 12:04:36.632169\n",
            "Treinamento: batch 462 terminou com 12:04:36.779363\n",
            " 463/1875 [======>.......................] - ETA: 3:29 - loss: 1.4365 - accuracy: 0.4120Treinamento: batch 463 iniciou com 12:04:36.781009\n",
            "Treinamento: batch 463 terminou com 12:04:36.925229\n",
            " 464/1875 [======>.......................] - ETA: 3:28 - loss: 1.4358 - accuracy: 0.4124Treinamento: batch 464 iniciou com 12:04:36.929200\n",
            "Treinamento: batch 464 terminou com 12:04:37.071876\n",
            " 465/1875 [======>.......................] - ETA: 3:28 - loss: 1.4353 - accuracy: 0.4126Treinamento: batch 465 iniciou com 12:04:37.073579\n",
            "Treinamento: batch 465 terminou com 12:04:37.220341\n",
            " 466/1875 [======>.......................] - ETA: 3:28 - loss: 1.4343 - accuracy: 0.4132Treinamento: batch 466 iniciou com 12:04:37.222081\n",
            "Treinamento: batch 466 terminou com 12:04:37.367975\n",
            " 467/1875 [======>.......................] - ETA: 3:28 - loss: 1.4327 - accuracy: 0.4139Treinamento: batch 467 iniciou com 12:04:37.369562\n",
            "Treinamento: batch 467 terminou com 12:04:37.523839\n",
            " 468/1875 [======>.......................] - ETA: 3:28 - loss: 1.4323 - accuracy: 0.4140Treinamento: batch 468 iniciou com 12:04:37.525545\n",
            "Treinamento: batch 468 terminou com 12:04:37.670774\n",
            " 469/1875 [======>.......................] - ETA: 3:28 - loss: 1.4314 - accuracy: 0.4145Treinamento: batch 469 iniciou com 12:04:37.672424\n",
            "Treinamento: batch 469 terminou com 12:04:37.815882\n",
            " 470/1875 [======>.......................] - ETA: 3:28 - loss: 1.4310 - accuracy: 0.4147Treinamento: batch 470 iniciou com 12:04:37.818065\n",
            "Treinamento: batch 470 terminou com 12:04:37.971141\n",
            " 471/1875 [======>.......................] - ETA: 3:27 - loss: 1.4301 - accuracy: 0.4150Treinamento: batch 471 iniciou com 12:04:37.972866\n",
            "Treinamento: batch 471 terminou com 12:04:38.120670\n",
            " 472/1875 [======>.......................] - ETA: 3:27 - loss: 1.4299 - accuracy: 0.4151Treinamento: batch 472 iniciou com 12:04:38.122394\n",
            "Treinamento: batch 472 terminou com 12:04:38.272446\n",
            " 473/1875 [======>.......................] - ETA: 3:27 - loss: 1.4284 - accuracy: 0.4159Treinamento: batch 473 iniciou com 12:04:38.274167\n",
            "Treinamento: batch 473 terminou com 12:04:38.416753\n",
            " 474/1875 [======>.......................] - ETA: 3:27 - loss: 1.4273 - accuracy: 0.4165Treinamento: batch 474 iniciou com 12:04:38.418666\n",
            "Treinamento: batch 474 terminou com 12:04:38.566878\n",
            " 475/1875 [======>.......................] - ETA: 3:27 - loss: 1.4266 - accuracy: 0.4170Treinamento: batch 475 iniciou com 12:04:38.568554\n",
            "Treinamento: batch 475 terminou com 12:04:38.714786\n",
            " 476/1875 [======>.......................] - ETA: 3:27 - loss: 1.4254 - accuracy: 0.4175Treinamento: batch 476 iniciou com 12:04:38.716532\n",
            "Treinamento: batch 476 terminou com 12:04:38.859562\n",
            " 477/1875 [======>.......................] - ETA: 3:27 - loss: 1.4250 - accuracy: 0.4180Treinamento: batch 477 iniciou com 12:04:38.861211\n",
            "Treinamento: batch 477 terminou com 12:04:39.021796\n",
            " 478/1875 [======>.......................] - ETA: 3:26 - loss: 1.4244 - accuracy: 0.4183Treinamento: batch 478 iniciou com 12:04:39.023718\n",
            "Treinamento: batch 478 terminou com 12:04:39.171844\n",
            " 479/1875 [======>.......................] - ETA: 3:26 - loss: 1.4244 - accuracy: 0.4184Treinamento: batch 479 iniciou com 12:04:39.173520\n",
            "Treinamento: batch 479 terminou com 12:04:39.316052\n",
            " 480/1875 [======>.......................] - ETA: 3:26 - loss: 1.4231 - accuracy: 0.4191Treinamento: batch 480 iniciou com 12:04:39.317813\n",
            "Treinamento: batch 480 terminou com 12:04:39.463925\n",
            " 481/1875 [======>.......................] - ETA: 3:26 - loss: 1.4220 - accuracy: 0.4196Treinamento: batch 481 iniciou com 12:04:39.469472\n",
            "Treinamento: batch 481 terminou com 12:04:39.613950\n",
            " 482/1875 [======>.......................] - ETA: 3:26 - loss: 1.4218 - accuracy: 0.4195Treinamento: batch 482 iniciou com 12:04:39.615705\n",
            "Treinamento: batch 482 terminou com 12:04:39.762582\n",
            " 483/1875 [======>.......................] - ETA: 3:26 - loss: 1.4210 - accuracy: 0.4196Treinamento: batch 483 iniciou com 12:04:39.764298\n",
            "Treinamento: batch 483 terminou com 12:04:39.912060\n",
            " 484/1875 [======>.......................] - ETA: 3:26 - loss: 1.4200 - accuracy: 0.4201Treinamento: batch 484 iniciou com 12:04:39.913675\n",
            "Treinamento: batch 484 terminou com 12:04:40.068823\n",
            " 485/1875 [======>.......................] - ETA: 3:25 - loss: 1.4192 - accuracy: 0.4205Treinamento: batch 485 iniciou com 12:04:40.070394\n",
            "Treinamento: batch 485 terminou com 12:04:40.221021\n",
            " 486/1875 [======>.......................] - ETA: 3:25 - loss: 1.4184 - accuracy: 0.4207Treinamento: batch 486 iniciou com 12:04:40.227710\n",
            "Treinamento: batch 486 terminou com 12:04:40.370821\n",
            " 487/1875 [======>.......................] - ETA: 3:25 - loss: 1.4177 - accuracy: 0.4208Treinamento: batch 487 iniciou com 12:04:40.372479\n",
            "Treinamento: batch 487 terminou com 12:04:40.514187\n",
            " 488/1875 [======>.......................] - ETA: 3:25 - loss: 1.4171 - accuracy: 0.4211Treinamento: batch 488 iniciou com 12:04:40.515970\n",
            "Treinamento: batch 488 terminou com 12:04:40.659191\n",
            " 489/1875 [======>.......................] - ETA: 3:25 - loss: 1.4159 - accuracy: 0.4216Treinamento: batch 489 iniciou com 12:04:40.660800\n",
            "Treinamento: batch 489 terminou com 12:04:40.805494\n",
            " 490/1875 [======>.......................] - ETA: 3:25 - loss: 1.4150 - accuracy: 0.4219Treinamento: batch 490 iniciou com 12:04:40.807287\n",
            "Treinamento: batch 490 terminou com 12:04:40.951280\n",
            " 491/1875 [======>.......................] - ETA: 3:25 - loss: 1.4140 - accuracy: 0.4223Treinamento: batch 491 iniciou com 12:04:40.953784\n",
            "Treinamento: batch 491 terminou com 12:04:41.108923\n",
            " 492/1875 [======>.......................] - ETA: 3:24 - loss: 1.4129 - accuracy: 0.4227Treinamento: batch 492 iniciou com 12:04:41.110894\n",
            "Treinamento: batch 492 terminou com 12:04:41.264920\n",
            " 493/1875 [======>.......................] - ETA: 3:24 - loss: 1.4122 - accuracy: 0.4229Treinamento: batch 493 iniciou com 12:04:41.266512\n",
            "Treinamento: batch 493 terminou com 12:04:41.412421\n",
            " 494/1875 [======>.......................] - ETA: 3:24 - loss: 1.4123 - accuracy: 0.4230Treinamento: batch 494 iniciou com 12:04:41.414159\n",
            "Treinamento: batch 494 terminou com 12:04:41.562934\n",
            " 495/1875 [======>.......................] - ETA: 3:24 - loss: 1.4120 - accuracy: 0.4232Treinamento: batch 495 iniciou com 12:04:41.564590\n",
            "Treinamento: batch 495 terminou com 12:04:41.713809\n",
            " 496/1875 [======>.......................] - ETA: 3:24 - loss: 1.4114 - accuracy: 0.4235Treinamento: batch 496 iniciou com 12:04:41.715545\n",
            "Treinamento: batch 496 terminou com 12:04:41.862326\n",
            " 497/1875 [======>.......................] - ETA: 3:24 - loss: 1.4109 - accuracy: 0.4235Treinamento: batch 497 iniciou com 12:04:41.864055\n",
            "Treinamento: batch 497 terminou com 12:04:42.017127\n",
            " 498/1875 [======>.......................] - ETA: 3:24 - loss: 1.4101 - accuracy: 0.4238Treinamento: batch 498 iniciou com 12:04:42.020369\n",
            "Treinamento: batch 498 terminou com 12:04:42.174166\n",
            " 499/1875 [======>.......................] - ETA: 3:23 - loss: 1.4092 - accuracy: 0.4243Treinamento: batch 499 iniciou com 12:04:42.175953\n",
            "Treinamento: batch 499 terminou com 12:04:42.332105\n",
            " 500/1875 [=======>......................] - ETA: 3:23 - loss: 1.4080 - accuracy: 0.4248Treinamento: batch 500 iniciou com 12:04:42.333717\n",
            "Treinamento: batch 500 terminou com 12:04:42.481975\n",
            " 501/1875 [=======>......................] - ETA: 3:23 - loss: 1.4074 - accuracy: 0.4251Treinamento: batch 501 iniciou com 12:04:42.483588\n",
            "Treinamento: batch 501 terminou com 12:04:42.632584\n",
            " 502/1875 [=======>......................] - ETA: 3:23 - loss: 1.4064 - accuracy: 0.4255Treinamento: batch 502 iniciou com 12:04:42.634493\n",
            "Treinamento: batch 502 terminou com 12:04:42.778447\n",
            " 503/1875 [=======>......................] - ETA: 3:23 - loss: 1.4057 - accuracy: 0.4258Treinamento: batch 503 iniciou com 12:04:42.780148\n",
            "Treinamento: batch 503 terminou com 12:04:42.933905\n",
            " 504/1875 [=======>......................] - ETA: 3:23 - loss: 1.4048 - accuracy: 0.4261Treinamento: batch 504 iniciou com 12:04:42.935657\n",
            "Treinamento: batch 504 terminou com 12:04:43.092055\n",
            " 505/1875 [=======>......................] - ETA: 3:23 - loss: 1.4039 - accuracy: 0.4264Treinamento: batch 505 iniciou com 12:04:43.093782\n",
            "Treinamento: batch 505 terminou com 12:04:43.240121\n",
            " 506/1875 [=======>......................] - ETA: 3:22 - loss: 1.4031 - accuracy: 0.4264Treinamento: batch 506 iniciou com 12:04:43.241706\n",
            "Treinamento: batch 506 terminou com 12:04:43.386778\n",
            " 507/1875 [=======>......................] - ETA: 3:22 - loss: 1.4028 - accuracy: 0.4265Treinamento: batch 507 iniciou com 12:04:43.388472\n",
            "Treinamento: batch 507 terminou com 12:04:43.562768\n",
            " 508/1875 [=======>......................] - ETA: 3:22 - loss: 1.4018 - accuracy: 0.4270Treinamento: batch 508 iniciou com 12:04:43.564459\n",
            "Treinamento: batch 508 terminou com 12:04:43.706149\n",
            " 509/1875 [=======>......................] - ETA: 3:22 - loss: 1.4010 - accuracy: 0.4276Treinamento: batch 509 iniciou com 12:04:43.708366\n",
            "Treinamento: batch 509 terminou com 12:04:43.852691\n",
            " 510/1875 [=======>......................] - ETA: 3:22 - loss: 1.4007 - accuracy: 0.4278Treinamento: batch 510 iniciou com 12:04:43.854270\n",
            "Treinamento: batch 510 terminou com 12:04:44.005740\n",
            " 511/1875 [=======>......................] - ETA: 3:22 - loss: 1.3998 - accuracy: 0.4281Treinamento: batch 511 iniciou com 12:04:44.007692\n",
            "Treinamento: batch 511 terminou com 12:04:44.162748\n",
            " 512/1875 [=======>......................] - ETA: 3:22 - loss: 1.3991 - accuracy: 0.4285Treinamento: batch 512 iniciou com 12:04:44.164481\n",
            "Treinamento: batch 512 terminou com 12:04:44.313031\n",
            " 513/1875 [=======>......................] - ETA: 3:22 - loss: 1.3985 - accuracy: 0.4289Treinamento: batch 513 iniciou com 12:04:44.316949\n",
            "Treinamento: batch 513 terminou com 12:04:44.457423\n",
            " 514/1875 [=======>......................] - ETA: 3:21 - loss: 1.3981 - accuracy: 0.4291Treinamento: batch 514 iniciou com 12:04:44.459105\n",
            "Treinamento: batch 514 terminou com 12:04:44.604238\n",
            " 515/1875 [=======>......................] - ETA: 3:21 - loss: 1.3976 - accuracy: 0.4292Treinamento: batch 515 iniciou com 12:04:44.606077\n",
            "Treinamento: batch 515 terminou com 12:04:44.752156\n",
            " 516/1875 [=======>......................] - ETA: 3:21 - loss: 1.3967 - accuracy: 0.4297Treinamento: batch 516 iniciou com 12:04:44.754135\n",
            "Treinamento: batch 516 terminou com 12:04:44.893935\n",
            " 517/1875 [=======>......................] - ETA: 3:21 - loss: 1.3964 - accuracy: 0.4296Treinamento: batch 517 iniciou com 12:04:44.895843\n",
            "Treinamento: batch 517 terminou com 12:04:45.045071\n",
            " 518/1875 [=======>......................] - ETA: 3:21 - loss: 1.3956 - accuracy: 0.4299Treinamento: batch 518 iniciou com 12:04:45.046730\n",
            "Treinamento: batch 518 terminou com 12:04:45.201823\n",
            " 519/1875 [=======>......................] - ETA: 3:21 - loss: 1.3952 - accuracy: 0.4303Treinamento: batch 519 iniciou com 12:04:45.203395\n",
            "Treinamento: batch 519 terminou com 12:04:45.358944\n",
            " 520/1875 [=======>......................] - ETA: 3:21 - loss: 1.3949 - accuracy: 0.4303Treinamento: batch 520 iniciou com 12:04:45.360677\n",
            "Treinamento: batch 520 terminou com 12:04:45.504886\n",
            " 521/1875 [=======>......................] - ETA: 3:20 - loss: 1.3943 - accuracy: 0.4308Treinamento: batch 521 iniciou com 12:04:45.506478\n",
            "Treinamento: batch 521 terminou com 12:04:45.649955\n",
            " 522/1875 [=======>......................] - ETA: 3:20 - loss: 1.3937 - accuracy: 0.4313Treinamento: batch 522 iniciou com 12:04:45.651565\n",
            "Treinamento: batch 522 terminou com 12:04:45.798827\n",
            " 523/1875 [=======>......................] - ETA: 3:20 - loss: 1.3933 - accuracy: 0.4315Treinamento: batch 523 iniciou com 12:04:45.800589\n",
            "Treinamento: batch 523 terminou com 12:04:45.950370\n",
            " 524/1875 [=======>......................] - ETA: 3:20 - loss: 1.3925 - accuracy: 0.4318Treinamento: batch 524 iniciou com 12:04:45.952459\n",
            "Treinamento: batch 524 terminou com 12:04:46.104060\n",
            " 525/1875 [=======>......................] - ETA: 3:20 - loss: 1.3916 - accuracy: 0.4323Treinamento: batch 525 iniciou com 12:04:46.105788\n",
            "Treinamento: batch 525 terminou com 12:04:46.249759\n",
            " 526/1875 [=======>......................] - ETA: 3:20 - loss: 1.3907 - accuracy: 0.4327Treinamento: batch 526 iniciou com 12:04:46.251525\n",
            "Treinamento: batch 526 terminou com 12:04:46.397047\n",
            " 527/1875 [=======>......................] - ETA: 3:19 - loss: 1.3905 - accuracy: 0.4328Treinamento: batch 527 iniciou com 12:04:46.398817\n",
            "Treinamento: batch 527 terminou com 12:04:46.542818\n",
            " 528/1875 [=======>......................] - ETA: 3:19 - loss: 1.3897 - accuracy: 0.4333Treinamento: batch 528 iniciou com 12:04:46.544540\n",
            "Treinamento: batch 528 terminou com 12:04:46.699370\n",
            " 529/1875 [=======>......................] - ETA: 3:19 - loss: 1.3890 - accuracy: 0.4337Treinamento: batch 529 iniciou com 12:04:46.701147\n",
            "Treinamento: batch 529 terminou com 12:04:46.846217\n",
            " 530/1875 [=======>......................] - ETA: 3:19 - loss: 1.3881 - accuracy: 0.4341Treinamento: batch 530 iniciou com 12:04:46.847966\n",
            "Treinamento: batch 530 terminou com 12:04:46.992367\n",
            " 531/1875 [=======>......................] - ETA: 3:19 - loss: 1.3875 - accuracy: 0.4344Treinamento: batch 531 iniciou com 12:04:46.994115\n",
            "Treinamento: batch 531 terminou com 12:04:47.151718\n",
            " 532/1875 [=======>......................] - ETA: 3:19 - loss: 1.3867 - accuracy: 0.4348Treinamento: batch 532 iniciou com 12:04:47.153489\n",
            "Treinamento: batch 532 terminou com 12:04:47.301438\n",
            " 533/1875 [=======>......................] - ETA: 3:19 - loss: 1.3860 - accuracy: 0.4350Treinamento: batch 533 iniciou com 12:04:47.303170\n",
            "Treinamento: batch 533 terminou com 12:04:47.447812\n",
            " 534/1875 [=======>......................] - ETA: 3:18 - loss: 1.3852 - accuracy: 0.4353Treinamento: batch 534 iniciou com 12:04:47.452785\n",
            "Treinamento: batch 534 terminou com 12:04:47.593526\n",
            " 535/1875 [=======>......................] - ETA: 3:18 - loss: 1.3842 - accuracy: 0.4357Treinamento: batch 535 iniciou com 12:04:47.595316\n",
            "Treinamento: batch 535 terminou com 12:04:47.741890\n",
            " 536/1875 [=======>......................] - ETA: 3:18 - loss: 1.3830 - accuracy: 0.4362Treinamento: batch 536 iniciou com 12:04:47.743661\n",
            "Treinamento: batch 536 terminou com 12:04:47.891280\n",
            " 537/1875 [=======>......................] - ETA: 3:18 - loss: 1.3818 - accuracy: 0.4366Treinamento: batch 537 iniciou com 12:04:47.893009\n",
            "Treinamento: batch 537 terminou com 12:04:48.046515\n",
            " 538/1875 [=======>......................] - ETA: 3:18 - loss: 1.3804 - accuracy: 0.4372Treinamento: batch 538 iniciou com 12:04:48.048577\n",
            "Treinamento: batch 538 terminou com 12:04:48.203514\n",
            " 539/1875 [=======>......................] - ETA: 3:18 - loss: 1.3796 - accuracy: 0.4375Treinamento: batch 539 iniciou com 12:04:48.205262\n",
            "Treinamento: batch 539 terminou com 12:04:48.351512\n",
            " 540/1875 [=======>......................] - ETA: 3:18 - loss: 1.3787 - accuracy: 0.4380Treinamento: batch 540 iniciou com 12:04:48.353295\n",
            "Treinamento: batch 540 terminou com 12:04:48.496311\n",
            " 541/1875 [=======>......................] - ETA: 3:17 - loss: 1.3774 - accuracy: 0.4387Treinamento: batch 541 iniciou com 12:04:48.498053\n",
            "Treinamento: batch 541 terminou com 12:04:48.642079\n",
            " 542/1875 [=======>......................] - ETA: 3:17 - loss: 1.3764 - accuracy: 0.4390Treinamento: batch 542 iniciou com 12:04:48.643913\n",
            "Treinamento: batch 542 terminou com 12:04:48.787097\n",
            " 543/1875 [=======>......................] - ETA: 3:17 - loss: 1.3756 - accuracy: 0.4391Treinamento: batch 543 iniciou com 12:04:48.788786\n",
            "Treinamento: batch 543 terminou com 12:04:48.936772\n",
            " 544/1875 [=======>......................] - ETA: 3:17 - loss: 1.3752 - accuracy: 0.4393Treinamento: batch 544 iniciou com 12:04:48.939022\n",
            "Treinamento: batch 544 terminou com 12:04:49.080883\n",
            " 545/1875 [=======>......................] - ETA: 3:17 - loss: 1.3750 - accuracy: 0.4394Treinamento: batch 545 iniciou com 12:04:49.082668\n",
            "Treinamento: batch 545 terminou com 12:04:49.244492\n",
            " 546/1875 [=======>......................] - ETA: 3:17 - loss: 1.3744 - accuracy: 0.4398Treinamento: batch 546 iniciou com 12:04:49.246535\n",
            "Treinamento: batch 546 terminou com 12:04:49.391680\n",
            " 547/1875 [=======>......................] - ETA: 3:17 - loss: 1.3736 - accuracy: 0.4403Treinamento: batch 547 iniciou com 12:04:49.393944\n",
            "Treinamento: batch 547 terminou com 12:04:49.541242\n",
            " 548/1875 [=======>......................] - ETA: 3:16 - loss: 1.3730 - accuracy: 0.4406Treinamento: batch 548 iniciou com 12:04:49.543023\n",
            "Treinamento: batch 548 terminou com 12:04:49.684889\n",
            " 549/1875 [=======>......................] - ETA: 3:16 - loss: 1.3723 - accuracy: 0.4409Treinamento: batch 549 iniciou com 12:04:49.686693\n",
            "Treinamento: batch 549 terminou com 12:04:49.832604\n",
            " 550/1875 [=======>......................] - ETA: 3:16 - loss: 1.3716 - accuracy: 0.4411Treinamento: batch 550 iniciou com 12:04:49.834390\n",
            "Treinamento: batch 550 terminou com 12:04:49.981896\n",
            " 551/1875 [=======>......................] - ETA: 3:16 - loss: 1.3707 - accuracy: 0.4414Treinamento: batch 551 iniciou com 12:04:49.983584\n",
            "Treinamento: batch 551 terminou com 12:04:50.131738\n",
            " 552/1875 [=======>......................] - ETA: 3:16 - loss: 1.3697 - accuracy: 0.4419Treinamento: batch 552 iniciou com 12:04:50.133553\n",
            "Treinamento: batch 552 terminou com 12:04:50.284120\n",
            " 553/1875 [=======>......................] - ETA: 3:16 - loss: 1.3691 - accuracy: 0.4424Treinamento: batch 553 iniciou com 12:04:50.285974\n",
            "Treinamento: batch 553 terminou com 12:04:50.429231\n",
            " 554/1875 [=======>......................] - ETA: 3:16 - loss: 1.3682 - accuracy: 0.4429Treinamento: batch 554 iniciou com 12:04:50.433471\n",
            "Treinamento: batch 554 terminou com 12:04:50.570493\n",
            " 555/1875 [=======>......................] - ETA: 3:15 - loss: 1.3676 - accuracy: 0.4431Treinamento: batch 555 iniciou com 12:04:50.572144\n",
            "Treinamento: batch 555 terminou com 12:04:50.724041\n",
            " 556/1875 [=======>......................] - ETA: 3:15 - loss: 1.3673 - accuracy: 0.4434Treinamento: batch 556 iniciou com 12:04:50.725775\n",
            "Treinamento: batch 556 terminou com 12:04:50.873388\n",
            " 557/1875 [=======>......................] - ETA: 3:15 - loss: 1.3664 - accuracy: 0.4439Treinamento: batch 557 iniciou com 12:04:50.875050\n",
            "Treinamento: batch 557 terminou com 12:04:51.028106\n",
            " 558/1875 [=======>......................] - ETA: 3:15 - loss: 1.3656 - accuracy: 0.4441Treinamento: batch 558 iniciou com 12:04:51.029934\n",
            "Treinamento: batch 558 terminou com 12:04:51.172367\n",
            " 559/1875 [=======>......................] - ETA: 3:15 - loss: 1.3646 - accuracy: 0.4444Treinamento: batch 559 iniciou com 12:04:51.174085\n",
            "Treinamento: batch 559 terminou com 12:04:51.328649\n",
            " 560/1875 [=======>......................] - ETA: 3:15 - loss: 1.3644 - accuracy: 0.4445Treinamento: batch 560 iniciou com 12:04:51.330884\n",
            "Treinamento: batch 560 terminou com 12:04:51.475812\n",
            " 561/1875 [=======>......................] - ETA: 3:15 - loss: 1.3638 - accuracy: 0.4446Treinamento: batch 561 iniciou com 12:04:51.477559\n",
            "Treinamento: batch 561 terminou com 12:04:51.619490\n",
            " 562/1875 [=======>......................] - ETA: 3:14 - loss: 1.3627 - accuracy: 0.4451Treinamento: batch 562 iniciou com 12:04:51.621257\n",
            "Treinamento: batch 562 terminou com 12:04:51.769157\n",
            " 563/1875 [========>.....................] - ETA: 3:14 - loss: 1.3619 - accuracy: 0.4455Treinamento: batch 563 iniciou com 12:04:51.770989\n",
            "Treinamento: batch 563 terminou com 12:04:51.920189\n",
            " 564/1875 [========>.....................] - ETA: 3:14 - loss: 1.3610 - accuracy: 0.4460Treinamento: batch 564 iniciou com 12:04:51.922568\n",
            "Treinamento: batch 564 terminou com 12:04:52.069414\n",
            " 565/1875 [========>.....................] - ETA: 3:14 - loss: 1.3602 - accuracy: 0.4463Treinamento: batch 565 iniciou com 12:04:52.071686\n",
            "Treinamento: batch 565 terminou com 12:04:52.228167\n",
            " 566/1875 [========>.....................] - ETA: 3:14 - loss: 1.3595 - accuracy: 0.4465Treinamento: batch 566 iniciou com 12:04:52.229999\n",
            "Treinamento: batch 566 terminou com 12:04:52.373287\n",
            " 567/1875 [========>.....................] - ETA: 3:14 - loss: 1.3591 - accuracy: 0.4466Treinamento: batch 567 iniciou com 12:04:52.374952\n",
            "Treinamento: batch 567 terminou com 12:04:52.517961\n",
            " 568/1875 [========>.....................] - ETA: 3:13 - loss: 1.3589 - accuracy: 0.4470Treinamento: batch 568 iniciou com 12:04:52.519583\n",
            "Treinamento: batch 568 terminou com 12:04:52.666835\n",
            " 569/1875 [========>.....................] - ETA: 3:13 - loss: 1.3588 - accuracy: 0.4472Treinamento: batch 569 iniciou com 12:04:52.668595\n",
            "Treinamento: batch 569 terminou com 12:04:52.813097\n",
            " 570/1875 [========>.....................] - ETA: 3:13 - loss: 1.3584 - accuracy: 0.4475Treinamento: batch 570 iniciou com 12:04:52.814843\n",
            "Treinamento: batch 570 terminou com 12:04:52.958003\n",
            " 571/1875 [========>.....................] - ETA: 3:13 - loss: 1.3579 - accuracy: 0.4477Treinamento: batch 571 iniciou com 12:04:52.959774\n",
            "Treinamento: batch 571 terminou com 12:04:53.110350\n",
            " 572/1875 [========>.....................] - ETA: 3:13 - loss: 1.3576 - accuracy: 0.4478Treinamento: batch 572 iniciou com 12:04:53.112125\n",
            "Treinamento: batch 572 terminou com 12:04:53.269473\n",
            " 573/1875 [========>.....................] - ETA: 3:13 - loss: 1.3570 - accuracy: 0.4481Treinamento: batch 573 iniciou com 12:04:53.271245\n",
            "Treinamento: batch 573 terminou com 12:04:53.423708\n",
            " 574/1875 [========>.....................] - ETA: 3:13 - loss: 1.3568 - accuracy: 0.4482Treinamento: batch 574 iniciou com 12:04:53.425452\n",
            "Treinamento: batch 574 terminou com 12:04:53.572508\n",
            " 575/1875 [========>.....................] - ETA: 3:12 - loss: 1.3561 - accuracy: 0.4484Treinamento: batch 575 iniciou com 12:04:53.574587\n",
            "Treinamento: batch 575 terminou com 12:04:53.733894\n",
            " 576/1875 [========>.....................] - ETA: 3:12 - loss: 1.3553 - accuracy: 0.4488Treinamento: batch 576 iniciou com 12:04:53.735783\n",
            "Treinamento: batch 576 terminou com 12:04:53.892293\n",
            " 577/1875 [========>.....................] - ETA: 3:12 - loss: 1.3548 - accuracy: 0.4491Treinamento: batch 577 iniciou com 12:04:53.894454\n",
            "Treinamento: batch 577 terminou com 12:04:54.039831\n",
            " 578/1875 [========>.....................] - ETA: 3:12 - loss: 1.3541 - accuracy: 0.4494Treinamento: batch 578 iniciou com 12:04:54.041589\n",
            "Treinamento: batch 578 terminou com 12:04:54.187117\n",
            " 579/1875 [========>.....................] - ETA: 3:12 - loss: 1.3540 - accuracy: 0.4494Treinamento: batch 579 iniciou com 12:04:54.189052\n",
            "Treinamento: batch 579 terminou com 12:04:54.342968\n",
            " 580/1875 [========>.....................] - ETA: 3:12 - loss: 1.3530 - accuracy: 0.4498Treinamento: batch 580 iniciou com 12:04:54.344854\n",
            "Treinamento: batch 580 terminou com 12:04:54.493777\n",
            " 581/1875 [========>.....................] - ETA: 3:12 - loss: 1.3521 - accuracy: 0.4500Treinamento: batch 581 iniciou com 12:04:54.495488\n",
            "Treinamento: batch 581 terminou com 12:04:54.642841\n",
            " 582/1875 [========>.....................] - ETA: 3:12 - loss: 1.3517 - accuracy: 0.4503Treinamento: batch 582 iniciou com 12:04:54.645062\n",
            "Treinamento: batch 582 terminou com 12:04:54.794150\n",
            " 583/1875 [========>.....................] - ETA: 3:11 - loss: 1.3509 - accuracy: 0.4507Treinamento: batch 583 iniciou com 12:04:54.796111\n",
            "Treinamento: batch 583 terminou com 12:04:54.942155\n",
            " 584/1875 [========>.....................] - ETA: 3:11 - loss: 1.3502 - accuracy: 0.4508Treinamento: batch 584 iniciou com 12:04:54.944337\n",
            "Treinamento: batch 584 terminou com 12:04:55.092782\n",
            " 585/1875 [========>.....................] - ETA: 3:11 - loss: 1.3494 - accuracy: 0.4511Treinamento: batch 585 iniciou com 12:04:55.094512\n",
            "Treinamento: batch 585 terminou com 12:04:55.243837\n",
            " 586/1875 [========>.....................] - ETA: 3:11 - loss: 1.3498 - accuracy: 0.4513Treinamento: batch 586 iniciou com 12:04:55.245851\n",
            "Treinamento: batch 586 terminou com 12:04:55.396931\n",
            " 587/1875 [========>.....................] - ETA: 3:11 - loss: 1.3489 - accuracy: 0.4518Treinamento: batch 587 iniciou com 12:04:55.398781\n",
            "Treinamento: batch 587 terminou com 12:04:55.553052\n",
            " 588/1875 [========>.....................] - ETA: 3:11 - loss: 1.3483 - accuracy: 0.4520Treinamento: batch 588 iniciou com 12:04:55.554826\n",
            "Treinamento: batch 588 terminou com 12:04:55.697764\n",
            " 589/1875 [========>.....................] - ETA: 3:10 - loss: 1.3479 - accuracy: 0.4522Treinamento: batch 589 iniciou com 12:04:55.699518\n",
            "Treinamento: batch 589 terminou com 12:04:55.838965\n",
            " 590/1875 [========>.....................] - ETA: 3:10 - loss: 1.3475 - accuracy: 0.4525Treinamento: batch 590 iniciou com 12:04:55.840951\n",
            "Treinamento: batch 590 terminou com 12:04:55.986497\n",
            " 591/1875 [========>.....................] - ETA: 3:10 - loss: 1.3473 - accuracy: 0.4527Treinamento: batch 591 iniciou com 12:04:55.988245\n",
            "Treinamento: batch 591 terminou com 12:04:56.139878\n",
            " 592/1875 [========>.....................] - ETA: 3:10 - loss: 1.3468 - accuracy: 0.4529Treinamento: batch 592 iniciou com 12:04:56.141676\n",
            "Treinamento: batch 592 terminou com 12:04:56.291572\n",
            " 593/1875 [========>.....................] - ETA: 3:10 - loss: 1.3460 - accuracy: 0.4533Treinamento: batch 593 iniciou com 12:04:56.301114\n",
            "Treinamento: batch 593 terminou com 12:04:56.446221\n",
            " 594/1875 [========>.....................] - ETA: 3:10 - loss: 1.3450 - accuracy: 0.4538Treinamento: batch 594 iniciou com 12:04:56.448011\n",
            "Treinamento: batch 594 terminou com 12:04:56.590846\n",
            " 595/1875 [========>.....................] - ETA: 3:10 - loss: 1.3445 - accuracy: 0.4539Treinamento: batch 595 iniciou com 12:04:56.592561\n",
            "Treinamento: batch 595 terminou com 12:04:56.739256\n",
            " 596/1875 [========>.....................] - ETA: 3:09 - loss: 1.3443 - accuracy: 0.4541Treinamento: batch 596 iniciou com 12:04:56.741031\n",
            "Treinamento: batch 596 terminou com 12:04:56.884229\n",
            " 597/1875 [========>.....................] - ETA: 3:09 - loss: 1.3442 - accuracy: 0.4540Treinamento: batch 597 iniciou com 12:04:56.885972\n",
            "Treinamento: batch 597 terminou com 12:04:57.037226\n",
            " 598/1875 [========>.....................] - ETA: 3:09 - loss: 1.3438 - accuracy: 0.4544Treinamento: batch 598 iniciou com 12:04:57.039044\n",
            "Treinamento: batch 598 terminou com 12:04:57.180918\n",
            " 599/1875 [========>.....................] - ETA: 3:09 - loss: 1.3432 - accuracy: 0.4548Treinamento: batch 599 iniciou com 12:04:57.182866\n",
            "Treinamento: batch 599 terminou com 12:04:57.341915\n",
            " 600/1875 [========>.....................] - ETA: 3:09 - loss: 1.3427 - accuracy: 0.4549Treinamento: batch 600 iniciou com 12:04:57.344051\n",
            "Treinamento: batch 600 terminou com 12:04:57.493260\n",
            " 601/1875 [========>.....................] - ETA: 3:09 - loss: 1.3420 - accuracy: 0.4552Treinamento: batch 601 iniciou com 12:04:57.494963\n",
            "Treinamento: batch 601 terminou com 12:04:57.641069\n",
            " 602/1875 [========>.....................] - ETA: 3:09 - loss: 1.3419 - accuracy: 0.4554Treinamento: batch 602 iniciou com 12:04:57.642772\n",
            "Treinamento: batch 602 terminou com 12:04:57.791992\n",
            " 603/1875 [========>.....................] - ETA: 3:08 - loss: 1.3411 - accuracy: 0.4556Treinamento: batch 603 iniciou com 12:04:57.793811\n",
            "Treinamento: batch 603 terminou com 12:04:57.945156\n",
            " 604/1875 [========>.....................] - ETA: 3:08 - loss: 1.3406 - accuracy: 0.4557Treinamento: batch 604 iniciou com 12:04:57.946950\n",
            "Treinamento: batch 604 terminou com 12:04:58.089805\n",
            " 605/1875 [========>.....................] - ETA: 3:08 - loss: 1.3399 - accuracy: 0.4559Treinamento: batch 605 iniciou com 12:04:58.091472\n",
            "Treinamento: batch 605 terminou com 12:04:58.236065\n",
            " 606/1875 [========>.....................] - ETA: 3:08 - loss: 1.3389 - accuracy: 0.4564Treinamento: batch 606 iniciou com 12:04:58.238693\n",
            "Treinamento: batch 606 terminou com 12:04:58.388848\n",
            " 607/1875 [========>.....................] - ETA: 3:08 - loss: 1.3383 - accuracy: 0.4567Treinamento: batch 607 iniciou com 12:04:58.390611\n",
            "Treinamento: batch 607 terminou com 12:04:58.545900\n",
            " 608/1875 [========>.....................] - ETA: 3:08 - loss: 1.3378 - accuracy: 0.4569Treinamento: batch 608 iniciou com 12:04:58.547702\n",
            "Treinamento: batch 608 terminou com 12:04:58.693883\n",
            " 609/1875 [========>.....................] - ETA: 3:08 - loss: 1.3367 - accuracy: 0.4574Treinamento: batch 609 iniciou com 12:04:58.695668\n",
            "Treinamento: batch 609 terminou com 12:04:58.841078\n",
            " 610/1875 [========>.....................] - ETA: 3:07 - loss: 1.3360 - accuracy: 0.4578Treinamento: batch 610 iniciou com 12:04:58.842673\n",
            "Treinamento: batch 610 terminou com 12:04:58.989369\n",
            " 611/1875 [========>.....................] - ETA: 3:07 - loss: 1.3352 - accuracy: 0.4582Treinamento: batch 611 iniciou com 12:04:58.991921\n",
            "Treinamento: batch 611 terminou com 12:04:59.138741\n",
            " 612/1875 [========>.....................] - ETA: 3:07 - loss: 1.3349 - accuracy: 0.4583Treinamento: batch 612 iniciou com 12:04:59.140470\n",
            "Treinamento: batch 612 terminou com 12:04:59.280981\n",
            " 613/1875 [========>.....................] - ETA: 3:07 - loss: 1.3340 - accuracy: 0.4588Treinamento: batch 613 iniciou com 12:04:59.282743\n",
            "Treinamento: batch 613 terminou com 12:04:59.433990\n",
            " 614/1875 [========>.....................] - ETA: 3:07 - loss: 1.3337 - accuracy: 0.4588Treinamento: batch 614 iniciou com 12:04:59.435750\n",
            "Treinamento: batch 614 terminou com 12:04:59.583838\n",
            " 615/1875 [========>.....................] - ETA: 3:07 - loss: 1.3329 - accuracy: 0.4592Treinamento: batch 615 iniciou com 12:04:59.585670\n",
            "Treinamento: batch 615 terminou com 12:04:59.725165\n",
            " 616/1875 [========>.....................] - ETA: 3:07 - loss: 1.3321 - accuracy: 0.4595Treinamento: batch 616 iniciou com 12:04:59.726896\n",
            "Treinamento: batch 616 terminou com 12:04:59.879466\n",
            " 617/1875 [========>.....................] - ETA: 3:06 - loss: 1.3309 - accuracy: 0.4601Treinamento: batch 617 iniciou com 12:04:59.881289\n",
            "Treinamento: batch 617 terminou com 12:05:00.025033\n",
            " 618/1875 [========>.....................] - ETA: 3:06 - loss: 1.3303 - accuracy: 0.4604Treinamento: batch 618 iniciou com 12:05:00.026787\n",
            "Treinamento: batch 618 terminou com 12:05:00.176787\n",
            " 619/1875 [========>.....................] - ETA: 3:06 - loss: 1.3298 - accuracy: 0.4606Treinamento: batch 619 iniciou com 12:05:00.178499\n",
            "Treinamento: batch 619 terminou com 12:05:00.320698\n",
            " 620/1875 [========>.....................] - ETA: 3:06 - loss: 1.3292 - accuracy: 0.4608Treinamento: batch 620 iniciou com 12:05:00.324413\n",
            "Treinamento: batch 620 terminou com 12:05:00.473132\n",
            " 621/1875 [========>.....................] - ETA: 3:06 - loss: 1.3290 - accuracy: 0.4610Treinamento: batch 621 iniciou com 12:05:00.474995\n",
            "Treinamento: batch 621 terminou com 12:05:00.620186\n",
            " 622/1875 [========>.....................] - ETA: 3:06 - loss: 1.3281 - accuracy: 0.4613Treinamento: batch 622 iniciou com 12:05:00.624311\n",
            "Treinamento: batch 622 terminou com 12:05:00.767244\n",
            " 623/1875 [========>.....................] - ETA: 3:05 - loss: 1.3281 - accuracy: 0.4613Treinamento: batch 623 iniciou com 12:05:00.772184\n",
            "Treinamento: batch 623 terminou com 12:05:00.920745\n",
            " 624/1875 [========>.....................] - ETA: 3:05 - loss: 1.3278 - accuracy: 0.4615Treinamento: batch 624 iniciou com 12:05:00.922867\n",
            "Treinamento: batch 624 terminou com 12:05:01.070142\n",
            " 625/1875 [=========>....................] - ETA: 3:05 - loss: 1.3271 - accuracy: 0.4620Treinamento: batch 625 iniciou com 12:05:01.071978\n",
            "Treinamento: batch 625 terminou com 12:05:01.213971\n",
            " 626/1875 [=========>....................] - ETA: 3:05 - loss: 1.3257 - accuracy: 0.4625Treinamento: batch 626 iniciou com 12:05:01.215695\n",
            "Treinamento: batch 626 terminou com 12:05:01.360733\n",
            " 627/1875 [=========>....................] - ETA: 3:05 - loss: 1.3249 - accuracy: 0.4628Treinamento: batch 627 iniciou com 12:05:01.362805\n",
            "Treinamento: batch 627 terminou com 12:05:01.521403\n",
            " 628/1875 [=========>....................] - ETA: 3:05 - loss: 1.3240 - accuracy: 0.4632Treinamento: batch 628 iniciou com 12:05:01.523873\n",
            "Treinamento: batch 628 terminou com 12:05:01.666869\n",
            " 629/1875 [=========>....................] - ETA: 3:05 - loss: 1.3233 - accuracy: 0.4636Treinamento: batch 629 iniciou com 12:05:01.668527\n",
            "Treinamento: batch 629 terminou com 12:05:01.811016\n",
            " 630/1875 [=========>....................] - ETA: 3:04 - loss: 1.3227 - accuracy: 0.4638Treinamento: batch 630 iniciou com 12:05:01.813066\n",
            "Treinamento: batch 630 terminou com 12:05:01.961737\n",
            " 631/1875 [=========>....................] - ETA: 3:04 - loss: 1.3220 - accuracy: 0.4639Treinamento: batch 631 iniciou com 12:05:01.963462\n",
            "Treinamento: batch 631 terminou com 12:05:02.111768\n",
            " 632/1875 [=========>....................] - ETA: 3:04 - loss: 1.3215 - accuracy: 0.4640Treinamento: batch 632 iniciou com 12:05:02.116091\n",
            "Treinamento: batch 632 terminou com 12:05:02.264220\n",
            " 633/1875 [=========>....................] - ETA: 3:04 - loss: 1.3210 - accuracy: 0.4643Treinamento: batch 633 iniciou com 12:05:02.266079\n",
            "Treinamento: batch 633 terminou com 12:05:02.410442\n",
            " 634/1875 [=========>....................] - ETA: 3:04 - loss: 1.3204 - accuracy: 0.4644Treinamento: batch 634 iniciou com 12:05:02.418079\n",
            "Treinamento: batch 634 terminou com 12:05:02.568591\n",
            " 635/1875 [=========>....................] - ETA: 3:04 - loss: 1.3196 - accuracy: 0.4648Treinamento: batch 635 iniciou com 12:05:02.570345\n",
            "Treinamento: batch 635 terminou com 12:05:02.719194\n",
            " 636/1875 [=========>....................] - ETA: 3:04 - loss: 1.3189 - accuracy: 0.4652Treinamento: batch 636 iniciou com 12:05:02.721313\n",
            "Treinamento: batch 636 terminou com 12:05:02.867690\n",
            " 637/1875 [=========>....................] - ETA: 3:03 - loss: 1.3185 - accuracy: 0.4654Treinamento: batch 637 iniciou com 12:05:02.869348\n",
            "Treinamento: batch 637 terminou com 12:05:03.026039\n",
            " 638/1875 [=========>....................] - ETA: 3:03 - loss: 1.3181 - accuracy: 0.4656Treinamento: batch 638 iniciou com 12:05:03.029154\n",
            "Treinamento: batch 638 terminou com 12:05:03.177401\n",
            " 639/1875 [=========>....................] - ETA: 3:03 - loss: 1.3171 - accuracy: 0.4661Treinamento: batch 639 iniciou com 12:05:03.179277\n",
            "Treinamento: batch 639 terminou com 12:05:03.323520\n",
            " 640/1875 [=========>....................] - ETA: 3:03 - loss: 1.3164 - accuracy: 0.4664Treinamento: batch 640 iniciou com 12:05:03.325910\n",
            "Treinamento: batch 640 terminou com 12:05:03.478064\n",
            " 641/1875 [=========>....................] - ETA: 3:03 - loss: 1.3167 - accuracy: 0.4664Treinamento: batch 641 iniciou com 12:05:03.480446\n",
            "Treinamento: batch 641 terminou com 12:05:03.628583\n",
            " 642/1875 [=========>....................] - ETA: 3:03 - loss: 1.3158 - accuracy: 0.4668Treinamento: batch 642 iniciou com 12:05:03.630572\n",
            "Treinamento: batch 642 terminou com 12:05:03.775277\n",
            " 643/1875 [=========>....................] - ETA: 3:03 - loss: 1.3155 - accuracy: 0.4669Treinamento: batch 643 iniciou com 12:05:03.777029\n",
            "Treinamento: batch 643 terminou com 12:05:03.923038\n",
            " 644/1875 [=========>....................] - ETA: 3:02 - loss: 1.3148 - accuracy: 0.4672Treinamento: batch 644 iniciou com 12:05:03.924916\n",
            "Treinamento: batch 644 terminou com 12:05:04.093802\n",
            " 645/1875 [=========>....................] - ETA: 3:02 - loss: 1.3145 - accuracy: 0.4673Treinamento: batch 645 iniciou com 12:05:04.095519\n",
            "Treinamento: batch 645 terminou com 12:05:04.246572\n",
            " 646/1875 [=========>....................] - ETA: 3:02 - loss: 1.3146 - accuracy: 0.4673Treinamento: batch 646 iniciou com 12:05:04.248394\n",
            "Treinamento: batch 646 terminou com 12:05:04.395812\n",
            " 647/1875 [=========>....................] - ETA: 3:02 - loss: 1.3139 - accuracy: 0.4676Treinamento: batch 647 iniciou com 12:05:04.397594\n",
            "Treinamento: batch 647 terminou com 12:05:04.552224\n",
            " 648/1875 [=========>....................] - ETA: 3:02 - loss: 1.3137 - accuracy: 0.4677Treinamento: batch 648 iniciou com 12:05:04.556248\n",
            "Treinamento: batch 648 terminou com 12:05:04.695835\n",
            " 649/1875 [=========>....................] - ETA: 3:02 - loss: 1.3126 - accuracy: 0.4683Treinamento: batch 649 iniciou com 12:05:04.697514\n",
            "Treinamento: batch 649 terminou com 12:05:04.842520\n",
            " 650/1875 [=========>....................] - ETA: 3:02 - loss: 1.3124 - accuracy: 0.4684Treinamento: batch 650 iniciou com 12:05:04.844466\n",
            "Treinamento: batch 650 terminou com 12:05:04.984537\n",
            " 651/1875 [=========>....................] - ETA: 3:01 - loss: 1.3116 - accuracy: 0.4688Treinamento: batch 651 iniciou com 12:05:04.986315\n",
            "Treinamento: batch 651 terminou com 12:05:05.128112\n",
            " 652/1875 [=========>....................] - ETA: 3:01 - loss: 1.3108 - accuracy: 0.4691Treinamento: batch 652 iniciou com 12:05:05.129914\n",
            "Treinamento: batch 652 terminou com 12:05:05.278809\n",
            " 653/1875 [=========>....................] - ETA: 3:01 - loss: 1.3104 - accuracy: 0.4694Treinamento: batch 653 iniciou com 12:05:05.280512\n",
            "Treinamento: batch 653 terminou com 12:05:05.430817\n",
            " 654/1875 [=========>....................] - ETA: 3:01 - loss: 1.3100 - accuracy: 0.4697Treinamento: batch 654 iniciou com 12:05:05.432577\n",
            "Treinamento: batch 654 terminou com 12:05:05.588504\n",
            " 655/1875 [=========>....................] - ETA: 3:01 - loss: 1.3091 - accuracy: 0.4701Treinamento: batch 655 iniciou com 12:05:05.590574\n",
            "Treinamento: batch 655 terminou com 12:05:05.732733\n",
            " 656/1875 [=========>....................] - ETA: 3:01 - loss: 1.3093 - accuracy: 0.4700Treinamento: batch 656 iniciou com 12:05:05.734457\n",
            "Treinamento: batch 656 terminou com 12:05:05.879162\n",
            " 657/1875 [=========>....................] - ETA: 3:01 - loss: 1.3085 - accuracy: 0.4702Treinamento: batch 657 iniciou com 12:05:05.880771\n",
            "Treinamento: batch 657 terminou com 12:05:06.020421\n",
            " 658/1875 [=========>....................] - ETA: 3:00 - loss: 1.3082 - accuracy: 0.4702Treinamento: batch 658 iniciou com 12:05:06.022333\n",
            "Treinamento: batch 658 terminou com 12:05:06.166172\n",
            " 659/1875 [=========>....................] - ETA: 3:00 - loss: 1.3075 - accuracy: 0.4706Treinamento: batch 659 iniciou com 12:05:06.168034\n",
            "Treinamento: batch 659 terminou com 12:05:06.314068\n",
            " 660/1875 [=========>....................] - ETA: 3:00 - loss: 1.3071 - accuracy: 0.4708Treinamento: batch 660 iniciou com 12:05:06.315819\n",
            "Treinamento: batch 660 terminou com 12:05:06.456656\n",
            " 661/1875 [=========>....................] - ETA: 3:00 - loss: 1.3068 - accuracy: 0.4709Treinamento: batch 661 iniciou com 12:05:06.458470\n",
            "Treinamento: batch 661 terminou com 12:05:06.615926\n",
            " 662/1875 [=========>....................] - ETA: 3:00 - loss: 1.3068 - accuracy: 0.4712Treinamento: batch 662 iniciou com 12:05:06.617709\n",
            "Treinamento: batch 662 terminou com 12:05:06.771193\n",
            " 663/1875 [=========>....................] - ETA: 3:00 - loss: 1.3060 - accuracy: 0.4716Treinamento: batch 663 iniciou com 12:05:06.773200\n",
            "Treinamento: batch 663 terminou com 12:05:06.917194\n",
            " 664/1875 [=========>....................] - ETA: 3:00 - loss: 1.3054 - accuracy: 0.4718Treinamento: batch 664 iniciou com 12:05:06.919397\n",
            "Treinamento: batch 664 terminou com 12:05:07.061683\n",
            " 665/1875 [=========>....................] - ETA: 2:59 - loss: 1.3048 - accuracy: 0.4723Treinamento: batch 665 iniciou com 12:05:07.063847\n",
            "Treinamento: batch 665 terminou com 12:05:07.205454\n",
            " 666/1875 [=========>....................] - ETA: 2:59 - loss: 1.3044 - accuracy: 0.4724Treinamento: batch 666 iniciou com 12:05:07.207326\n",
            "Treinamento: batch 666 terminou com 12:05:07.352162\n",
            " 667/1875 [=========>....................] - ETA: 2:59 - loss: 1.3035 - accuracy: 0.4728Treinamento: batch 667 iniciou com 12:05:07.354233\n",
            "Treinamento: batch 667 terminou com 12:05:07.496148\n",
            " 668/1875 [=========>....................] - ETA: 2:59 - loss: 1.3027 - accuracy: 0.4730Treinamento: batch 668 iniciou com 12:05:07.497902\n",
            "Treinamento: batch 668 terminou com 12:05:07.652746\n",
            " 669/1875 [=========>....................] - ETA: 2:59 - loss: 1.3017 - accuracy: 0.4734Treinamento: batch 669 iniciou com 12:05:07.654466\n",
            "Treinamento: batch 669 terminou com 12:05:07.795772\n",
            " 670/1875 [=========>....................] - ETA: 2:59 - loss: 1.3013 - accuracy: 0.4736Treinamento: batch 670 iniciou com 12:05:07.797539\n",
            "Treinamento: batch 670 terminou com 12:05:07.942845\n",
            " 671/1875 [=========>....................] - ETA: 2:58 - loss: 1.3005 - accuracy: 0.4741Treinamento: batch 671 iniciou com 12:05:07.944558\n",
            "Treinamento: batch 671 terminou com 12:05:08.088230\n",
            " 672/1875 [=========>....................] - ETA: 2:58 - loss: 1.3000 - accuracy: 0.4742Treinamento: batch 672 iniciou com 12:05:08.089976\n",
            "Treinamento: batch 672 terminou com 12:05:08.238369\n",
            " 673/1875 [=========>....................] - ETA: 2:58 - loss: 1.2993 - accuracy: 0.4746Treinamento: batch 673 iniciou com 12:05:08.240386\n",
            "Treinamento: batch 673 terminou com 12:05:08.385435\n",
            " 674/1875 [=========>....................] - ETA: 2:58 - loss: 1.2993 - accuracy: 0.4745Treinamento: batch 674 iniciou com 12:05:08.387189\n",
            "Treinamento: batch 674 terminou com 12:05:08.528995\n",
            " 675/1875 [=========>....................] - ETA: 2:58 - loss: 1.2988 - accuracy: 0.4747Treinamento: batch 675 iniciou com 12:05:08.533706\n",
            "Treinamento: batch 675 terminou com 12:05:08.681103\n",
            " 676/1875 [=========>....................] - ETA: 2:58 - loss: 1.2984 - accuracy: 0.4748Treinamento: batch 676 iniciou com 12:05:08.682766\n",
            "Treinamento: batch 676 terminou com 12:05:08.824093\n",
            " 677/1875 [=========>....................] - ETA: 2:58 - loss: 1.2977 - accuracy: 0.4751Treinamento: batch 677 iniciou com 12:05:08.825937\n",
            "Treinamento: batch 677 terminou com 12:05:08.969376\n",
            " 678/1875 [=========>....................] - ETA: 2:57 - loss: 1.2973 - accuracy: 0.4752Treinamento: batch 678 iniciou com 12:05:08.971351\n",
            "Treinamento: batch 678 terminou com 12:05:09.112977\n",
            " 679/1875 [=========>....................] - ETA: 2:57 - loss: 1.2965 - accuracy: 0.4755Treinamento: batch 679 iniciou com 12:05:09.114826\n",
            "Treinamento: batch 679 terminou com 12:05:09.255144\n",
            " 680/1875 [=========>....................] - ETA: 2:57 - loss: 1.2958 - accuracy: 0.4756Treinamento: batch 680 iniciou com 12:05:09.257013\n",
            "Treinamento: batch 680 terminou com 12:05:09.400033\n",
            " 681/1875 [=========>....................] - ETA: 2:57 - loss: 1.2951 - accuracy: 0.4759Treinamento: batch 681 iniciou com 12:05:09.401715\n",
            "Treinamento: batch 681 terminou com 12:05:09.548813\n",
            " 682/1875 [=========>....................] - ETA: 2:57 - loss: 1.2948 - accuracy: 0.4761Treinamento: batch 682 iniciou com 12:05:09.550667\n",
            "Treinamento: batch 682 terminou com 12:05:09.703406\n",
            " 683/1875 [=========>....................] - ETA: 2:57 - loss: 1.2940 - accuracy: 0.4765Treinamento: batch 683 iniciou com 12:05:09.705449\n",
            "Treinamento: batch 683 terminou com 12:05:09.852863\n",
            " 684/1875 [=========>....................] - ETA: 2:56 - loss: 1.2936 - accuracy: 0.4766Treinamento: batch 684 iniciou com 12:05:09.854674\n",
            "Treinamento: batch 684 terminou com 12:05:09.998985\n",
            " 685/1875 [=========>....................] - ETA: 2:56 - loss: 1.2927 - accuracy: 0.4771Treinamento: batch 685 iniciou com 12:05:10.000618\n",
            "Treinamento: batch 685 terminou com 12:05:10.145654\n",
            " 686/1875 [=========>....................] - ETA: 2:56 - loss: 1.2919 - accuracy: 0.4775Treinamento: batch 686 iniciou com 12:05:10.147771\n",
            "Treinamento: batch 686 terminou com 12:05:10.295933\n",
            " 687/1875 [=========>....................] - ETA: 2:56 - loss: 1.2912 - accuracy: 0.4777Treinamento: batch 687 iniciou com 12:05:10.297684\n",
            "Treinamento: batch 687 terminou com 12:05:10.446847\n",
            " 688/1875 [==========>...................] - ETA: 2:56 - loss: 1.2905 - accuracy: 0.4782Treinamento: batch 688 iniciou com 12:05:10.450000\n",
            "Treinamento: batch 688 terminou com 12:05:10.603803\n",
            " 689/1875 [==========>...................] - ETA: 2:56 - loss: 1.2899 - accuracy: 0.4785Treinamento: batch 689 iniciou com 12:05:10.605483\n",
            "Treinamento: batch 689 terminou com 12:05:10.750445\n",
            " 690/1875 [==========>...................] - ETA: 2:56 - loss: 1.2892 - accuracy: 0.4788Treinamento: batch 690 iniciou com 12:05:10.752167\n",
            "Treinamento: batch 690 terminou com 12:05:10.897890\n",
            " 691/1875 [==========>...................] - ETA: 2:55 - loss: 1.2887 - accuracy: 0.4791Treinamento: batch 691 iniciou com 12:05:10.899403\n",
            "Treinamento: batch 691 terminou com 12:05:11.036336\n",
            " 692/1875 [==========>...................] - ETA: 2:55 - loss: 1.2881 - accuracy: 0.4794Treinamento: batch 692 iniciou com 12:05:11.037840\n",
            "Treinamento: batch 692 terminou com 12:05:11.181560\n",
            " 693/1875 [==========>...................] - ETA: 2:55 - loss: 1.2875 - accuracy: 0.4796Treinamento: batch 693 iniciou com 12:05:11.182857\n",
            "Treinamento: batch 693 terminou com 12:05:11.337320\n",
            " 694/1875 [==========>...................] - ETA: 2:55 - loss: 1.2872 - accuracy: 0.4797Treinamento: batch 694 iniciou com 12:05:11.340403\n",
            "Treinamento: batch 694 terminou com 12:05:11.473952\n",
            " 695/1875 [==========>...................] - ETA: 2:55 - loss: 1.2868 - accuracy: 0.4799Treinamento: batch 695 iniciou com 12:05:11.475371\n",
            "Treinamento: batch 695 terminou com 12:05:11.615608\n",
            " 696/1875 [==========>...................] - ETA: 2:55 - loss: 1.2864 - accuracy: 0.4800Treinamento: batch 696 iniciou com 12:05:11.619343\n",
            "Treinamento: batch 696 terminou com 12:05:11.750342\n",
            " 697/1875 [==========>...................] - ETA: 2:54 - loss: 1.2859 - accuracy: 0.4803Treinamento: batch 697 iniciou com 12:05:11.751771\n",
            "Treinamento: batch 697 terminou com 12:05:11.894972\n",
            " 698/1875 [==========>...................] - ETA: 2:54 - loss: 1.2854 - accuracy: 0.4803Treinamento: batch 698 iniciou com 12:05:11.896773\n",
            "Treinamento: batch 698 terminou com 12:05:12.046373\n",
            " 699/1875 [==========>...................] - ETA: 2:54 - loss: 1.2850 - accuracy: 0.4804Treinamento: batch 699 iniciou com 12:05:12.048121\n",
            "Treinamento: batch 699 terminou com 12:05:12.192955\n",
            " 700/1875 [==========>...................] - ETA: 2:54 - loss: 1.2847 - accuracy: 0.4806Treinamento: batch 700 iniciou com 12:05:12.194743\n",
            "Treinamento: batch 700 terminou com 12:05:12.349683\n",
            " 701/1875 [==========>...................] - ETA: 2:54 - loss: 1.2843 - accuracy: 0.4810Treinamento: batch 701 iniciou com 12:05:12.351498\n",
            "Treinamento: batch 701 terminou com 12:05:12.497077\n",
            " 702/1875 [==========>...................] - ETA: 2:54 - loss: 1.2836 - accuracy: 0.4813Treinamento: batch 702 iniciou com 12:05:12.498800\n",
            "Treinamento: batch 702 terminou com 12:05:12.655781\n",
            " 703/1875 [==========>...................] - ETA: 2:54 - loss: 1.2832 - accuracy: 0.4816Treinamento: batch 703 iniciou com 12:05:12.657515\n",
            "Treinamento: batch 703 terminou com 12:05:12.799029\n",
            " 704/1875 [==========>...................] - ETA: 2:53 - loss: 1.2825 - accuracy: 0.4819Treinamento: batch 704 iniciou com 12:05:12.800738\n",
            "Treinamento: batch 704 terminou com 12:05:12.946042\n",
            " 705/1875 [==========>...................] - ETA: 2:53 - loss: 1.2822 - accuracy: 0.4821Treinamento: batch 705 iniciou com 12:05:12.947929\n",
            "Treinamento: batch 705 terminou com 12:05:13.094562\n",
            " 706/1875 [==========>...................] - ETA: 2:53 - loss: 1.2819 - accuracy: 0.4822Treinamento: batch 706 iniciou com 12:05:13.096417\n",
            "Treinamento: batch 706 terminou com 12:05:13.243273\n",
            " 707/1875 [==========>...................] - ETA: 2:53 - loss: 1.2817 - accuracy: 0.4824Treinamento: batch 707 iniciou com 12:05:13.245011\n",
            "Treinamento: batch 707 terminou com 12:05:13.392740\n",
            " 708/1875 [==========>...................] - ETA: 2:53 - loss: 1.2810 - accuracy: 0.4826Treinamento: batch 708 iniciou com 12:05:13.397376\n",
            "Treinamento: batch 708 terminou com 12:05:13.537773\n",
            " 709/1875 [==========>...................] - ETA: 2:53 - loss: 1.2805 - accuracy: 0.4828Treinamento: batch 709 iniciou com 12:05:13.539366\n",
            "Treinamento: batch 709 terminou com 12:05:13.687588\n",
            " 710/1875 [==========>...................] - ETA: 2:53 - loss: 1.2796 - accuracy: 0.4832Treinamento: batch 710 iniciou com 12:05:13.689328\n",
            "Treinamento: batch 710 terminou com 12:05:13.836231\n",
            " 711/1875 [==========>...................] - ETA: 2:52 - loss: 1.2788 - accuracy: 0.4836Treinamento: batch 711 iniciou com 12:05:13.838006\n",
            "Treinamento: batch 711 terminou com 12:05:13.983390\n",
            " 712/1875 [==========>...................] - ETA: 2:52 - loss: 1.2780 - accuracy: 0.4841Treinamento: batch 712 iniciou com 12:05:13.985149\n",
            "Treinamento: batch 712 terminou com 12:05:14.128479\n",
            " 713/1875 [==========>...................] - ETA: 2:52 - loss: 1.2777 - accuracy: 0.4842Treinamento: batch 713 iniciou com 12:05:14.130372\n",
            "Treinamento: batch 713 terminou com 12:05:14.276048\n",
            " 714/1875 [==========>...................] - ETA: 2:52 - loss: 1.2772 - accuracy: 0.4845Treinamento: batch 714 iniciou com 12:05:14.277910\n",
            "Treinamento: batch 714 terminou com 12:05:14.459082\n",
            " 715/1875 [==========>...................] - ETA: 2:52 - loss: 1.2764 - accuracy: 0.4847Treinamento: batch 715 iniciou com 12:05:14.461311\n",
            "Treinamento: batch 715 terminou com 12:05:14.607904\n",
            " 716/1875 [==========>...................] - ETA: 2:52 - loss: 1.2761 - accuracy: 0.4850Treinamento: batch 716 iniciou com 12:05:14.609682\n",
            "Treinamento: batch 716 terminou com 12:05:14.763848\n",
            " 717/1875 [==========>...................] - ETA: 2:52 - loss: 1.2754 - accuracy: 0.4853Treinamento: batch 717 iniciou com 12:05:14.765553\n",
            "Treinamento: batch 717 terminou com 12:05:14.912887\n",
            " 718/1875 [==========>...................] - ETA: 2:51 - loss: 1.2751 - accuracy: 0.4854Treinamento: batch 718 iniciou com 12:05:14.914499\n",
            "Treinamento: batch 718 terminou com 12:05:15.058490\n",
            " 719/1875 [==========>...................] - ETA: 2:51 - loss: 1.2742 - accuracy: 0.4857Treinamento: batch 719 iniciou com 12:05:15.060324\n",
            "Treinamento: batch 719 terminou com 12:05:15.206955\n",
            " 720/1875 [==========>...................] - ETA: 2:51 - loss: 1.2737 - accuracy: 0.4863Treinamento: batch 720 iniciou com 12:05:15.208619\n",
            "Treinamento: batch 720 terminou com 12:05:15.355611\n",
            " 721/1875 [==========>...................] - ETA: 2:51 - loss: 1.2733 - accuracy: 0.4864Treinamento: batch 721 iniciou com 12:05:15.357340\n",
            "Treinamento: batch 721 terminou com 12:05:15.497330\n",
            " 722/1875 [==========>...................] - ETA: 2:51 - loss: 1.2727 - accuracy: 0.4868Treinamento: batch 722 iniciou com 12:05:15.499155\n",
            "Treinamento: batch 722 terminou com 12:05:15.636550\n",
            " 723/1875 [==========>...................] - ETA: 2:51 - loss: 1.2723 - accuracy: 0.4869Treinamento: batch 723 iniciou com 12:05:15.638387\n",
            "Treinamento: batch 723 terminou com 12:05:15.790887\n",
            " 724/1875 [==========>...................] - ETA: 2:51 - loss: 1.2716 - accuracy: 0.4874Treinamento: batch 724 iniciou com 12:05:15.792604\n",
            "Treinamento: batch 724 terminou com 12:05:15.944763\n",
            " 725/1875 [==========>...................] - ETA: 2:50 - loss: 1.2715 - accuracy: 0.4876Treinamento: batch 725 iniciou com 12:05:15.946525\n",
            "Treinamento: batch 725 terminou com 12:05:16.094168\n",
            " 726/1875 [==========>...................] - ETA: 2:50 - loss: 1.2709 - accuracy: 0.4879Treinamento: batch 726 iniciou com 12:05:16.095877\n",
            "Treinamento: batch 726 terminou com 12:05:16.236898\n",
            " 727/1875 [==========>...................] - ETA: 2:50 - loss: 1.2706 - accuracy: 0.4880Treinamento: batch 727 iniciou com 12:05:16.238612\n",
            "Treinamento: batch 727 terminou com 12:05:16.384387\n",
            " 728/1875 [==========>...................] - ETA: 2:50 - loss: 1.2700 - accuracy: 0.4882Treinamento: batch 728 iniciou com 12:05:16.386139\n",
            "Treinamento: batch 728 terminou com 12:05:16.527565\n",
            " 729/1875 [==========>...................] - ETA: 2:50 - loss: 1.2695 - accuracy: 0.4884Treinamento: batch 729 iniciou com 12:05:16.529297\n",
            "Treinamento: batch 729 terminou com 12:05:16.675934\n",
            " 730/1875 [==========>...................] - ETA: 2:50 - loss: 1.2688 - accuracy: 0.4886Treinamento: batch 730 iniciou com 12:05:16.677668\n",
            "Treinamento: batch 730 terminou com 12:05:16.828816\n",
            " 731/1875 [==========>...................] - ETA: 2:49 - loss: 1.2682 - accuracy: 0.4888Treinamento: batch 731 iniciou com 12:05:16.830556\n",
            "Treinamento: batch 731 terminou com 12:05:16.974473\n",
            " 732/1875 [==========>...................] - ETA: 2:49 - loss: 1.2677 - accuracy: 0.4889Treinamento: batch 732 iniciou com 12:05:16.976118\n",
            "Treinamento: batch 732 terminou com 12:05:17.123439\n",
            " 733/1875 [==========>...................] - ETA: 2:49 - loss: 1.2675 - accuracy: 0.4889Treinamento: batch 733 iniciou com 12:05:17.125248\n",
            "Treinamento: batch 733 terminou com 12:05:17.268842\n",
            " 734/1875 [==========>...................] - ETA: 2:49 - loss: 1.2666 - accuracy: 0.4891Treinamento: batch 734 iniciou com 12:05:17.270507\n",
            "Treinamento: batch 734 terminou com 12:05:17.412528\n",
            " 735/1875 [==========>...................] - ETA: 2:49 - loss: 1.2663 - accuracy: 0.4893Treinamento: batch 735 iniciou com 12:05:17.414163\n",
            "Treinamento: batch 735 terminou com 12:05:17.567231\n",
            " 736/1875 [==========>...................] - ETA: 2:49 - loss: 1.2656 - accuracy: 0.4896Treinamento: batch 736 iniciou com 12:05:17.572254\n",
            "Treinamento: batch 736 terminou com 12:05:17.715309\n",
            " 737/1875 [==========>...................] - ETA: 2:49 - loss: 1.2658 - accuracy: 0.4896Treinamento: batch 737 iniciou com 12:05:17.717958\n",
            "Treinamento: batch 737 terminou com 12:05:17.870861\n",
            " 738/1875 [==========>...................] - ETA: 2:48 - loss: 1.2657 - accuracy: 0.4897Treinamento: batch 738 iniciou com 12:05:17.872612\n",
            "Treinamento: batch 738 terminou com 12:05:18.016534\n",
            " 739/1875 [==========>...................] - ETA: 2:48 - loss: 1.2652 - accuracy: 0.4899Treinamento: batch 739 iniciou com 12:05:18.018395\n",
            "Treinamento: batch 739 terminou com 12:05:18.161888\n",
            " 740/1875 [==========>...................] - ETA: 2:48 - loss: 1.2647 - accuracy: 0.4901Treinamento: batch 740 iniciou com 12:05:18.163618\n",
            "Treinamento: batch 740 terminou com 12:05:18.308023\n",
            " 741/1875 [==========>...................] - ETA: 2:48 - loss: 1.2647 - accuracy: 0.4901Treinamento: batch 741 iniciou com 12:05:18.309827\n",
            "Treinamento: batch 741 terminou com 12:05:18.454898\n",
            " 742/1875 [==========>...................] - ETA: 2:48 - loss: 1.2641 - accuracy: 0.4904Treinamento: batch 742 iniciou com 12:05:18.456607\n",
            "Treinamento: batch 742 terminou com 12:05:18.600017\n",
            " 743/1875 [==========>...................] - ETA: 2:48 - loss: 1.2638 - accuracy: 0.4905Treinamento: batch 743 iniciou com 12:05:18.601768\n",
            "Treinamento: batch 743 terminou com 12:05:18.744550\n",
            " 744/1875 [==========>...................] - ETA: 2:48 - loss: 1.2629 - accuracy: 0.4908Treinamento: batch 744 iniciou com 12:05:18.746352\n",
            "Treinamento: batch 744 terminou com 12:05:18.911583\n",
            " 745/1875 [==========>...................] - ETA: 2:47 - loss: 1.2622 - accuracy: 0.4911Treinamento: batch 745 iniciou com 12:05:18.913366\n",
            "Treinamento: batch 745 terminou com 12:05:19.061472\n",
            " 746/1875 [==========>...................] - ETA: 2:47 - loss: 1.2619 - accuracy: 0.4912Treinamento: batch 746 iniciou com 12:05:19.063154\n",
            "Treinamento: batch 746 terminou com 12:05:19.203999\n",
            " 747/1875 [==========>...................] - ETA: 2:47 - loss: 1.2616 - accuracy: 0.4913Treinamento: batch 747 iniciou com 12:05:19.205817\n",
            "Treinamento: batch 747 terminou com 12:05:19.349556\n",
            " 748/1875 [==========>...................] - ETA: 2:47 - loss: 1.2610 - accuracy: 0.4916Treinamento: batch 748 iniciou com 12:05:19.351344\n",
            "Treinamento: batch 748 terminou com 12:05:19.495124\n",
            " 749/1875 [==========>...................] - ETA: 2:47 - loss: 1.2608 - accuracy: 0.4917Treinamento: batch 749 iniciou com 12:05:19.497153\n",
            "Treinamento: batch 749 terminou com 12:05:19.640421\n",
            " 750/1875 [===========>..................] - ETA: 2:47 - loss: 1.2606 - accuracy: 0.4918Treinamento: batch 750 iniciou com 12:05:19.642140\n",
            "Treinamento: batch 750 terminou com 12:05:19.796304\n",
            " 751/1875 [===========>..................] - ETA: 2:46 - loss: 1.2604 - accuracy: 0.4918Treinamento: batch 751 iniciou com 12:05:19.798003\n",
            "Treinamento: batch 751 terminou com 12:05:19.947074\n",
            " 752/1875 [===========>..................] - ETA: 2:46 - loss: 1.2600 - accuracy: 0.4920Treinamento: batch 752 iniciou com 12:05:19.948766\n",
            "Treinamento: batch 752 terminou com 12:05:20.093457\n",
            " 753/1875 [===========>..................] - ETA: 2:46 - loss: 1.2595 - accuracy: 0.4920Treinamento: batch 753 iniciou com 12:05:20.095247\n",
            "Treinamento: batch 753 terminou com 12:05:20.246387\n",
            " 754/1875 [===========>..................] - ETA: 2:46 - loss: 1.2594 - accuracy: 0.4920Treinamento: batch 754 iniciou com 12:05:20.248124\n",
            "Treinamento: batch 754 terminou com 12:05:20.393024\n",
            " 755/1875 [===========>..................] - ETA: 2:46 - loss: 1.2592 - accuracy: 0.4921Treinamento: batch 755 iniciou com 12:05:20.394789\n",
            "Treinamento: batch 755 terminou com 12:05:20.539018\n",
            " 756/1875 [===========>..................] - ETA: 2:46 - loss: 1.2585 - accuracy: 0.4924Treinamento: batch 756 iniciou com 12:05:20.540724\n",
            "Treinamento: batch 756 terminou com 12:05:20.684843\n",
            " 757/1875 [===========>..................] - ETA: 2:46 - loss: 1.2584 - accuracy: 0.4925Treinamento: batch 757 iniciou com 12:05:20.686533\n",
            "Treinamento: batch 757 terminou com 12:05:20.836923\n",
            " 758/1875 [===========>..................] - ETA: 2:45 - loss: 1.2578 - accuracy: 0.4929Treinamento: batch 758 iniciou com 12:05:20.838671\n",
            "Treinamento: batch 758 terminou com 12:05:20.985337\n",
            " 759/1875 [===========>..................] - ETA: 2:45 - loss: 1.2574 - accuracy: 0.4932Treinamento: batch 759 iniciou com 12:05:20.987058\n",
            "Treinamento: batch 759 terminou com 12:05:21.133905\n",
            " 760/1875 [===========>..................] - ETA: 2:45 - loss: 1.2570 - accuracy: 0.4934Treinamento: batch 760 iniciou com 12:05:21.135616\n",
            "Treinamento: batch 760 terminou com 12:05:21.283770\n",
            " 761/1875 [===========>..................] - ETA: 2:45 - loss: 1.2568 - accuracy: 0.4936Treinamento: batch 761 iniciou com 12:05:21.285378\n",
            "Treinamento: batch 761 terminou com 12:05:21.427731\n",
            " 762/1875 [===========>..................] - ETA: 2:45 - loss: 1.2566 - accuracy: 0.4937Treinamento: batch 762 iniciou com 12:05:21.429476\n",
            "Treinamento: batch 762 terminou com 12:05:21.572232\n",
            " 763/1875 [===========>..................] - ETA: 2:45 - loss: 1.2559 - accuracy: 0.4939Treinamento: batch 763 iniciou com 12:05:21.574209\n",
            "Treinamento: batch 763 terminou com 12:05:21.726940\n",
            " 764/1875 [===========>..................] - ETA: 2:45 - loss: 1.2553 - accuracy: 0.4940Treinamento: batch 764 iniciou com 12:05:21.728617\n",
            "Treinamento: batch 764 terminou com 12:05:21.880147\n",
            " 765/1875 [===========>..................] - ETA: 2:44 - loss: 1.2551 - accuracy: 0.4941Treinamento: batch 765 iniciou com 12:05:21.881854\n",
            "Treinamento: batch 765 terminou com 12:05:22.028575\n",
            " 766/1875 [===========>..................] - ETA: 2:44 - loss: 1.2544 - accuracy: 0.4944Treinamento: batch 766 iniciou com 12:05:22.030336\n",
            "Treinamento: batch 766 terminou com 12:05:22.177513\n",
            " 767/1875 [===========>..................] - ETA: 2:44 - loss: 1.2539 - accuracy: 0.4947Treinamento: batch 767 iniciou com 12:05:22.179269\n",
            "Treinamento: batch 767 terminou com 12:05:22.323433\n",
            " 768/1875 [===========>..................] - ETA: 2:44 - loss: 1.2532 - accuracy: 0.4952Treinamento: batch 768 iniciou com 12:05:22.325876\n",
            "Treinamento: batch 768 terminou com 12:05:22.469691\n",
            " 769/1875 [===========>..................] - ETA: 2:44 - loss: 1.2526 - accuracy: 0.4953Treinamento: batch 769 iniciou com 12:05:22.471365\n",
            "Treinamento: batch 769 terminou com 12:05:22.618011\n",
            " 770/1875 [===========>..................] - ETA: 2:44 - loss: 1.2519 - accuracy: 0.4956Treinamento: batch 770 iniciou com 12:05:22.620524\n",
            "Treinamento: batch 770 terminou com 12:05:22.759264\n",
            " 771/1875 [===========>..................] - ETA: 2:44 - loss: 1.2516 - accuracy: 0.4958Treinamento: batch 771 iniciou com 12:05:22.761114\n",
            "Treinamento: batch 771 terminou com 12:05:22.914818\n",
            " 772/1875 [===========>..................] - ETA: 2:43 - loss: 1.2510 - accuracy: 0.4960Treinamento: batch 772 iniciou com 12:05:22.916700\n",
            "Treinamento: batch 772 terminou com 12:05:23.070407\n",
            " 773/1875 [===========>..................] - ETA: 2:43 - loss: 1.2503 - accuracy: 0.4963Treinamento: batch 773 iniciou com 12:05:23.072450\n",
            "Treinamento: batch 773 terminou com 12:05:23.214446\n",
            " 774/1875 [===========>..................] - ETA: 2:43 - loss: 1.2500 - accuracy: 0.4964Treinamento: batch 774 iniciou com 12:05:23.218796\n",
            "Treinamento: batch 774 terminou com 12:05:23.361681\n",
            " 775/1875 [===========>..................] - ETA: 2:43 - loss: 1.2495 - accuracy: 0.4965Treinamento: batch 775 iniciou com 12:05:23.363818\n",
            "Treinamento: batch 775 terminou com 12:05:23.507673\n",
            " 776/1875 [===========>..................] - ETA: 2:43 - loss: 1.2487 - accuracy: 0.4968Treinamento: batch 776 iniciou com 12:05:23.509394\n",
            "Treinamento: batch 776 terminou com 12:05:23.654544\n",
            " 777/1875 [===========>..................] - ETA: 2:43 - loss: 1.2488 - accuracy: 0.4969Treinamento: batch 777 iniciou com 12:05:23.656394\n",
            "Treinamento: batch 777 terminou com 12:05:23.795584\n",
            " 778/1875 [===========>..................] - ETA: 2:42 - loss: 1.2480 - accuracy: 0.4973Treinamento: batch 778 iniciou com 12:05:23.797289\n",
            "Treinamento: batch 778 terminou com 12:05:23.955026\n",
            " 779/1875 [===========>..................] - ETA: 2:42 - loss: 1.2474 - accuracy: 0.4976Treinamento: batch 779 iniciou com 12:05:23.957112\n",
            "Treinamento: batch 779 terminou com 12:05:24.112996\n",
            " 780/1875 [===========>..................] - ETA: 2:42 - loss: 1.2471 - accuracy: 0.4976Treinamento: batch 780 iniciou com 12:05:24.114790\n",
            "Treinamento: batch 780 terminou com 12:05:24.260299\n",
            " 781/1875 [===========>..................] - ETA: 2:42 - loss: 1.2467 - accuracy: 0.4978Treinamento: batch 781 iniciou com 12:05:24.261957\n",
            "Treinamento: batch 781 terminou com 12:05:24.407546\n",
            " 782/1875 [===========>..................] - ETA: 2:42 - loss: 1.2462 - accuracy: 0.4980Treinamento: batch 782 iniciou com 12:05:24.409647\n",
            "Treinamento: batch 782 terminou com 12:05:24.553286\n",
            " 783/1875 [===========>..................] - ETA: 2:42 - loss: 1.2467 - accuracy: 0.4978Treinamento: batch 783 iniciou com 12:05:24.555046\n",
            "Treinamento: batch 783 terminou com 12:05:24.709927\n",
            " 784/1875 [===========>..................] - ETA: 2:42 - loss: 1.2464 - accuracy: 0.4981Treinamento: batch 784 iniciou com 12:05:24.711761\n",
            "Treinamento: batch 784 terminou com 12:05:24.886236\n",
            " 785/1875 [===========>..................] - ETA: 2:41 - loss: 1.2459 - accuracy: 0.4984Treinamento: batch 785 iniciou com 12:05:24.895484\n",
            "Treinamento: batch 785 terminou com 12:05:25.040087\n",
            " 786/1875 [===========>..................] - ETA: 2:41 - loss: 1.2454 - accuracy: 0.4986Treinamento: batch 786 iniciou com 12:05:25.041748\n",
            "Treinamento: batch 786 terminou com 12:05:25.183667\n",
            " 787/1875 [===========>..................] - ETA: 2:41 - loss: 1.2446 - accuracy: 0.4990Treinamento: batch 787 iniciou com 12:05:25.185862\n",
            "Treinamento: batch 787 terminou com 12:05:25.331041\n",
            " 788/1875 [===========>..................] - ETA: 2:41 - loss: 1.2440 - accuracy: 0.4991Treinamento: batch 788 iniciou com 12:05:25.332874\n",
            "Treinamento: batch 788 terminou com 12:05:25.483713\n",
            " 789/1875 [===========>..................] - ETA: 2:41 - loss: 1.2435 - accuracy: 0.4993Treinamento: batch 789 iniciou com 12:05:25.485587\n",
            "Treinamento: batch 789 terminou com 12:05:25.634351\n",
            " 790/1875 [===========>..................] - ETA: 2:41 - loss: 1.2429 - accuracy: 0.4996Treinamento: batch 790 iniciou com 12:05:25.636107\n",
            "Treinamento: batch 790 terminou com 12:05:25.780733\n",
            " 791/1875 [===========>..................] - ETA: 2:41 - loss: 1.2427 - accuracy: 0.4997Treinamento: batch 791 iniciou com 12:05:25.782340\n",
            "Treinamento: batch 791 terminou com 12:05:25.937884\n",
            " 792/1875 [===========>..................] - ETA: 2:40 - loss: 1.2420 - accuracy: 0.5000Treinamento: batch 792 iniciou com 12:05:25.939604\n",
            "Treinamento: batch 792 terminou com 12:05:26.081106\n",
            " 793/1875 [===========>..................] - ETA: 2:40 - loss: 1.2416 - accuracy: 0.5001Treinamento: batch 793 iniciou com 12:05:26.082877\n",
            "Treinamento: batch 793 terminou com 12:05:26.232564\n",
            " 794/1875 [===========>..................] - ETA: 2:40 - loss: 1.2412 - accuracy: 0.5003Treinamento: batch 794 iniciou com 12:05:26.234298\n",
            "Treinamento: batch 794 terminou com 12:05:26.378472\n",
            " 795/1875 [===========>..................] - ETA: 2:40 - loss: 1.2407 - accuracy: 0.5005Treinamento: batch 795 iniciou com 12:05:26.380371\n",
            "Treinamento: batch 795 terminou com 12:05:26.523675\n",
            " 796/1875 [===========>..................] - ETA: 2:40 - loss: 1.2404 - accuracy: 0.5007Treinamento: batch 796 iniciou com 12:05:26.526165\n",
            "Treinamento: batch 796 terminou com 12:05:26.669862\n",
            " 797/1875 [===========>..................] - ETA: 2:40 - loss: 1.2399 - accuracy: 0.5010Treinamento: batch 797 iniciou com 12:05:26.671916\n",
            "Treinamento: batch 797 terminou com 12:05:26.816860\n",
            " 798/1875 [===========>..................] - ETA: 2:40 - loss: 1.2392 - accuracy: 0.5012Treinamento: batch 798 iniciou com 12:05:26.819206\n",
            "Treinamento: batch 798 terminou com 12:05:26.969593\n",
            " 799/1875 [===========>..................] - ETA: 2:39 - loss: 1.2388 - accuracy: 0.5016Treinamento: batch 799 iniciou com 12:05:26.971386\n",
            "Treinamento: batch 799 terminou com 12:05:27.127056\n",
            " 800/1875 [===========>..................] - ETA: 2:39 - loss: 1.2389 - accuracy: 0.5015Treinamento: batch 800 iniciou com 12:05:27.128965\n",
            "Treinamento: batch 800 terminou com 12:05:27.268516\n",
            " 801/1875 [===========>..................] - ETA: 2:39 - loss: 1.2386 - accuracy: 0.5017Treinamento: batch 801 iniciou com 12:05:27.270156\n",
            "Treinamento: batch 801 terminou com 12:05:27.418289\n",
            " 802/1875 [===========>..................] - ETA: 2:39 - loss: 1.2379 - accuracy: 0.5020Treinamento: batch 802 iniciou com 12:05:27.420320\n",
            "Treinamento: batch 802 terminou com 12:05:27.561294\n",
            " 803/1875 [===========>..................] - ETA: 2:39 - loss: 1.2377 - accuracy: 0.5021Treinamento: batch 803 iniciou com 12:05:27.563494\n",
            "Treinamento: batch 803 terminou com 12:05:27.719744\n",
            " 804/1875 [===========>..................] - ETA: 2:39 - loss: 1.2373 - accuracy: 0.5023Treinamento: batch 804 iniciou com 12:05:27.721533\n",
            "Treinamento: batch 804 terminou com 12:05:27.867924\n",
            " 805/1875 [===========>..................] - ETA: 2:39 - loss: 1.2369 - accuracy: 0.5025Treinamento: batch 805 iniciou com 12:05:27.869671\n",
            "Treinamento: batch 805 terminou com 12:05:28.021066\n",
            " 806/1875 [===========>..................] - ETA: 2:38 - loss: 1.2371 - accuracy: 0.5024Treinamento: batch 806 iniciou com 12:05:28.022842\n",
            "Treinamento: batch 806 terminou com 12:05:28.168697\n",
            " 807/1875 [===========>..................] - ETA: 2:38 - loss: 1.2367 - accuracy: 0.5025Treinamento: batch 807 iniciou com 12:05:28.170326\n",
            "Treinamento: batch 807 terminou com 12:05:28.320752\n",
            " 808/1875 [===========>..................] - ETA: 2:38 - loss: 1.2364 - accuracy: 0.5026Treinamento: batch 808 iniciou com 12:05:28.322805\n",
            "Treinamento: batch 808 terminou com 12:05:28.468989\n",
            " 809/1875 [===========>..................] - ETA: 2:38 - loss: 1.2359 - accuracy: 0.5027Treinamento: batch 809 iniciou com 12:05:28.470780\n",
            "Treinamento: batch 809 terminou com 12:05:28.617824\n",
            " 810/1875 [===========>..................] - ETA: 2:38 - loss: 1.2358 - accuracy: 0.5027Treinamento: batch 810 iniciou com 12:05:28.619519\n",
            "Treinamento: batch 810 terminou com 12:05:28.762937\n",
            " 811/1875 [===========>..................] - ETA: 2:38 - loss: 1.2353 - accuracy: 0.5029Treinamento: batch 811 iniciou com 12:05:28.764603\n",
            "Treinamento: batch 811 terminou com 12:05:28.906661\n",
            " 812/1875 [===========>..................] - ETA: 2:37 - loss: 1.2349 - accuracy: 0.5031Treinamento: batch 812 iniciou com 12:05:28.908402\n",
            "Treinamento: batch 812 terminou com 12:05:29.064910\n",
            " 813/1875 [============>.................] - ETA: 2:37 - loss: 1.2344 - accuracy: 0.5032Treinamento: batch 813 iniciou com 12:05:29.066911\n",
            "Treinamento: batch 813 terminou com 12:05:29.206255\n",
            " 814/1875 [============>.................] - ETA: 2:37 - loss: 1.2340 - accuracy: 0.5034Treinamento: batch 814 iniciou com 12:05:29.208099\n",
            "Treinamento: batch 814 terminou com 12:05:29.351452\n",
            " 815/1875 [============>.................] - ETA: 2:37 - loss: 1.2337 - accuracy: 0.5036Treinamento: batch 815 iniciou com 12:05:29.353505\n",
            "Treinamento: batch 815 terminou com 12:05:29.496820\n",
            " 816/1875 [============>.................] - ETA: 2:37 - loss: 1.2335 - accuracy: 0.5036Treinamento: batch 816 iniciou com 12:05:29.498707\n",
            "Treinamento: batch 816 terminou com 12:05:29.642274\n",
            " 817/1875 [============>.................] - ETA: 2:37 - loss: 1.2329 - accuracy: 0.5039Treinamento: batch 817 iniciou com 12:05:29.643956\n",
            "Treinamento: batch 817 terminou com 12:05:29.787423\n",
            " 818/1875 [============>.................] - ETA: 2:37 - loss: 1.2325 - accuracy: 0.5040Treinamento: batch 818 iniciou com 12:05:29.789547\n",
            "Treinamento: batch 818 terminou com 12:05:29.939955\n",
            " 819/1875 [============>.................] - ETA: 2:36 - loss: 1.2322 - accuracy: 0.5042Treinamento: batch 819 iniciou com 12:05:29.941513\n",
            "Treinamento: batch 819 terminou com 12:05:30.094588\n",
            " 820/1875 [============>.................] - ETA: 2:36 - loss: 1.2319 - accuracy: 0.5043Treinamento: batch 820 iniciou com 12:05:30.096351\n",
            "Treinamento: batch 820 terminou com 12:05:30.238868\n",
            " 821/1875 [============>.................] - ETA: 2:36 - loss: 1.2318 - accuracy: 0.5043Treinamento: batch 821 iniciou com 12:05:30.240615\n",
            "Treinamento: batch 821 terminou com 12:05:30.382534\n",
            " 822/1875 [============>.................] - ETA: 2:36 - loss: 1.2314 - accuracy: 0.5046Treinamento: batch 822 iniciou com 12:05:30.384288\n",
            "Treinamento: batch 822 terminou com 12:05:30.527093\n",
            " 823/1875 [============>.................] - ETA: 2:36 - loss: 1.2314 - accuracy: 0.5046Treinamento: batch 823 iniciou com 12:05:30.528885\n",
            "Treinamento: batch 823 terminou com 12:05:30.671317\n",
            " 824/1875 [============>.................] - ETA: 2:36 - loss: 1.2309 - accuracy: 0.5048Treinamento: batch 824 iniciou com 12:05:30.673046\n",
            "Treinamento: batch 824 terminou com 12:05:30.818574\n",
            " 825/1875 [============>.................] - ETA: 2:36 - loss: 1.2304 - accuracy: 0.5050Treinamento: batch 825 iniciou com 12:05:30.820416\n",
            "Treinamento: batch 825 terminou com 12:05:30.963717\n",
            " 826/1875 [============>.................] - ETA: 2:35 - loss: 1.2299 - accuracy: 0.5052Treinamento: batch 826 iniciou com 12:05:30.965285\n",
            "Treinamento: batch 826 terminou com 12:05:31.120438\n",
            " 827/1875 [============>.................] - ETA: 2:35 - loss: 1.2298 - accuracy: 0.5053Treinamento: batch 827 iniciou com 12:05:31.122187\n",
            "Treinamento: batch 827 terminou com 12:05:31.263099\n",
            " 828/1875 [============>.................] - ETA: 2:35 - loss: 1.2292 - accuracy: 0.5055Treinamento: batch 828 iniciou com 12:05:31.264784\n",
            "Treinamento: batch 828 terminou com 12:05:31.404853\n",
            " 829/1875 [============>.................] - ETA: 2:35 - loss: 1.2286 - accuracy: 0.5060Treinamento: batch 829 iniciou com 12:05:31.406876\n",
            "Treinamento: batch 829 terminou com 12:05:31.556904\n",
            " 830/1875 [============>.................] - ETA: 2:35 - loss: 1.2284 - accuracy: 0.5059Treinamento: batch 830 iniciou com 12:05:31.558785\n",
            "Treinamento: batch 830 terminou com 12:05:31.701652\n",
            " 831/1875 [============>.................] - ETA: 2:35 - loss: 1.2279 - accuracy: 0.5063Treinamento: batch 831 iniciou com 12:05:31.703320\n",
            "Treinamento: batch 831 terminou com 12:05:31.845567\n",
            " 832/1875 [============>.................] - ETA: 2:34 - loss: 1.2278 - accuracy: 0.5065Treinamento: batch 832 iniciou com 12:05:31.847347\n",
            "Treinamento: batch 832 terminou com 12:05:31.989999\n",
            " 833/1875 [============>.................] - ETA: 2:34 - loss: 1.2277 - accuracy: 0.5066Treinamento: batch 833 iniciou com 12:05:31.991750\n",
            "Treinamento: batch 833 terminou com 12:05:32.144683\n",
            " 834/1875 [============>.................] - ETA: 2:34 - loss: 1.2273 - accuracy: 0.5069Treinamento: batch 834 iniciou com 12:05:32.146791\n",
            "Treinamento: batch 834 terminou com 12:05:32.294707\n",
            " 835/1875 [============>.................] - ETA: 2:34 - loss: 1.2270 - accuracy: 0.5071Treinamento: batch 835 iniciou com 12:05:32.296825\n",
            "Treinamento: batch 835 terminou com 12:05:32.439798\n",
            " 836/1875 [============>.................] - ETA: 2:34 - loss: 1.2264 - accuracy: 0.5074Treinamento: batch 836 iniciou com 12:05:32.441508\n",
            "Treinamento: batch 836 terminou com 12:05:32.586461\n",
            " 837/1875 [============>.................] - ETA: 2:34 - loss: 1.2260 - accuracy: 0.5076Treinamento: batch 837 iniciou com 12:05:32.588618\n",
            "Treinamento: batch 837 terminou com 12:05:32.728034\n",
            " 838/1875 [============>.................] - ETA: 2:34 - loss: 1.2255 - accuracy: 0.5078Treinamento: batch 838 iniciou com 12:05:32.730299\n",
            "Treinamento: batch 838 terminou com 12:05:32.871601\n",
            " 839/1875 [============>.................] - ETA: 2:33 - loss: 1.2249 - accuracy: 0.5080Treinamento: batch 839 iniciou com 12:05:32.873347\n",
            "Treinamento: batch 839 terminou com 12:05:33.014488\n",
            " 840/1875 [============>.................] - ETA: 2:33 - loss: 1.2244 - accuracy: 0.5083Treinamento: batch 840 iniciou com 12:05:33.016271\n",
            "Treinamento: batch 840 terminou com 12:05:33.170789\n",
            " 841/1875 [============>.................] - ETA: 2:33 - loss: 1.2239 - accuracy: 0.5085Treinamento: batch 841 iniciou com 12:05:33.172486\n",
            "Treinamento: batch 841 terminou com 12:05:33.334303\n",
            " 842/1875 [============>.................] - ETA: 2:33 - loss: 1.2240 - accuracy: 0.5085Treinamento: batch 842 iniciou com 12:05:33.342727\n",
            "Treinamento: batch 842 terminou com 12:05:33.488981\n",
            " 843/1875 [============>.................] - ETA: 2:33 - loss: 1.2241 - accuracy: 0.5085Treinamento: batch 843 iniciou com 12:05:33.490708\n",
            "Treinamento: batch 843 terminou com 12:05:33.639034\n",
            " 844/1875 [============>.................] - ETA: 2:33 - loss: 1.2235 - accuracy: 0.5088Treinamento: batch 844 iniciou com 12:05:33.640655\n",
            "Treinamento: batch 844 terminou com 12:05:33.782293\n",
            " 845/1875 [============>.................] - ETA: 2:33 - loss: 1.2232 - accuracy: 0.5089Treinamento: batch 845 iniciou com 12:05:33.784005\n",
            "Treinamento: batch 845 terminou com 12:05:33.928091\n",
            " 846/1875 [============>.................] - ETA: 2:32 - loss: 1.2230 - accuracy: 0.5090Treinamento: batch 846 iniciou com 12:05:33.929898\n",
            "Treinamento: batch 846 terminou com 12:05:34.083584\n",
            " 847/1875 [============>.................] - ETA: 2:32 - loss: 1.2225 - accuracy: 0.5092Treinamento: batch 847 iniciou com 12:05:34.085583\n",
            "Treinamento: batch 847 terminou com 12:05:34.230613\n",
            " 848/1875 [============>.................] - ETA: 2:32 - loss: 1.2221 - accuracy: 0.5094Treinamento: batch 848 iniciou com 12:05:34.232456\n",
            "Treinamento: batch 848 terminou com 12:05:34.371010\n",
            " 849/1875 [============>.................] - ETA: 2:32 - loss: 1.2215 - accuracy: 0.5097Treinamento: batch 849 iniciou com 12:05:34.372871\n",
            "Treinamento: batch 849 terminou com 12:05:34.517577\n",
            " 850/1875 [============>.................] - ETA: 2:32 - loss: 1.2215 - accuracy: 0.5098Treinamento: batch 850 iniciou com 12:05:34.520418\n",
            "Treinamento: batch 850 terminou com 12:05:34.658843\n",
            " 851/1875 [============>.................] - ETA: 2:32 - loss: 1.2212 - accuracy: 0.5099Treinamento: batch 851 iniciou com 12:05:34.660467\n",
            "Treinamento: batch 851 terminou com 12:05:34.811593\n",
            " 852/1875 [============>.................] - ETA: 2:31 - loss: 1.2209 - accuracy: 0.5100Treinamento: batch 852 iniciou com 12:05:34.813257\n",
            "Treinamento: batch 852 terminou com 12:05:34.955249\n",
            " 853/1875 [============>.................] - ETA: 2:31 - loss: 1.2207 - accuracy: 0.5101Treinamento: batch 853 iniciou com 12:05:34.957088\n",
            "Treinamento: batch 853 terminou com 12:05:35.137219\n",
            " 854/1875 [============>.................] - ETA: 2:31 - loss: 1.2205 - accuracy: 0.5102Treinamento: batch 854 iniciou com 12:05:35.138988\n",
            "Treinamento: batch 854 terminou com 12:05:35.283875\n",
            " 855/1875 [============>.................] - ETA: 2:31 - loss: 1.2201 - accuracy: 0.5104Treinamento: batch 855 iniciou com 12:05:35.285620\n",
            "Treinamento: batch 855 terminou com 12:05:35.426880\n",
            " 856/1875 [============>.................] - ETA: 2:31 - loss: 1.2198 - accuracy: 0.5104Treinamento: batch 856 iniciou com 12:05:35.428979\n",
            "Treinamento: batch 856 terminou com 12:05:35.569527\n",
            " 857/1875 [============>.................] - ETA: 2:31 - loss: 1.2194 - accuracy: 0.5105Treinamento: batch 857 iniciou com 12:05:35.571040\n",
            "Treinamento: batch 857 terminou com 12:05:35.715054\n",
            " 858/1875 [============>.................] - ETA: 2:31 - loss: 1.2189 - accuracy: 0.5107Treinamento: batch 858 iniciou com 12:05:35.716786\n",
            "Treinamento: batch 858 terminou com 12:05:35.858888\n",
            " 859/1875 [============>.................] - ETA: 2:30 - loss: 1.2182 - accuracy: 0.5110Treinamento: batch 859 iniciou com 12:05:35.860756\n",
            "Treinamento: batch 859 terminou com 12:05:36.001573\n",
            " 860/1875 [============>.................] - ETA: 2:30 - loss: 1.2181 - accuracy: 0.5110Treinamento: batch 860 iniciou com 12:05:36.003853\n",
            "Treinamento: batch 860 terminou com 12:05:36.154218\n",
            " 861/1875 [============>.................] - ETA: 2:30 - loss: 1.2177 - accuracy: 0.5111Treinamento: batch 861 iniciou com 12:05:36.158474\n",
            "Treinamento: batch 861 terminou com 12:05:36.308968\n",
            " 862/1875 [============>.................] - ETA: 2:30 - loss: 1.2171 - accuracy: 0.5113Treinamento: batch 862 iniciou com 12:05:36.310667\n",
            "Treinamento: batch 862 terminou com 12:05:36.449563\n",
            " 863/1875 [============>.................] - ETA: 2:30 - loss: 1.2166 - accuracy: 0.5115Treinamento: batch 863 iniciou com 12:05:36.451316\n",
            "Treinamento: batch 863 terminou com 12:05:36.594056\n",
            " 864/1875 [============>.................] - ETA: 2:30 - loss: 1.2162 - accuracy: 0.5116Treinamento: batch 864 iniciou com 12:05:36.595798\n",
            "Treinamento: batch 864 terminou com 12:05:36.735420\n",
            " 865/1875 [============>.................] - ETA: 2:30 - loss: 1.2160 - accuracy: 0.5117Treinamento: batch 865 iniciou com 12:05:36.737147\n",
            "Treinamento: batch 865 terminou com 12:05:36.875828\n",
            " 866/1875 [============>.................] - ETA: 2:29 - loss: 1.2154 - accuracy: 0.5119Treinamento: batch 866 iniciou com 12:05:36.877570\n",
            "Treinamento: batch 866 terminou com 12:05:37.024475\n",
            " 867/1875 [============>.................] - ETA: 2:29 - loss: 1.2152 - accuracy: 0.5120Treinamento: batch 867 iniciou com 12:05:37.026214\n",
            "Treinamento: batch 867 terminou com 12:05:37.179069\n",
            " 868/1875 [============>.................] - ETA: 2:29 - loss: 1.2148 - accuracy: 0.5122Treinamento: batch 868 iniciou com 12:05:37.180761\n",
            "Treinamento: batch 868 terminou com 12:05:37.323389\n",
            " 869/1875 [============>.................] - ETA: 2:29 - loss: 1.2146 - accuracy: 0.5123Treinamento: batch 869 iniciou com 12:05:37.325107\n",
            "Treinamento: batch 869 terminou com 12:05:37.469785\n",
            " 870/1875 [============>.................] - ETA: 2:29 - loss: 1.2140 - accuracy: 0.5125Treinamento: batch 870 iniciou com 12:05:37.471490\n",
            "Treinamento: batch 870 terminou com 12:05:37.610544\n",
            " 871/1875 [============>.................] - ETA: 2:29 - loss: 1.2139 - accuracy: 0.5125Treinamento: batch 871 iniciou com 12:05:37.612334\n",
            "Treinamento: batch 871 terminou com 12:05:37.752670\n",
            " 872/1875 [============>.................] - ETA: 2:28 - loss: 1.2133 - accuracy: 0.5128Treinamento: batch 872 iniciou com 12:05:37.754384\n",
            "Treinamento: batch 872 terminou com 12:05:37.899437\n",
            " 873/1875 [============>.................] - ETA: 2:28 - loss: 1.2130 - accuracy: 0.5128Treinamento: batch 873 iniciou com 12:05:37.901101\n",
            "Treinamento: batch 873 terminou com 12:05:38.049516\n",
            " 874/1875 [============>.................] - ETA: 2:28 - loss: 1.2126 - accuracy: 0.5130Treinamento: batch 874 iniciou com 12:05:38.051439\n",
            "Treinamento: batch 874 terminou com 12:05:38.213243\n",
            " 875/1875 [=============>................] - ETA: 2:28 - loss: 1.2118 - accuracy: 0.5133Treinamento: batch 875 iniciou com 12:05:38.217240\n",
            "Treinamento: batch 875 terminou com 12:05:38.361053\n",
            " 876/1875 [=============>................] - ETA: 2:28 - loss: 1.2113 - accuracy: 0.5134Treinamento: batch 876 iniciou com 12:05:38.362808\n",
            "Treinamento: batch 876 terminou com 12:05:38.508559\n",
            " 877/1875 [=============>................] - ETA: 2:28 - loss: 1.2109 - accuracy: 0.5136Treinamento: batch 877 iniciou com 12:05:38.510672\n",
            "Treinamento: batch 877 terminou com 12:05:38.661085\n",
            " 878/1875 [=============>................] - ETA: 2:28 - loss: 1.2102 - accuracy: 0.5139Treinamento: batch 878 iniciou com 12:05:38.662845\n",
            "Treinamento: batch 878 terminou com 12:05:38.804403\n",
            " 879/1875 [=============>................] - ETA: 2:27 - loss: 1.2100 - accuracy: 0.5139Treinamento: batch 879 iniciou com 12:05:38.806746\n",
            "Treinamento: batch 879 terminou com 12:05:38.958996\n",
            " 880/1875 [=============>................] - ETA: 2:27 - loss: 1.2097 - accuracy: 0.5141Treinamento: batch 880 iniciou com 12:05:38.960815\n",
            "Treinamento: batch 880 terminou com 12:05:39.103716\n",
            " 881/1875 [=============>................] - ETA: 2:27 - loss: 1.2096 - accuracy: 0.5141Treinamento: batch 881 iniciou com 12:05:39.105525\n",
            "Treinamento: batch 881 terminou com 12:05:39.261017\n",
            " 882/1875 [=============>................] - ETA: 2:27 - loss: 1.2092 - accuracy: 0.5142Treinamento: batch 882 iniciou com 12:05:39.263163\n",
            "Treinamento: batch 882 terminou com 12:05:39.408379\n",
            " 883/1875 [=============>................] - ETA: 2:27 - loss: 1.2086 - accuracy: 0.5145Treinamento: batch 883 iniciou com 12:05:39.410102\n",
            "Treinamento: batch 883 terminou com 12:05:39.548588\n",
            " 884/1875 [=============>................] - ETA: 2:27 - loss: 1.2081 - accuracy: 0.5147Treinamento: batch 884 iniciou com 12:05:39.550342\n",
            "Treinamento: batch 884 terminou com 12:05:39.696018\n",
            " 885/1875 [=============>................] - ETA: 2:27 - loss: 1.2079 - accuracy: 0.5147Treinamento: batch 885 iniciou com 12:05:39.697613\n",
            "Treinamento: batch 885 terminou com 12:05:39.837942\n",
            " 886/1875 [=============>................] - ETA: 2:26 - loss: 1.2072 - accuracy: 0.5150Treinamento: batch 886 iniciou com 12:05:39.839748\n",
            "Treinamento: batch 886 terminou com 12:05:39.981097\n",
            " 887/1875 [=============>................] - ETA: 2:26 - loss: 1.2066 - accuracy: 0.5152Treinamento: batch 887 iniciou com 12:05:39.982822\n",
            "Treinamento: batch 887 terminou com 12:05:40.126967\n",
            " 888/1875 [=============>................] - ETA: 2:26 - loss: 1.2064 - accuracy: 0.5156Treinamento: batch 888 iniciou com 12:05:40.128725\n",
            "Treinamento: batch 888 terminou com 12:05:40.279738\n",
            " 889/1875 [=============>................] - ETA: 2:26 - loss: 1.2063 - accuracy: 0.5157Treinamento: batch 889 iniciou com 12:05:40.281690\n",
            "Treinamento: batch 889 terminou com 12:05:40.439172\n",
            " 890/1875 [=============>................] - ETA: 2:26 - loss: 1.2061 - accuracy: 0.5157Treinamento: batch 890 iniciou com 12:05:40.441252\n",
            "Treinamento: batch 890 terminou com 12:05:40.585407\n",
            " 891/1875 [=============>................] - ETA: 2:26 - loss: 1.2055 - accuracy: 0.5159Treinamento: batch 891 iniciou com 12:05:40.587200\n",
            "Treinamento: batch 891 terminou com 12:05:40.729497\n",
            " 892/1875 [=============>................] - ETA: 2:26 - loss: 1.2053 - accuracy: 0.5160Treinamento: batch 892 iniciou com 12:05:40.731295\n",
            "Treinamento: batch 892 terminou com 12:05:40.877623\n",
            " 893/1875 [=============>................] - ETA: 2:25 - loss: 1.2048 - accuracy: 0.5163Treinamento: batch 893 iniciou com 12:05:40.879483\n",
            "Treinamento: batch 893 terminou com 12:05:41.021442\n",
            " 894/1875 [=============>................] - ETA: 2:25 - loss: 1.2044 - accuracy: 0.5163Treinamento: batch 894 iniciou com 12:05:41.023193\n",
            "Treinamento: batch 894 terminou com 12:05:41.168887\n",
            " 895/1875 [=============>................] - ETA: 2:25 - loss: 1.2044 - accuracy: 0.5164Treinamento: batch 895 iniciou com 12:05:41.170669\n",
            "Treinamento: batch 895 terminou com 12:05:41.330472\n",
            " 896/1875 [=============>................] - ETA: 2:25 - loss: 1.2042 - accuracy: 0.5165Treinamento: batch 896 iniciou com 12:05:41.332304\n",
            "Treinamento: batch 896 terminou com 12:05:41.473165\n",
            " 897/1875 [=============>................] - ETA: 2:25 - loss: 1.2040 - accuracy: 0.5167Treinamento: batch 897 iniciou com 12:05:41.474886\n",
            "Treinamento: batch 897 terminou com 12:05:41.622129\n",
            " 898/1875 [=============>................] - ETA: 2:25 - loss: 1.2038 - accuracy: 0.5167Treinamento: batch 898 iniciou com 12:05:41.624239\n",
            "Treinamento: batch 898 terminou com 12:05:41.772835\n",
            " 899/1875 [=============>................] - ETA: 2:24 - loss: 1.2037 - accuracy: 0.5168Treinamento: batch 899 iniciou com 12:05:41.774907\n",
            "Treinamento: batch 899 terminou com 12:05:41.919033\n",
            " 900/1875 [=============>................] - ETA: 2:24 - loss: 1.2034 - accuracy: 0.5169Treinamento: batch 900 iniciou com 12:05:41.921120\n",
            "Treinamento: batch 900 terminou com 12:05:42.067368\n",
            " 901/1875 [=============>................] - ETA: 2:24 - loss: 1.2030 - accuracy: 0.5170Treinamento: batch 901 iniciou com 12:05:42.069335\n",
            "Treinamento: batch 901 terminou com 12:05:42.214701\n",
            " 902/1875 [=============>................] - ETA: 2:24 - loss: 1.2031 - accuracy: 0.5169Treinamento: batch 902 iniciou com 12:05:42.216536\n",
            "Treinamento: batch 902 terminou com 12:05:42.375497\n",
            " 903/1875 [=============>................] - ETA: 2:24 - loss: 1.2027 - accuracy: 0.5171Treinamento: batch 903 iniciou com 12:05:42.377341\n",
            "Treinamento: batch 903 terminou com 12:05:42.520405\n",
            " 904/1875 [=============>................] - ETA: 2:24 - loss: 1.2023 - accuracy: 0.5173Treinamento: batch 904 iniciou com 12:05:42.522176\n",
            "Treinamento: batch 904 terminou com 12:05:42.672121\n",
            " 905/1875 [=============>................] - ETA: 2:24 - loss: 1.2018 - accuracy: 0.5176Treinamento: batch 905 iniciou com 12:05:42.673955\n",
            "Treinamento: batch 905 terminou com 12:05:42.815980\n",
            " 906/1875 [=============>................] - ETA: 2:23 - loss: 1.2012 - accuracy: 0.5178Treinamento: batch 906 iniciou com 12:05:42.817943\n",
            "Treinamento: batch 906 terminou com 12:05:42.957339\n",
            " 907/1875 [=============>................] - ETA: 2:23 - loss: 1.2007 - accuracy: 0.5181Treinamento: batch 907 iniciou com 12:05:42.958917\n",
            "Treinamento: batch 907 terminou com 12:05:43.095471\n",
            " 908/1875 [=============>................] - ETA: 2:23 - loss: 1.2003 - accuracy: 0.5183Treinamento: batch 908 iniciou com 12:05:43.096916\n",
            "Treinamento: batch 908 terminou com 12:05:43.228669\n",
            " 909/1875 [=============>................] - ETA: 2:23 - loss: 1.1999 - accuracy: 0.5184Treinamento: batch 909 iniciou com 12:05:43.231801\n",
            "Treinamento: batch 909 terminou com 12:05:43.379777\n",
            " 910/1875 [=============>................] - ETA: 2:23 - loss: 1.1996 - accuracy: 0.5185Treinamento: batch 910 iniciou com 12:05:43.381211\n",
            "Treinamento: batch 910 terminou com 12:05:43.517960\n",
            " 911/1875 [=============>................] - ETA: 2:23 - loss: 1.1994 - accuracy: 0.5187Treinamento: batch 911 iniciou com 12:05:43.520116\n",
            "Treinamento: batch 911 terminou com 12:05:43.653661\n",
            " 912/1875 [=============>................] - ETA: 2:23 - loss: 1.1989 - accuracy: 0.5189Treinamento: batch 912 iniciou com 12:05:43.655342\n",
            "Treinamento: batch 912 terminou com 12:05:43.794416\n",
            " 913/1875 [=============>................] - ETA: 2:22 - loss: 1.1982 - accuracy: 0.5192Treinamento: batch 913 iniciou com 12:05:43.795872\n",
            "Treinamento: batch 913 terminou com 12:05:43.941745\n",
            " 914/1875 [=============>................] - ETA: 2:22 - loss: 1.1979 - accuracy: 0.5194Treinamento: batch 914 iniciou com 12:05:43.943335\n",
            "Treinamento: batch 914 terminou com 12:05:44.084866\n",
            " 915/1875 [=============>................] - ETA: 2:22 - loss: 1.1977 - accuracy: 0.5196Treinamento: batch 915 iniciou com 12:05:44.086582\n",
            "Treinamento: batch 915 terminou com 12:05:44.232576\n",
            " 916/1875 [=============>................] - ETA: 2:22 - loss: 1.1974 - accuracy: 0.5198Treinamento: batch 916 iniciou com 12:05:44.234355\n",
            "Treinamento: batch 916 terminou com 12:05:44.389669\n",
            " 917/1875 [=============>................] - ETA: 2:22 - loss: 1.1971 - accuracy: 0.5199Treinamento: batch 917 iniciou com 12:05:44.391846\n",
            "Treinamento: batch 917 terminou com 12:05:44.538197\n",
            " 918/1875 [=============>................] - ETA: 2:22 - loss: 1.1968 - accuracy: 0.5200Treinamento: batch 918 iniciou com 12:05:44.540207\n",
            "Treinamento: batch 918 terminou com 12:05:44.685504\n",
            " 919/1875 [=============>................] - ETA: 2:21 - loss: 1.1965 - accuracy: 0.5201Treinamento: batch 919 iniciou com 12:05:44.687230\n",
            "Treinamento: batch 919 terminou com 12:05:44.830249\n",
            " 920/1875 [=============>................] - ETA: 2:21 - loss: 1.1959 - accuracy: 0.5203Treinamento: batch 920 iniciou com 12:05:44.832065\n",
            "Treinamento: batch 920 terminou com 12:05:44.973371\n",
            " 921/1875 [=============>................] - ETA: 2:21 - loss: 1.1955 - accuracy: 0.5205Treinamento: batch 921 iniciou com 12:05:44.975172\n",
            "Treinamento: batch 921 terminou com 12:05:45.123784\n",
            " 922/1875 [=============>................] - ETA: 2:21 - loss: 1.1951 - accuracy: 0.5206Treinamento: batch 922 iniciou com 12:05:45.125516\n",
            "Treinamento: batch 922 terminou com 12:05:45.277425\n",
            " 923/1875 [=============>................] - ETA: 2:21 - loss: 1.1947 - accuracy: 0.5207Treinamento: batch 923 iniciou com 12:05:45.279249\n",
            "Treinamento: batch 923 terminou com 12:05:45.462834\n",
            " 924/1875 [=============>................] - ETA: 2:21 - loss: 1.1945 - accuracy: 0.5208Treinamento: batch 924 iniciou com 12:05:45.464558\n",
            "Treinamento: batch 924 terminou com 12:05:45.612941\n",
            " 925/1875 [=============>................] - ETA: 2:21 - loss: 1.1942 - accuracy: 0.5209Treinamento: batch 925 iniciou com 12:05:45.614790\n",
            "Treinamento: batch 925 terminou com 12:05:45.761299\n",
            " 926/1875 [=============>................] - ETA: 2:20 - loss: 1.1938 - accuracy: 0.5211Treinamento: batch 926 iniciou com 12:05:45.762933\n",
            "Treinamento: batch 926 terminou com 12:05:45.907930\n",
            " 927/1875 [=============>................] - ETA: 2:20 - loss: 1.1936 - accuracy: 0.5212Treinamento: batch 927 iniciou com 12:05:45.909591\n",
            "Treinamento: batch 927 terminou com 12:05:46.054850\n",
            " 928/1875 [=============>................] - ETA: 2:20 - loss: 1.1935 - accuracy: 0.5212Treinamento: batch 928 iniciou com 12:05:46.057019\n",
            "Treinamento: batch 928 terminou com 12:05:46.203214\n",
            " 929/1875 [=============>................] - ETA: 2:20 - loss: 1.1932 - accuracy: 0.5214Treinamento: batch 929 iniciou com 12:05:46.205005\n",
            "Treinamento: batch 929 terminou com 12:05:46.355715\n",
            " 930/1875 [=============>................] - ETA: 2:20 - loss: 1.1929 - accuracy: 0.5215Treinamento: batch 930 iniciou com 12:05:46.357908\n",
            "Treinamento: batch 930 terminou com 12:05:46.504425\n",
            " 931/1875 [=============>................] - ETA: 2:20 - loss: 1.1928 - accuracy: 0.5217Treinamento: batch 931 iniciou com 12:05:46.506155\n",
            "Treinamento: batch 931 terminou com 12:05:46.648939\n",
            " 932/1875 [=============>................] - ETA: 2:20 - loss: 1.1923 - accuracy: 0.5219Treinamento: batch 932 iniciou com 12:05:46.650683\n",
            "Treinamento: batch 932 terminou com 12:05:46.794124\n",
            " 933/1875 [=============>................] - ETA: 2:19 - loss: 1.1920 - accuracy: 0.5220Treinamento: batch 933 iniciou com 12:05:46.795923\n",
            "Treinamento: batch 933 terminou com 12:05:46.936887\n",
            " 934/1875 [=============>................] - ETA: 2:19 - loss: 1.1918 - accuracy: 0.5220Treinamento: batch 934 iniciou com 12:05:46.938871\n",
            "Treinamento: batch 934 terminou com 12:05:47.083575\n",
            " 935/1875 [=============>................] - ETA: 2:19 - loss: 1.1915 - accuracy: 0.5222Treinamento: batch 935 iniciou com 12:05:47.085551\n",
            "Treinamento: batch 935 terminou com 12:05:47.236686\n",
            " 936/1875 [=============>................] - ETA: 2:19 - loss: 1.1914 - accuracy: 0.5223Treinamento: batch 936 iniciou com 12:05:47.240263\n",
            "Treinamento: batch 936 terminou com 12:05:47.396823\n",
            " 937/1875 [=============>................] - ETA: 2:19 - loss: 1.1913 - accuracy: 0.5222Treinamento: batch 937 iniciou com 12:05:47.398503\n",
            "Treinamento: batch 937 terminou com 12:05:47.544549\n",
            " 938/1875 [==============>...............] - ETA: 2:19 - loss: 1.1907 - accuracy: 0.5224Treinamento: batch 938 iniciou com 12:05:47.546327\n",
            "Treinamento: batch 938 terminou com 12:05:47.688214\n",
            " 939/1875 [==============>...............] - ETA: 2:19 - loss: 1.1901 - accuracy: 0.5227Treinamento: batch 939 iniciou com 12:05:47.689984\n",
            "Treinamento: batch 939 terminou com 12:05:47.830070\n",
            " 940/1875 [==============>...............] - ETA: 2:18 - loss: 1.1899 - accuracy: 0.5228Treinamento: batch 940 iniciou com 12:05:47.831856\n",
            "Treinamento: batch 940 terminou com 12:05:47.980378\n",
            " 941/1875 [==============>...............] - ETA: 2:18 - loss: 1.1895 - accuracy: 0.5229Treinamento: batch 941 iniciou com 12:05:47.982915\n",
            "Treinamento: batch 941 terminou com 12:05:48.130573\n",
            " 942/1875 [==============>...............] - ETA: 2:18 - loss: 1.1894 - accuracy: 0.5229Treinamento: batch 942 iniciou com 12:05:48.132164\n",
            "Treinamento: batch 942 terminou com 12:05:48.269904\n",
            " 943/1875 [==============>...............] - ETA: 2:18 - loss: 1.1891 - accuracy: 0.5231Treinamento: batch 943 iniciou com 12:05:48.271695\n",
            "Treinamento: batch 943 terminou com 12:05:48.429307\n",
            " 944/1875 [==============>...............] - ETA: 2:18 - loss: 1.1888 - accuracy: 0.5233Treinamento: batch 944 iniciou com 12:05:48.431135\n",
            "Treinamento: batch 944 terminou com 12:05:48.571328\n",
            " 945/1875 [==============>...............] - ETA: 2:18 - loss: 1.1882 - accuracy: 0.5236Treinamento: batch 945 iniciou com 12:05:48.573127\n",
            "Treinamento: batch 945 terminou com 12:05:48.722036\n",
            " 946/1875 [==============>...............] - ETA: 2:17 - loss: 1.1880 - accuracy: 0.5237Treinamento: batch 946 iniciou com 12:05:48.723723\n",
            "Treinamento: batch 946 terminou com 12:05:48.864517\n",
            " 947/1875 [==============>...............] - ETA: 2:17 - loss: 1.1882 - accuracy: 0.5237Treinamento: batch 947 iniciou com 12:05:48.866153\n",
            "Treinamento: batch 947 terminou com 12:05:49.007111\n",
            " 948/1875 [==============>...............] - ETA: 2:17 - loss: 1.1877 - accuracy: 0.5240Treinamento: batch 948 iniciou com 12:05:49.008839\n",
            "Treinamento: batch 948 terminou com 12:05:49.156442\n",
            " 949/1875 [==============>...............] - ETA: 2:17 - loss: 1.1874 - accuracy: 0.5241Treinamento: batch 949 iniciou com 12:05:49.158159\n",
            "Treinamento: batch 949 terminou com 12:05:49.306735\n",
            " 950/1875 [==============>...............] - ETA: 2:17 - loss: 1.1872 - accuracy: 0.5242Treinamento: batch 950 iniciou com 12:05:49.308792\n",
            "Treinamento: batch 950 terminou com 12:05:49.462923\n",
            " 951/1875 [==============>...............] - ETA: 2:17 - loss: 1.1868 - accuracy: 0.5243Treinamento: batch 951 iniciou com 12:05:49.464749\n",
            "Treinamento: batch 951 terminou com 12:05:49.611727\n",
            " 952/1875 [==============>...............] - ETA: 2:17 - loss: 1.1862 - accuracy: 0.5247Treinamento: batch 952 iniciou com 12:05:49.613409\n",
            "Treinamento: batch 952 terminou com 12:05:49.759038\n",
            " 953/1875 [==============>...............] - ETA: 2:16 - loss: 1.1859 - accuracy: 0.5249Treinamento: batch 953 iniciou com 12:05:49.760757\n",
            "Treinamento: batch 953 terminou com 12:05:49.903901\n",
            " 954/1875 [==============>...............] - ETA: 2:16 - loss: 1.1857 - accuracy: 0.5249Treinamento: batch 954 iniciou com 12:05:49.905472\n",
            "Treinamento: batch 954 terminou com 12:05:50.059125\n",
            " 955/1875 [==============>...............] - ETA: 2:16 - loss: 1.1853 - accuracy: 0.5251Treinamento: batch 955 iniciou com 12:05:50.060883\n",
            "Treinamento: batch 955 terminou com 12:05:50.200735\n",
            " 956/1875 [==============>...............] - ETA: 2:16 - loss: 1.1849 - accuracy: 0.5254Treinamento: batch 956 iniciou com 12:05:50.202467\n",
            "Treinamento: batch 956 terminou com 12:05:50.341028\n",
            " 957/1875 [==============>...............] - ETA: 2:16 - loss: 1.1845 - accuracy: 0.5255Treinamento: batch 957 iniciou com 12:05:50.342731\n",
            "Treinamento: batch 957 terminou com 12:05:50.495496\n",
            " 958/1875 [==============>...............] - ETA: 2:16 - loss: 1.1844 - accuracy: 0.5256Treinamento: batch 958 iniciou com 12:05:50.497569\n",
            "Treinamento: batch 958 terminou com 12:05:50.646903\n",
            " 959/1875 [==============>...............] - ETA: 2:16 - loss: 1.1843 - accuracy: 0.5257Treinamento: batch 959 iniciou com 12:05:50.648731\n",
            "Treinamento: batch 959 terminou com 12:05:50.789912\n",
            " 960/1875 [==============>...............] - ETA: 2:15 - loss: 1.1837 - accuracy: 0.5260Treinamento: batch 960 iniciou com 12:05:50.791673\n",
            "Treinamento: batch 960 terminou com 12:05:50.935919\n",
            " 961/1875 [==============>...............] - ETA: 2:15 - loss: 1.1834 - accuracy: 0.5263Treinamento: batch 961 iniciou com 12:05:50.937672\n",
            "Treinamento: batch 961 terminou com 12:05:51.081374\n",
            " 962/1875 [==============>...............] - ETA: 2:15 - loss: 1.1832 - accuracy: 0.5262Treinamento: batch 962 iniciou com 12:05:51.083170\n",
            "Treinamento: batch 962 terminou com 12:05:51.227904\n",
            " 963/1875 [==============>...............] - ETA: 2:15 - loss: 1.1831 - accuracy: 0.5263Treinamento: batch 963 iniciou com 12:05:51.229860\n",
            "Treinamento: batch 963 terminou com 12:05:51.370766\n",
            " 964/1875 [==============>...............] - ETA: 2:15 - loss: 1.1829 - accuracy: 0.5264Treinamento: batch 964 iniciou com 12:05:51.372461\n",
            "Treinamento: batch 964 terminou com 12:05:51.529994\n",
            " 965/1875 [==============>...............] - ETA: 2:15 - loss: 1.1826 - accuracy: 0.5264Treinamento: batch 965 iniciou com 12:05:51.531740\n",
            "Treinamento: batch 965 terminou com 12:05:51.677388\n",
            " 966/1875 [==============>...............] - ETA: 2:14 - loss: 1.1824 - accuracy: 0.5266Treinamento: batch 966 iniciou com 12:05:51.679143\n",
            "Treinamento: batch 966 terminou com 12:05:51.827248\n",
            " 967/1875 [==============>...............] - ETA: 2:14 - loss: 1.1819 - accuracy: 0.5268Treinamento: batch 967 iniciou com 12:05:51.828964\n",
            "Treinamento: batch 967 terminou com 12:05:51.972104\n",
            " 968/1875 [==============>...............] - ETA: 2:14 - loss: 1.1821 - accuracy: 0.5268Treinamento: batch 968 iniciou com 12:05:51.973829\n",
            "Treinamento: batch 968 terminou com 12:05:52.120161\n",
            " 969/1875 [==============>...............] - ETA: 2:14 - loss: 1.1819 - accuracy: 0.5270Treinamento: batch 969 iniciou com 12:05:52.122020\n",
            "Treinamento: batch 969 terminou com 12:05:52.262772\n",
            " 970/1875 [==============>...............] - ETA: 2:14 - loss: 1.1820 - accuracy: 0.5271Treinamento: batch 970 iniciou com 12:05:52.264521\n",
            "Treinamento: batch 970 terminou com 12:05:52.417056\n",
            " 971/1875 [==============>...............] - ETA: 2:14 - loss: 1.1817 - accuracy: 0.5272Treinamento: batch 971 iniciou com 12:05:52.419247\n",
            "Treinamento: batch 971 terminou com 12:05:52.574805\n",
            " 972/1875 [==============>...............] - ETA: 2:14 - loss: 1.1815 - accuracy: 0.5273Treinamento: batch 972 iniciou com 12:05:52.576970\n",
            "Treinamento: batch 972 terminou com 12:05:52.725415\n",
            " 973/1875 [==============>...............] - ETA: 2:13 - loss: 1.1814 - accuracy: 0.5274Treinamento: batch 973 iniciou com 12:05:52.728057\n",
            "Treinamento: batch 973 terminou com 12:05:52.873471\n",
            " 974/1875 [==============>...............] - ETA: 2:13 - loss: 1.1810 - accuracy: 0.5275Treinamento: batch 974 iniciou com 12:05:52.875326\n",
            "Treinamento: batch 974 terminou com 12:05:53.027257\n",
            " 975/1875 [==============>...............] - ETA: 2:13 - loss: 1.1809 - accuracy: 0.5276Treinamento: batch 975 iniciou com 12:05:53.029444\n",
            "Treinamento: batch 975 terminou com 12:05:53.176485\n",
            " 976/1875 [==============>...............] - ETA: 2:13 - loss: 1.1805 - accuracy: 0.5277Treinamento: batch 976 iniciou com 12:05:53.178285\n",
            "Treinamento: batch 976 terminou com 12:05:53.322903\n",
            " 977/1875 [==============>...............] - ETA: 2:13 - loss: 1.1803 - accuracy: 0.5279Treinamento: batch 977 iniciou com 12:05:53.327176\n",
            "Treinamento: batch 977 terminou com 12:05:53.470518\n",
            " 978/1875 [==============>...............] - ETA: 2:13 - loss: 1.1801 - accuracy: 0.5280Treinamento: batch 978 iniciou com 12:05:53.472259\n",
            "Treinamento: batch 978 terminou com 12:05:53.625196\n",
            " 979/1875 [==============>...............] - ETA: 2:13 - loss: 1.1798 - accuracy: 0.5281Treinamento: batch 979 iniciou com 12:05:53.626930\n",
            "Treinamento: batch 979 terminou com 12:05:53.778147\n",
            " 980/1875 [==============>...............] - ETA: 2:12 - loss: 1.1797 - accuracy: 0.5281Treinamento: batch 980 iniciou com 12:05:53.781181\n",
            "Treinamento: batch 980 terminou com 12:05:53.924148\n",
            " 981/1875 [==============>...............] - ETA: 2:12 - loss: 1.1792 - accuracy: 0.5284Treinamento: batch 981 iniciou com 12:05:53.927274\n",
            "Treinamento: batch 981 terminou com 12:05:54.076005\n",
            " 982/1875 [==============>...............] - ETA: 2:12 - loss: 1.1789 - accuracy: 0.5285Treinamento: batch 982 iniciou com 12:05:54.078045\n",
            "Treinamento: batch 982 terminou com 12:05:54.221959\n",
            " 983/1875 [==============>...............] - ETA: 2:12 - loss: 1.1787 - accuracy: 0.5287Treinamento: batch 983 iniciou com 12:05:54.223798\n",
            "Treinamento: batch 983 terminou com 12:05:54.369008\n",
            " 984/1875 [==============>...............] - ETA: 2:12 - loss: 1.1786 - accuracy: 0.5287Treinamento: batch 984 iniciou com 12:05:54.370830\n",
            "Treinamento: batch 984 terminou com 12:05:54.527113\n",
            " 985/1875 [==============>...............] - ETA: 2:12 - loss: 1.1784 - accuracy: 0.5288Treinamento: batch 985 iniciou com 12:05:54.529930\n",
            "Treinamento: batch 985 terminou com 12:05:54.678259\n",
            " 986/1875 [==============>...............] - ETA: 2:12 - loss: 1.1782 - accuracy: 0.5290Treinamento: batch 986 iniciou com 12:05:54.680011\n",
            "Treinamento: batch 986 terminou com 12:05:54.825767\n",
            " 987/1875 [==============>...............] - ETA: 2:11 - loss: 1.1780 - accuracy: 0.5290Treinamento: batch 987 iniciou com 12:05:54.828279\n",
            "Treinamento: batch 987 terminou com 12:05:54.970049\n",
            " 988/1875 [==============>...............] - ETA: 2:11 - loss: 1.1777 - accuracy: 0.5292Treinamento: batch 988 iniciou com 12:05:54.971833\n",
            "Treinamento: batch 988 terminou com 12:05:55.126853\n",
            " 989/1875 [==============>...............] - ETA: 2:11 - loss: 1.1773 - accuracy: 0.5294Treinamento: batch 989 iniciou com 12:05:55.128430\n",
            "Treinamento: batch 989 terminou com 12:05:55.276463\n",
            " 990/1875 [==============>...............] - ETA: 2:11 - loss: 1.1768 - accuracy: 0.5295Treinamento: batch 990 iniciou com 12:05:55.278246\n",
            "Treinamento: batch 990 terminou com 12:05:55.418770\n",
            " 991/1875 [==============>...............] - ETA: 2:11 - loss: 1.1766 - accuracy: 0.5297Treinamento: batch 991 iniciou com 12:05:55.420756\n",
            "Treinamento: batch 991 terminou com 12:05:55.570284\n",
            " 992/1875 [==============>...............] - ETA: 2:11 - loss: 1.1762 - accuracy: 0.5299Treinamento: batch 992 iniciou com 12:05:55.572509\n",
            "Treinamento: batch 992 terminou com 12:05:55.718158\n",
            " 993/1875 [==============>...............] - ETA: 2:11 - loss: 1.1760 - accuracy: 0.5301Treinamento: batch 993 iniciou com 12:05:55.719887\n",
            "Treinamento: batch 993 terminou com 12:05:55.895538\n",
            " 994/1875 [==============>...............] - ETA: 2:10 - loss: 1.1759 - accuracy: 0.5300Treinamento: batch 994 iniciou com 12:05:55.897366\n",
            "Treinamento: batch 994 terminou com 12:05:56.045418\n",
            " 995/1875 [==============>...............] - ETA: 2:10 - loss: 1.1760 - accuracy: 0.5300Treinamento: batch 995 iniciou com 12:05:56.047100\n",
            "Treinamento: batch 995 terminou com 12:05:56.190565\n",
            " 996/1875 [==============>...............] - ETA: 2:10 - loss: 1.1758 - accuracy: 0.5302Treinamento: batch 996 iniciou com 12:05:56.192205\n",
            "Treinamento: batch 996 terminou com 12:05:56.341607\n",
            " 997/1875 [==============>...............] - ETA: 2:10 - loss: 1.1753 - accuracy: 0.5303Treinamento: batch 997 iniciou com 12:05:56.344021\n",
            "Treinamento: batch 997 terminou com 12:05:56.494856\n",
            " 998/1875 [==============>...............] - ETA: 2:10 - loss: 1.1749 - accuracy: 0.5306Treinamento: batch 998 iniciou com 12:05:56.496681\n",
            "Treinamento: batch 998 terminou com 12:05:56.655951\n",
            " 999/1875 [==============>...............] - ETA: 2:10 - loss: 1.1745 - accuracy: 0.5308Treinamento: batch 999 iniciou com 12:05:56.657736\n",
            "Treinamento: batch 999 terminou com 12:05:56.799996\n",
            "1000/1875 [===============>..............] - ETA: 2:10 - loss: 1.1740 - accuracy: 0.5310Treinamento: batch 1000 iniciou com 12:05:56.801618\n",
            "Treinamento: batch 1000 terminou com 12:05:56.944595\n",
            "1001/1875 [===============>..............] - ETA: 2:09 - loss: 1.1737 - accuracy: 0.5313Treinamento: batch 1001 iniciou com 12:05:56.946338\n",
            "Treinamento: batch 1001 terminou com 12:05:57.088574\n",
            "1002/1875 [===============>..............] - ETA: 2:09 - loss: 1.1734 - accuracy: 0.5314Treinamento: batch 1002 iniciou com 12:05:57.090282\n",
            "Treinamento: batch 1002 terminou com 12:05:57.237103\n",
            "1003/1875 [===============>..............] - ETA: 2:09 - loss: 1.1730 - accuracy: 0.5316Treinamento: batch 1003 iniciou com 12:05:57.238911\n",
            "Treinamento: batch 1003 terminou com 12:05:57.389083\n",
            "1004/1875 [===============>..............] - ETA: 2:09 - loss: 1.1731 - accuracy: 0.5317Treinamento: batch 1004 iniciou com 12:05:57.390687\n",
            "Treinamento: batch 1004 terminou com 12:05:57.532711\n",
            "1005/1875 [===============>..............] - ETA: 2:09 - loss: 1.1731 - accuracy: 0.5317Treinamento: batch 1005 iniciou com 12:05:57.534408\n",
            "Treinamento: batch 1005 terminou com 12:05:57.687051\n",
            "1006/1875 [===============>..............] - ETA: 2:09 - loss: 1.1727 - accuracy: 0.5318Treinamento: batch 1006 iniciou com 12:05:57.688667\n",
            "Treinamento: batch 1006 terminou com 12:05:57.849789\n",
            "1007/1875 [===============>..............] - ETA: 2:08 - loss: 1.1723 - accuracy: 0.5319Treinamento: batch 1007 iniciou com 12:05:57.851752\n",
            "Treinamento: batch 1007 terminou com 12:05:57.996517\n",
            "1008/1875 [===============>..............] - ETA: 2:08 - loss: 1.1720 - accuracy: 0.5320Treinamento: batch 1008 iniciou com 12:05:57.998291\n",
            "Treinamento: batch 1008 terminou com 12:05:58.143570\n",
            "1009/1875 [===============>..............] - ETA: 2:08 - loss: 1.1716 - accuracy: 0.5321Treinamento: batch 1009 iniciou com 12:05:58.145782\n",
            "Treinamento: batch 1009 terminou com 12:05:58.291128\n",
            "1010/1875 [===============>..............] - ETA: 2:08 - loss: 1.1715 - accuracy: 0.5323Treinamento: batch 1010 iniciou com 12:05:58.292950\n",
            "Treinamento: batch 1010 terminou com 12:05:58.434440\n",
            "1011/1875 [===============>..............] - ETA: 2:08 - loss: 1.1710 - accuracy: 0.5324Treinamento: batch 1011 iniciou com 12:05:58.436320\n",
            "Treinamento: batch 1011 terminou com 12:05:58.582849\n",
            "1012/1875 [===============>..............] - ETA: 2:08 - loss: 1.1710 - accuracy: 0.5324Treinamento: batch 1012 iniciou com 12:05:58.584782\n",
            "Treinamento: batch 1012 terminou com 12:05:58.741977\n",
            "1013/1875 [===============>..............] - ETA: 2:08 - loss: 1.1706 - accuracy: 0.5325Treinamento: batch 1013 iniciou com 12:05:58.743716\n",
            "Treinamento: batch 1013 terminou com 12:05:58.885361\n",
            "1014/1875 [===============>..............] - ETA: 2:07 - loss: 1.1703 - accuracy: 0.5326Treinamento: batch 1014 iniciou com 12:05:58.886988\n",
            "Treinamento: batch 1014 terminou com 12:05:59.038623\n",
            "1015/1875 [===============>..............] - ETA: 2:07 - loss: 1.1698 - accuracy: 0.5328Treinamento: batch 1015 iniciou com 12:05:59.040428\n",
            "Treinamento: batch 1015 terminou com 12:05:59.187554\n",
            "1016/1875 [===============>..............] - ETA: 2:07 - loss: 1.1695 - accuracy: 0.5329Treinamento: batch 1016 iniciou com 12:05:59.189358\n",
            "Treinamento: batch 1016 terminou com 12:05:59.337976\n",
            "1017/1875 [===============>..............] - ETA: 2:07 - loss: 1.1692 - accuracy: 0.5330Treinamento: batch 1017 iniciou com 12:05:59.339690\n",
            "Treinamento: batch 1017 terminou com 12:05:59.482667\n",
            "1018/1875 [===============>..............] - ETA: 2:07 - loss: 1.1688 - accuracy: 0.5332Treinamento: batch 1018 iniciou com 12:05:59.484562\n",
            "Treinamento: batch 1018 terminou com 12:05:59.644131\n",
            "1019/1875 [===============>..............] - ETA: 2:07 - loss: 1.1687 - accuracy: 0.5332Treinamento: batch 1019 iniciou com 12:05:59.645836\n",
            "Treinamento: batch 1019 terminou com 12:05:59.787888\n",
            "1020/1875 [===============>..............] - ETA: 2:07 - loss: 1.1684 - accuracy: 0.5334Treinamento: batch 1020 iniciou com 12:05:59.789487\n",
            "Treinamento: batch 1020 terminou com 12:05:59.931069\n",
            "1021/1875 [===============>..............] - ETA: 2:06 - loss: 1.1680 - accuracy: 0.5335Treinamento: batch 1021 iniciou com 12:05:59.935156\n",
            "Treinamento: batch 1021 terminou com 12:06:00.085063\n",
            "1022/1875 [===============>..............] - ETA: 2:06 - loss: 1.1678 - accuracy: 0.5337Treinamento: batch 1022 iniciou com 12:06:00.086696\n",
            "Treinamento: batch 1022 terminou com 12:06:00.235011\n",
            "1023/1875 [===============>..............] - ETA: 2:06 - loss: 1.1673 - accuracy: 0.5339Treinamento: batch 1023 iniciou com 12:06:00.236701\n",
            "Treinamento: batch 1023 terminou com 12:06:00.379925\n",
            "1024/1875 [===============>..............] - ETA: 2:06 - loss: 1.1671 - accuracy: 0.5340Treinamento: batch 1024 iniciou com 12:06:00.381670\n",
            "Treinamento: batch 1024 terminou com 12:06:00.525471\n",
            "1025/1875 [===============>..............] - ETA: 2:06 - loss: 1.1666 - accuracy: 0.5341Treinamento: batch 1025 iniciou com 12:06:00.527565\n",
            "Treinamento: batch 1025 terminou com 12:06:00.685697\n",
            "1026/1875 [===============>..............] - ETA: 2:06 - loss: 1.1662 - accuracy: 0.5343Treinamento: batch 1026 iniciou com 12:06:00.687416\n",
            "Treinamento: batch 1026 terminou com 12:06:00.833246\n",
            "1027/1875 [===============>..............] - ETA: 2:06 - loss: 1.1662 - accuracy: 0.5343Treinamento: batch 1027 iniciou com 12:06:00.835212\n",
            "Treinamento: batch 1027 terminou com 12:06:00.991745\n",
            "1028/1875 [===============>..............] - ETA: 2:05 - loss: 1.1662 - accuracy: 0.5344Treinamento: batch 1028 iniciou com 12:06:00.993468\n",
            "Treinamento: batch 1028 terminou com 12:06:01.146902\n",
            "1029/1875 [===============>..............] - ETA: 2:05 - loss: 1.1658 - accuracy: 0.5345Treinamento: batch 1029 iniciou com 12:06:01.148816\n",
            "Treinamento: batch 1029 terminou com 12:06:01.293672\n",
            "1030/1875 [===============>..............] - ETA: 2:05 - loss: 1.1656 - accuracy: 0.5346Treinamento: batch 1030 iniciou com 12:06:01.295570\n",
            "Treinamento: batch 1030 terminou com 12:06:01.449375\n",
            "1031/1875 [===============>..............] - ETA: 2:05 - loss: 1.1653 - accuracy: 0.5348Treinamento: batch 1031 iniciou com 12:06:01.451172\n",
            "Treinamento: batch 1031 terminou com 12:06:01.596263\n",
            "1032/1875 [===============>..............] - ETA: 2:05 - loss: 1.1652 - accuracy: 0.5349Treinamento: batch 1032 iniciou com 12:06:01.598066\n",
            "Treinamento: batch 1032 terminou com 12:06:01.754480\n",
            "1033/1875 [===============>..............] - ETA: 2:05 - loss: 1.1650 - accuracy: 0.5349Treinamento: batch 1033 iniciou com 12:06:01.756667\n",
            "Treinamento: batch 1033 terminou com 12:06:01.900611\n",
            "1034/1875 [===============>..............] - ETA: 2:04 - loss: 1.1645 - accuracy: 0.5351Treinamento: batch 1034 iniciou com 12:06:01.902498\n",
            "Treinamento: batch 1034 terminou com 12:06:02.052200\n",
            "1035/1875 [===============>..............] - ETA: 2:04 - loss: 1.1642 - accuracy: 0.5353Treinamento: batch 1035 iniciou com 12:06:02.053976\n",
            "Treinamento: batch 1035 terminou com 12:06:02.194282\n",
            "1036/1875 [===============>..............] - ETA: 2:04 - loss: 1.1641 - accuracy: 0.5354Treinamento: batch 1036 iniciou com 12:06:02.195929\n",
            "Treinamento: batch 1036 terminou com 12:06:02.344146\n",
            "1037/1875 [===============>..............] - ETA: 2:04 - loss: 1.1640 - accuracy: 0.5354Treinamento: batch 1037 iniciou com 12:06:02.345874\n",
            "Treinamento: batch 1037 terminou com 12:06:02.492182\n",
            "1038/1875 [===============>..............] - ETA: 2:04 - loss: 1.1637 - accuracy: 0.5355Treinamento: batch 1038 iniciou com 12:06:02.493777\n",
            "Treinamento: batch 1038 terminou com 12:06:02.638776\n",
            "1039/1875 [===============>..............] - ETA: 2:04 - loss: 1.1632 - accuracy: 0.5357Treinamento: batch 1039 iniciou com 12:06:02.640483\n",
            "Treinamento: batch 1039 terminou com 12:06:02.807156\n",
            "1040/1875 [===============>..............] - ETA: 2:04 - loss: 1.1628 - accuracy: 0.5359Treinamento: batch 1040 iniciou com 12:06:02.808942\n",
            "Treinamento: batch 1040 terminou com 12:06:02.957258\n",
            "1041/1875 [===============>..............] - ETA: 2:03 - loss: 1.1628 - accuracy: 0.5359Treinamento: batch 1041 iniciou com 12:06:02.959017\n",
            "Treinamento: batch 1041 terminou com 12:06:03.102684\n",
            "1042/1875 [===============>..............] - ETA: 2:03 - loss: 1.1624 - accuracy: 0.5360Treinamento: batch 1042 iniciou com 12:06:03.104272\n",
            "Treinamento: batch 1042 terminou com 12:06:03.250448\n",
            "1043/1875 [===============>..............] - ETA: 2:03 - loss: 1.1620 - accuracy: 0.5362Treinamento: batch 1043 iniciou com 12:06:03.252360\n",
            "Treinamento: batch 1043 terminou com 12:06:03.403559\n",
            "1044/1875 [===============>..............] - ETA: 2:03 - loss: 1.1619 - accuracy: 0.5362Treinamento: batch 1044 iniciou com 12:06:03.405273\n",
            "Treinamento: batch 1044 terminou com 12:06:03.551123\n",
            "1045/1875 [===============>..............] - ETA: 2:03 - loss: 1.1616 - accuracy: 0.5363Treinamento: batch 1045 iniciou com 12:06:03.552913\n",
            "Treinamento: batch 1045 terminou com 12:06:03.696355\n",
            "1046/1875 [===============>..............] - ETA: 2:03 - loss: 1.1613 - accuracy: 0.5364Treinamento: batch 1046 iniciou com 12:06:03.698006\n",
            "Treinamento: batch 1046 terminou com 12:06:03.854506\n",
            "1047/1875 [===============>..............] - ETA: 2:03 - loss: 1.1611 - accuracy: 0.5365Treinamento: batch 1047 iniciou com 12:06:03.856358\n",
            "Treinamento: batch 1047 terminou com 12:06:04.005949\n",
            "1048/1875 [===============>..............] - ETA: 2:02 - loss: 1.1609 - accuracy: 0.5366Treinamento: batch 1048 iniciou com 12:06:04.007933\n",
            "Treinamento: batch 1048 terminou com 12:06:04.163804\n",
            "1049/1875 [===============>..............] - ETA: 2:02 - loss: 1.1609 - accuracy: 0.5367Treinamento: batch 1049 iniciou com 12:06:04.165457\n",
            "Treinamento: batch 1049 terminou com 12:06:04.309829\n",
            "1050/1875 [===============>..............] - ETA: 2:02 - loss: 1.1606 - accuracy: 0.5367Treinamento: batch 1050 iniciou com 12:06:04.311468\n",
            "Treinamento: batch 1050 terminou com 12:06:04.454301\n",
            "1051/1875 [===============>..............] - ETA: 2:02 - loss: 1.1603 - accuracy: 0.5368Treinamento: batch 1051 iniciou com 12:06:04.456340\n",
            "Treinamento: batch 1051 terminou com 12:06:04.601035\n",
            "1052/1875 [===============>..............] - ETA: 2:02 - loss: 1.1600 - accuracy: 0.5370Treinamento: batch 1052 iniciou com 12:06:04.602656\n",
            "Treinamento: batch 1052 terminou com 12:06:04.751831\n",
            "1053/1875 [===============>..............] - ETA: 2:02 - loss: 1.1598 - accuracy: 0.5370Treinamento: batch 1053 iniciou com 12:06:04.753479\n",
            "Treinamento: batch 1053 terminou com 12:06:04.899738\n",
            "1054/1875 [===============>..............] - ETA: 2:02 - loss: 1.1595 - accuracy: 0.5372Treinamento: batch 1054 iniciou com 12:06:04.901479\n",
            "Treinamento: batch 1054 terminou com 12:06:05.048177\n",
            "1055/1875 [===============>..............] - ETA: 2:01 - loss: 1.1593 - accuracy: 0.5374Treinamento: batch 1055 iniciou com 12:06:05.049851\n",
            "Treinamento: batch 1055 terminou com 12:06:05.195055\n",
            "1056/1875 [===============>..............] - ETA: 2:01 - loss: 1.1587 - accuracy: 0.5377Treinamento: batch 1056 iniciou com 12:06:05.196792\n",
            "Treinamento: batch 1056 terminou com 12:06:05.352786\n",
            "1057/1875 [===============>..............] - ETA: 2:01 - loss: 1.1584 - accuracy: 0.5378Treinamento: batch 1057 iniciou com 12:06:05.354447\n",
            "Treinamento: batch 1057 terminou com 12:06:05.493876\n",
            "1058/1875 [===============>..............] - ETA: 2:01 - loss: 1.1580 - accuracy: 0.5381Treinamento: batch 1058 iniciou com 12:06:05.495562\n",
            "Treinamento: batch 1058 terminou com 12:06:05.644641\n",
            "1059/1875 [===============>..............] - ETA: 2:01 - loss: 1.1576 - accuracy: 0.5383Treinamento: batch 1059 iniciou com 12:06:05.646451\n",
            "Treinamento: batch 1059 terminou com 12:06:05.803624\n",
            "1060/1875 [===============>..............] - ETA: 2:01 - loss: 1.1575 - accuracy: 0.5384Treinamento: batch 1060 iniciou com 12:06:05.805452\n",
            "Treinamento: batch 1060 terminou com 12:06:05.950290\n",
            "1061/1875 [===============>..............] - ETA: 2:01 - loss: 1.1573 - accuracy: 0.5385Treinamento: batch 1061 iniciou com 12:06:05.952158\n",
            "Treinamento: batch 1061 terminou com 12:06:06.111199\n",
            "1062/1875 [===============>..............] - ETA: 2:00 - loss: 1.1571 - accuracy: 0.5386Treinamento: batch 1062 iniciou com 12:06:06.113458\n",
            "Treinamento: batch 1062 terminou com 12:06:06.279518\n",
            "1063/1875 [================>.............] - ETA: 2:00 - loss: 1.1567 - accuracy: 0.5388Treinamento: batch 1063 iniciou com 12:06:06.281254\n",
            "Treinamento: batch 1063 terminou com 12:06:06.434439\n",
            "1064/1875 [================>.............] - ETA: 2:00 - loss: 1.1566 - accuracy: 0.5389Treinamento: batch 1064 iniciou com 12:06:06.436214\n",
            "Treinamento: batch 1064 terminou com 12:06:06.579116\n",
            "1065/1875 [================>.............] - ETA: 2:00 - loss: 1.1562 - accuracy: 0.5390Treinamento: batch 1065 iniciou com 12:06:06.580817\n",
            "Treinamento: batch 1065 terminou com 12:06:06.726445\n",
            "1066/1875 [================>.............] - ETA: 2:00 - loss: 1.1560 - accuracy: 0.5392Treinamento: batch 1066 iniciou com 12:06:06.728199\n",
            "Treinamento: batch 1066 terminou com 12:06:06.882505\n",
            "1067/1875 [================>.............] - ETA: 2:00 - loss: 1.1557 - accuracy: 0.5393Treinamento: batch 1067 iniciou com 12:06:06.884480\n",
            "Treinamento: batch 1067 terminou com 12:06:07.035017\n",
            "1068/1875 [================>.............] - ETA: 2:00 - loss: 1.1557 - accuracy: 0.5392Treinamento: batch 1068 iniciou com 12:06:07.036772\n",
            "Treinamento: batch 1068 terminou com 12:06:07.185363\n",
            "1069/1875 [================>.............] - ETA: 1:59 - loss: 1.1556 - accuracy: 0.5392Treinamento: batch 1069 iniciou com 12:06:07.187505\n",
            "Treinamento: batch 1069 terminou com 12:06:07.331924\n",
            "1070/1875 [================>.............] - ETA: 1:59 - loss: 1.1555 - accuracy: 0.5393Treinamento: batch 1070 iniciou com 12:06:07.335822\n",
            "Treinamento: batch 1070 terminou com 12:06:07.487839\n",
            "1071/1875 [================>.............] - ETA: 1:59 - loss: 1.1556 - accuracy: 0.5392Treinamento: batch 1071 iniciou com 12:06:07.489612\n",
            "Treinamento: batch 1071 terminou com 12:06:07.638472\n",
            "1072/1875 [================>.............] - ETA: 1:59 - loss: 1.1554 - accuracy: 0.5393Treinamento: batch 1072 iniciou com 12:06:07.640357\n",
            "Treinamento: batch 1072 terminou com 12:06:07.784104\n",
            "1073/1875 [================>.............] - ETA: 1:59 - loss: 1.1552 - accuracy: 0.5395Treinamento: batch 1073 iniciou com 12:06:07.785778\n",
            "Treinamento: batch 1073 terminou com 12:06:07.942371\n",
            "1074/1875 [================>.............] - ETA: 1:59 - loss: 1.1549 - accuracy: 0.5396Treinamento: batch 1074 iniciou com 12:06:07.944111\n",
            "Treinamento: batch 1074 terminou com 12:06:08.090378\n",
            "1075/1875 [================>.............] - ETA: 1:58 - loss: 1.1551 - accuracy: 0.5395Treinamento: batch 1075 iniciou com 12:06:08.092180\n",
            "Treinamento: batch 1075 terminou com 12:06:08.242560\n",
            "1076/1875 [================>.............] - ETA: 1:58 - loss: 1.1549 - accuracy: 0.5396Treinamento: batch 1076 iniciou com 12:06:08.244368\n",
            "Treinamento: batch 1076 terminou com 12:06:08.392386\n",
            "1077/1875 [================>.............] - ETA: 1:58 - loss: 1.1547 - accuracy: 0.5397Treinamento: batch 1077 iniciou com 12:06:08.394154\n",
            "Treinamento: batch 1077 terminou com 12:06:08.539567\n",
            "1078/1875 [================>.............] - ETA: 1:58 - loss: 1.1543 - accuracy: 0.5399Treinamento: batch 1078 iniciou com 12:06:08.541909\n",
            "Treinamento: batch 1078 terminou com 12:06:08.684841\n",
            "1079/1875 [================>.............] - ETA: 1:58 - loss: 1.1539 - accuracy: 0.5401Treinamento: batch 1079 iniciou com 12:06:08.686577\n",
            "Treinamento: batch 1079 terminou com 12:06:08.843322\n",
            "1080/1875 [================>.............] - ETA: 1:58 - loss: 1.1539 - accuracy: 0.5401Treinamento: batch 1080 iniciou com 12:06:08.845383\n",
            "Treinamento: batch 1080 terminou com 12:06:08.993263\n",
            "1081/1875 [================>.............] - ETA: 1:58 - loss: 1.1536 - accuracy: 0.5403Treinamento: batch 1081 iniciou com 12:06:08.995741\n",
            "Treinamento: batch 1081 terminou com 12:06:09.141927\n",
            "1082/1875 [================>.............] - ETA: 1:57 - loss: 1.1533 - accuracy: 0.5405Treinamento: batch 1082 iniciou com 12:06:09.143744\n",
            "Treinamento: batch 1082 terminou com 12:06:09.293299\n",
            "1083/1875 [================>.............] - ETA: 1:57 - loss: 1.1529 - accuracy: 0.5407Treinamento: batch 1083 iniciou com 12:06:09.294961\n",
            "Treinamento: batch 1083 terminou com 12:06:09.441110\n",
            "1084/1875 [================>.............] - ETA: 1:57 - loss: 1.1531 - accuracy: 0.5406Treinamento: batch 1084 iniciou com 12:06:09.442887\n",
            "Treinamento: batch 1084 terminou com 12:06:09.586926\n",
            "1085/1875 [================>.............] - ETA: 1:57 - loss: 1.1529 - accuracy: 0.5408Treinamento: batch 1085 iniciou com 12:06:09.588607\n",
            "Treinamento: batch 1085 terminou com 12:06:09.732252\n",
            "1086/1875 [================>.............] - ETA: 1:57 - loss: 1.1528 - accuracy: 0.5409Treinamento: batch 1086 iniciou com 12:06:09.733999\n",
            "Treinamento: batch 1086 terminou com 12:06:09.890273\n",
            "1087/1875 [================>.............] - ETA: 1:57 - loss: 1.1527 - accuracy: 0.5409Treinamento: batch 1087 iniciou com 12:06:09.892517\n",
            "Treinamento: batch 1087 terminou com 12:06:10.042186\n",
            "1088/1875 [================>.............] - ETA: 1:57 - loss: 1.1524 - accuracy: 0.5410Treinamento: batch 1088 iniciou com 12:06:10.044342\n",
            "Treinamento: batch 1088 terminou com 12:06:10.185076\n",
            "1089/1875 [================>.............] - ETA: 1:56 - loss: 1.1522 - accuracy: 0.5412Treinamento: batch 1089 iniciou com 12:06:10.186921\n",
            "Treinamento: batch 1089 terminou com 12:06:10.334419\n",
            "1090/1875 [================>.............] - ETA: 1:56 - loss: 1.1519 - accuracy: 0.5413Treinamento: batch 1090 iniciou com 12:06:10.336182\n",
            "Treinamento: batch 1090 terminou com 12:06:10.490730\n",
            "1091/1875 [================>.............] - ETA: 1:56 - loss: 1.1517 - accuracy: 0.5414Treinamento: batch 1091 iniciou com 12:06:10.492847\n",
            "Treinamento: batch 1091 terminou com 12:06:10.635146\n",
            "1092/1875 [================>.............] - ETA: 1:56 - loss: 1.1512 - accuracy: 0.5416Treinamento: batch 1092 iniciou com 12:06:10.636945\n",
            "Treinamento: batch 1092 terminou com 12:06:10.776252\n",
            "1093/1875 [================>.............] - ETA: 1:56 - loss: 1.1508 - accuracy: 0.5418Treinamento: batch 1093 iniciou com 12:06:10.777953\n",
            "Treinamento: batch 1093 terminou com 12:06:10.932080\n",
            "1094/1875 [================>.............] - ETA: 1:56 - loss: 1.1508 - accuracy: 0.5418Treinamento: batch 1094 iniciou com 12:06:10.933901\n",
            "Treinamento: batch 1094 terminou com 12:06:11.082490\n",
            "1095/1875 [================>.............] - ETA: 1:56 - loss: 1.1508 - accuracy: 0.5418Treinamento: batch 1095 iniciou com 12:06:11.084294\n",
            "Treinamento: batch 1095 terminou com 12:06:11.226487\n",
            "1096/1875 [================>.............] - ETA: 1:55 - loss: 1.1507 - accuracy: 0.5419Treinamento: batch 1096 iniciou com 12:06:11.228431\n",
            "Treinamento: batch 1096 terminou com 12:06:11.377403\n",
            "1097/1875 [================>.............] - ETA: 1:55 - loss: 1.1503 - accuracy: 0.5421Treinamento: batch 1097 iniciou com 12:06:11.379150\n",
            "Treinamento: batch 1097 terminou com 12:06:11.523114\n",
            "1098/1875 [================>.............] - ETA: 1:55 - loss: 1.1499 - accuracy: 0.5422Treinamento: batch 1098 iniciou com 12:06:11.526771\n",
            "Treinamento: batch 1098 terminou com 12:06:11.668498\n",
            "1099/1875 [================>.............] - ETA: 1:55 - loss: 1.1495 - accuracy: 0.5424Treinamento: batch 1099 iniciou com 12:06:11.670197\n",
            "Treinamento: batch 1099 terminou com 12:06:11.813891\n",
            "1100/1875 [================>.............] - ETA: 1:55 - loss: 1.1492 - accuracy: 0.5425Treinamento: batch 1100 iniciou com 12:06:11.815891\n",
            "Treinamento: batch 1100 terminou com 12:06:11.972131\n",
            "1101/1875 [================>.............] - ETA: 1:55 - loss: 1.1487 - accuracy: 0.5427Treinamento: batch 1101 iniciou com 12:06:11.975272\n",
            "Treinamento: batch 1101 terminou com 12:06:12.124811\n",
            "1102/1875 [================>.............] - ETA: 1:54 - loss: 1.1483 - accuracy: 0.5429Treinamento: batch 1102 iniciou com 12:06:12.129809\n",
            "Treinamento: batch 1102 terminou com 12:06:12.271527\n",
            "1103/1875 [================>.............] - ETA: 1:54 - loss: 1.1479 - accuracy: 0.5430Treinamento: batch 1103 iniciou com 12:06:12.273377\n",
            "Treinamento: batch 1103 terminou com 12:06:12.421056\n",
            "1104/1875 [================>.............] - ETA: 1:54 - loss: 1.1478 - accuracy: 0.5431Treinamento: batch 1104 iniciou com 12:06:12.422829\n",
            "Treinamento: batch 1104 terminou com 12:06:12.571366\n",
            "1105/1875 [================>.............] - ETA: 1:54 - loss: 1.1476 - accuracy: 0.5432Treinamento: batch 1105 iniciou com 12:06:12.573146\n",
            "Treinamento: batch 1105 terminou com 12:06:12.713613\n",
            "1106/1875 [================>.............] - ETA: 1:54 - loss: 1.1473 - accuracy: 0.5433Treinamento: batch 1106 iniciou com 12:06:12.715687\n",
            "Treinamento: batch 1106 terminou com 12:06:12.860102\n",
            "1107/1875 [================>.............] - ETA: 1:54 - loss: 1.1474 - accuracy: 0.5434Treinamento: batch 1107 iniciou com 12:06:12.862156\n",
            "Treinamento: batch 1107 terminou com 12:06:13.021496\n",
            "1108/1875 [================>.............] - ETA: 1:54 - loss: 1.1470 - accuracy: 0.5436Treinamento: batch 1108 iniciou com 12:06:13.023568\n",
            "Treinamento: batch 1108 terminou com 12:06:13.172796\n",
            "1109/1875 [================>.............] - ETA: 1:53 - loss: 1.1467 - accuracy: 0.5438Treinamento: batch 1109 iniciou com 12:06:13.175089\n",
            "Treinamento: batch 1109 terminou com 12:06:13.318250\n",
            "1110/1875 [================>.............] - ETA: 1:53 - loss: 1.1466 - accuracy: 0.5439Treinamento: batch 1110 iniciou com 12:06:13.320007\n",
            "Treinamento: batch 1110 terminou com 12:06:13.478484\n",
            "1111/1875 [================>.............] - ETA: 1:53 - loss: 1.1464 - accuracy: 0.5439Treinamento: batch 1111 iniciou com 12:06:13.480206\n",
            "Treinamento: batch 1111 terminou com 12:06:13.621439\n",
            "1112/1875 [================>.............] - ETA: 1:53 - loss: 1.1460 - accuracy: 0.5440Treinamento: batch 1112 iniciou com 12:06:13.623213\n",
            "Treinamento: batch 1112 terminou com 12:06:13.770529\n",
            "1113/1875 [================>.............] - ETA: 1:53 - loss: 1.1458 - accuracy: 0.5441Treinamento: batch 1113 iniciou com 12:06:13.773003\n",
            "Treinamento: batch 1113 terminou com 12:06:13.917964\n",
            "1114/1875 [================>.............] - ETA: 1:53 - loss: 1.1454 - accuracy: 0.5443Treinamento: batch 1114 iniciou com 12:06:13.920276\n",
            "Treinamento: batch 1114 terminou com 12:06:14.076057\n",
            "1115/1875 [================>.............] - ETA: 1:53 - loss: 1.1452 - accuracy: 0.5443Treinamento: batch 1115 iniciou com 12:06:14.080794\n",
            "Treinamento: batch 1115 terminou com 12:06:14.222513\n",
            "1116/1875 [================>.............] - ETA: 1:52 - loss: 1.1451 - accuracy: 0.5444Treinamento: batch 1116 iniciou com 12:06:14.224267\n",
            "Treinamento: batch 1116 terminou com 12:06:14.370226\n",
            "1117/1875 [================>.............] - ETA: 1:52 - loss: 1.1447 - accuracy: 0.5445Treinamento: batch 1117 iniciou com 12:06:14.371933\n",
            "Treinamento: batch 1117 terminou com 12:06:14.521564\n",
            "1118/1875 [================>.............] - ETA: 1:52 - loss: 1.1443 - accuracy: 0.5447Treinamento: batch 1118 iniciou com 12:06:14.523919\n",
            "Treinamento: batch 1118 terminou com 12:06:14.674863\n",
            "1119/1875 [================>.............] - ETA: 1:52 - loss: 1.1440 - accuracy: 0.5449Treinamento: batch 1119 iniciou com 12:06:14.676610\n",
            "Treinamento: batch 1119 terminou com 12:06:14.820560\n",
            "1120/1875 [================>.............] - ETA: 1:52 - loss: 1.1436 - accuracy: 0.5450Treinamento: batch 1120 iniciou com 12:06:14.822158\n",
            "Treinamento: batch 1120 terminou com 12:06:14.975105\n",
            "1121/1875 [================>.............] - ETA: 1:52 - loss: 1.1432 - accuracy: 0.5452Treinamento: batch 1121 iniciou com 12:06:14.976829\n",
            "Treinamento: batch 1121 terminou com 12:06:15.118345\n",
            "1122/1875 [================>.............] - ETA: 1:52 - loss: 1.1434 - accuracy: 0.5452Treinamento: batch 1122 iniciou com 12:06:15.120120\n",
            "Treinamento: batch 1122 terminou com 12:06:15.266378\n",
            "1123/1875 [================>.............] - ETA: 1:51 - loss: 1.1434 - accuracy: 0.5452Treinamento: batch 1123 iniciou com 12:06:15.268126\n",
            "Treinamento: batch 1123 terminou com 12:06:15.416820\n",
            "1124/1875 [================>.............] - ETA: 1:51 - loss: 1.1432 - accuracy: 0.5453Treinamento: batch 1124 iniciou com 12:06:15.419878\n",
            "Treinamento: batch 1124 terminou com 12:06:15.562302\n",
            "1125/1875 [=================>............] - ETA: 1:51 - loss: 1.1428 - accuracy: 0.5454Treinamento: batch 1125 iniciou com 12:06:15.564059\n",
            "Treinamento: batch 1125 terminou com 12:06:15.718244\n",
            "1126/1875 [=================>............] - ETA: 1:51 - loss: 1.1427 - accuracy: 0.5455Treinamento: batch 1126 iniciou com 12:06:15.720430\n",
            "Treinamento: batch 1126 terminou com 12:06:15.863321\n",
            "1127/1875 [=================>............] - ETA: 1:51 - loss: 1.1428 - accuracy: 0.5454Treinamento: batch 1127 iniciou com 12:06:15.865090\n",
            "Treinamento: batch 1127 terminou com 12:06:16.016872\n",
            "1128/1875 [=================>............] - ETA: 1:51 - loss: 1.1426 - accuracy: 0.5455Treinamento: batch 1128 iniciou com 12:06:16.021480\n",
            "Treinamento: batch 1128 terminou com 12:06:16.166576\n",
            "1129/1875 [=================>............] - ETA: 1:50 - loss: 1.1423 - accuracy: 0.5457Treinamento: batch 1129 iniciou com 12:06:16.168524\n",
            "Treinamento: batch 1129 terminou com 12:06:16.312526\n",
            "1130/1875 [=================>............] - ETA: 1:50 - loss: 1.1424 - accuracy: 0.5458Treinamento: batch 1130 iniciou com 12:06:16.314239\n",
            "Treinamento: batch 1130 terminou com 12:06:16.481135\n",
            "1131/1875 [=================>............] - ETA: 1:50 - loss: 1.1420 - accuracy: 0.5459Treinamento: batch 1131 iniciou com 12:06:16.482840\n",
            "Treinamento: batch 1131 terminou com 12:06:16.626464\n",
            "1132/1875 [=================>............] - ETA: 1:50 - loss: 1.1420 - accuracy: 0.5460Treinamento: batch 1132 iniciou com 12:06:16.628509\n",
            "Treinamento: batch 1132 terminou com 12:06:16.782089\n",
            "1133/1875 [=================>............] - ETA: 1:50 - loss: 1.1416 - accuracy: 0.5461Treinamento: batch 1133 iniciou com 12:06:16.783848\n",
            "Treinamento: batch 1133 terminou com 12:06:16.929911\n",
            "1134/1875 [=================>............] - ETA: 1:50 - loss: 1.1417 - accuracy: 0.5461Treinamento: batch 1134 iniciou com 12:06:16.931520\n",
            "Treinamento: batch 1134 terminou com 12:06:17.082598\n",
            "1135/1875 [=================>............] - ETA: 1:50 - loss: 1.1413 - accuracy: 0.5463Treinamento: batch 1135 iniciou com 12:06:17.084340\n",
            "Treinamento: batch 1135 terminou com 12:06:17.229738\n",
            "1136/1875 [=================>............] - ETA: 1:49 - loss: 1.1412 - accuracy: 0.5464Treinamento: batch 1136 iniciou com 12:06:17.231763\n",
            "Treinamento: batch 1136 terminou com 12:06:17.374780\n",
            "1137/1875 [=================>............] - ETA: 1:49 - loss: 1.1409 - accuracy: 0.5466Treinamento: batch 1137 iniciou com 12:06:17.376863\n",
            "Treinamento: batch 1137 terminou com 12:06:17.517789\n",
            "1138/1875 [=================>............] - ETA: 1:49 - loss: 1.1410 - accuracy: 0.5465Treinamento: batch 1138 iniciou com 12:06:17.519764\n",
            "Treinamento: batch 1138 terminou com 12:06:17.665014\n",
            "1139/1875 [=================>............] - ETA: 1:49 - loss: 1.1407 - accuracy: 0.5467Treinamento: batch 1139 iniciou com 12:06:17.666730\n",
            "Treinamento: batch 1139 terminou com 12:06:17.812137\n",
            "1140/1875 [=================>............] - ETA: 1:49 - loss: 1.1403 - accuracy: 0.5468Treinamento: batch 1140 iniciou com 12:06:17.813833\n",
            "Treinamento: batch 1140 terminou com 12:06:17.957483\n",
            "1141/1875 [=================>............] - ETA: 1:49 - loss: 1.1403 - accuracy: 0.5468Treinamento: batch 1141 iniciou com 12:06:17.959253\n",
            "Treinamento: batch 1141 terminou com 12:06:18.126599\n",
            "1142/1875 [=================>............] - ETA: 1:49 - loss: 1.1400 - accuracy: 0.5469Treinamento: batch 1142 iniciou com 12:06:18.128377\n",
            "Treinamento: batch 1142 terminou com 12:06:18.271682\n",
            "1143/1875 [=================>............] - ETA: 1:48 - loss: 1.1395 - accuracy: 0.5471Treinamento: batch 1143 iniciou com 12:06:18.273398\n",
            "Treinamento: batch 1143 terminou com 12:06:18.409345\n",
            "1144/1875 [=================>............] - ETA: 1:48 - loss: 1.1393 - accuracy: 0.5472Treinamento: batch 1144 iniciou com 12:06:18.411009\n",
            "Treinamento: batch 1144 terminou com 12:06:18.554417\n",
            "1145/1875 [=================>............] - ETA: 1:48 - loss: 1.1390 - accuracy: 0.5473Treinamento: batch 1145 iniciou com 12:06:18.556134\n",
            "Treinamento: batch 1145 terminou com 12:06:18.697501\n",
            "1146/1875 [=================>............] - ETA: 1:48 - loss: 1.1387 - accuracy: 0.5475Treinamento: batch 1146 iniciou com 12:06:18.699221\n",
            "Treinamento: batch 1146 terminou com 12:06:18.837921\n",
            "1147/1875 [=================>............] - ETA: 1:48 - loss: 1.1381 - accuracy: 0.5478Treinamento: batch 1147 iniciou com 12:06:18.839586\n",
            "Treinamento: batch 1147 terminou com 12:06:18.985391\n",
            "1148/1875 [=================>............] - ETA: 1:48 - loss: 1.1375 - accuracy: 0.5481Treinamento: batch 1148 iniciou com 12:06:18.987133\n",
            "Treinamento: batch 1148 terminou com 12:06:19.145683\n",
            "1149/1875 [=================>............] - ETA: 1:47 - loss: 1.1374 - accuracy: 0.5481Treinamento: batch 1149 iniciou com 12:06:19.147488\n",
            "Treinamento: batch 1149 terminou com 12:06:19.289696\n",
            "1150/1875 [=================>............] - ETA: 1:47 - loss: 1.1370 - accuracy: 0.5484Treinamento: batch 1150 iniciou com 12:06:19.294252\n",
            "Treinamento: batch 1150 terminou com 12:06:19.445904\n",
            "1151/1875 [=================>............] - ETA: 1:47 - loss: 1.1369 - accuracy: 0.5484Treinamento: batch 1151 iniciou com 12:06:19.447568\n",
            "Treinamento: batch 1151 terminou com 12:06:19.592758\n",
            "1152/1875 [=================>............] - ETA: 1:47 - loss: 1.1364 - accuracy: 0.5486Treinamento: batch 1152 iniciou com 12:06:19.594509\n",
            "Treinamento: batch 1152 terminou com 12:06:19.739882\n",
            "1153/1875 [=================>............] - ETA: 1:47 - loss: 1.1365 - accuracy: 0.5485Treinamento: batch 1153 iniciou com 12:06:19.741544\n",
            "Treinamento: batch 1153 terminou com 12:06:19.884621\n",
            "1154/1875 [=================>............] - ETA: 1:47 - loss: 1.1365 - accuracy: 0.5486Treinamento: batch 1154 iniciou com 12:06:19.886240\n",
            "Treinamento: batch 1154 terminou com 12:06:20.035782\n",
            "1155/1875 [=================>............] - ETA: 1:47 - loss: 1.1364 - accuracy: 0.5486Treinamento: batch 1155 iniciou com 12:06:20.037507\n",
            "Treinamento: batch 1155 terminou com 12:06:20.189988\n",
            "1156/1875 [=================>............] - ETA: 1:46 - loss: 1.1362 - accuracy: 0.5488Treinamento: batch 1156 iniciou com 12:06:20.192122\n",
            "Treinamento: batch 1156 terminou com 12:06:20.337127\n",
            "1157/1875 [=================>............] - ETA: 1:46 - loss: 1.1361 - accuracy: 0.5488Treinamento: batch 1157 iniciou com 12:06:20.338814\n",
            "Treinamento: batch 1157 terminou com 12:06:20.483101\n",
            "1158/1875 [=================>............] - ETA: 1:46 - loss: 1.1361 - accuracy: 0.5489Treinamento: batch 1158 iniciou com 12:06:20.484973\n",
            "Treinamento: batch 1158 terminou com 12:06:20.631155\n",
            "1159/1875 [=================>............] - ETA: 1:46 - loss: 1.1360 - accuracy: 0.5490Treinamento: batch 1159 iniciou com 12:06:20.632911\n",
            "Treinamento: batch 1159 terminou com 12:06:20.774790\n",
            "1160/1875 [=================>............] - ETA: 1:46 - loss: 1.1357 - accuracy: 0.5492Treinamento: batch 1160 iniciou com 12:06:20.776749\n",
            "Treinamento: batch 1160 terminou com 12:06:20.919013\n",
            "1161/1875 [=================>............] - ETA: 1:46 - loss: 1.1356 - accuracy: 0.5493Treinamento: batch 1161 iniciou com 12:06:20.920703\n",
            "Treinamento: batch 1161 terminou com 12:06:21.067977\n",
            "1162/1875 [=================>............] - ETA: 1:46 - loss: 1.1352 - accuracy: 0.5495Treinamento: batch 1162 iniciou com 12:06:21.069545\n",
            "Treinamento: batch 1162 terminou com 12:06:21.216846\n",
            "1163/1875 [=================>............] - ETA: 1:45 - loss: 1.1348 - accuracy: 0.5497Treinamento: batch 1163 iniciou com 12:06:21.218909\n",
            "Treinamento: batch 1163 terminou com 12:06:21.362452\n",
            "1164/1875 [=================>............] - ETA: 1:45 - loss: 1.1344 - accuracy: 0.5498Treinamento: batch 1164 iniciou com 12:06:21.364120\n",
            "Treinamento: batch 1164 terminou com 12:06:21.507410\n",
            "1165/1875 [=================>............] - ETA: 1:45 - loss: 1.1342 - accuracy: 0.5498Treinamento: batch 1165 iniciou com 12:06:21.509034\n",
            "Treinamento: batch 1165 terminou com 12:06:21.651851\n",
            "1166/1875 [=================>............] - ETA: 1:45 - loss: 1.1339 - accuracy: 0.5500Treinamento: batch 1166 iniciou com 12:06:21.653515\n",
            "Treinamento: batch 1166 terminou com 12:06:21.794141\n",
            "1167/1875 [=================>............] - ETA: 1:45 - loss: 1.1334 - accuracy: 0.5501Treinamento: batch 1167 iniciou com 12:06:21.795853\n",
            "Treinamento: batch 1167 terminou com 12:06:21.938315\n",
            "1168/1875 [=================>............] - ETA: 1:45 - loss: 1.1332 - accuracy: 0.5503Treinamento: batch 1168 iniciou com 12:06:21.940067\n",
            "Treinamento: batch 1168 terminou com 12:06:22.083435\n",
            "1169/1875 [=================>............] - ETA: 1:45 - loss: 1.1330 - accuracy: 0.5504Treinamento: batch 1169 iniciou com 12:06:22.085239\n",
            "Treinamento: batch 1169 terminou com 12:06:22.234804\n",
            "1170/1875 [=================>............] - ETA: 1:44 - loss: 1.1328 - accuracy: 0.5505Treinamento: batch 1170 iniciou com 12:06:22.236523\n",
            "Treinamento: batch 1170 terminou com 12:06:22.389883\n",
            "1171/1875 [=================>............] - ETA: 1:44 - loss: 1.1324 - accuracy: 0.5507Treinamento: batch 1171 iniciou com 12:06:22.391610\n",
            "Treinamento: batch 1171 terminou com 12:06:22.535826\n",
            "1172/1875 [=================>............] - ETA: 1:44 - loss: 1.1321 - accuracy: 0.5509Treinamento: batch 1172 iniciou com 12:06:22.538011\n",
            "Treinamento: batch 1172 terminou com 12:06:22.679195\n",
            "1173/1875 [=================>............] - ETA: 1:44 - loss: 1.1318 - accuracy: 0.5510Treinamento: batch 1173 iniciou com 12:06:22.680817\n",
            "Treinamento: batch 1173 terminou com 12:06:22.820618\n",
            "1174/1875 [=================>............] - ETA: 1:44 - loss: 1.1317 - accuracy: 0.5511Treinamento: batch 1174 iniciou com 12:06:22.822358\n",
            "Treinamento: batch 1174 terminou com 12:06:22.968284\n",
            "1175/1875 [=================>............] - ETA: 1:44 - loss: 1.1320 - accuracy: 0.5512Treinamento: batch 1175 iniciou com 12:06:22.969997\n",
            "Treinamento: batch 1175 terminou com 12:06:23.113752\n",
            "1176/1875 [=================>............] - ETA: 1:43 - loss: 1.1319 - accuracy: 0.5512Treinamento: batch 1176 iniciou com 12:06:23.115452\n",
            "Treinamento: batch 1176 terminou com 12:06:23.268072\n",
            "1177/1875 [=================>............] - ETA: 1:43 - loss: 1.1314 - accuracy: 0.5513Treinamento: batch 1177 iniciou com 12:06:23.269746\n",
            "Treinamento: batch 1177 terminou com 12:06:23.409936\n",
            "1178/1875 [=================>............] - ETA: 1:43 - loss: 1.1311 - accuracy: 0.5514Treinamento: batch 1178 iniciou com 12:06:23.411718\n",
            "Treinamento: batch 1178 terminou com 12:06:23.556875\n",
            "1179/1875 [=================>............] - ETA: 1:43 - loss: 1.1309 - accuracy: 0.5515Treinamento: batch 1179 iniciou com 12:06:23.558827\n",
            "Treinamento: batch 1179 terminou com 12:06:23.704984\n",
            "1180/1875 [=================>............] - ETA: 1:43 - loss: 1.1309 - accuracy: 0.5515Treinamento: batch 1180 iniciou com 12:06:23.706756\n",
            "Treinamento: batch 1180 terminou com 12:06:23.855490\n",
            "1181/1875 [=================>............] - ETA: 1:43 - loss: 1.1306 - accuracy: 0.5516Treinamento: batch 1181 iniciou com 12:06:23.857611\n",
            "Treinamento: batch 1181 terminou com 12:06:24.007095\n",
            "1182/1875 [=================>............] - ETA: 1:43 - loss: 1.1305 - accuracy: 0.5516Treinamento: batch 1182 iniciou com 12:06:24.009013\n",
            "Treinamento: batch 1182 terminou com 12:06:24.154895\n",
            "1183/1875 [=================>............] - ETA: 1:42 - loss: 1.1302 - accuracy: 0.5517Treinamento: batch 1183 iniciou com 12:06:24.158103\n",
            "Treinamento: batch 1183 terminou com 12:06:24.314863\n",
            "1184/1875 [=================>............] - ETA: 1:42 - loss: 1.1299 - accuracy: 0.5519Treinamento: batch 1184 iniciou com 12:06:24.316661\n",
            "Treinamento: batch 1184 terminou com 12:06:24.460980\n",
            "1185/1875 [=================>............] - ETA: 1:42 - loss: 1.1295 - accuracy: 0.5521Treinamento: batch 1185 iniciou com 12:06:24.462733\n",
            "Treinamento: batch 1185 terminou com 12:06:24.607259\n",
            "1186/1875 [=================>............] - ETA: 1:42 - loss: 1.1291 - accuracy: 0.5523Treinamento: batch 1186 iniciou com 12:06:24.608958\n",
            "Treinamento: batch 1186 terminou com 12:06:24.766731\n",
            "1187/1875 [=================>............] - ETA: 1:42 - loss: 1.1288 - accuracy: 0.5525Treinamento: batch 1187 iniciou com 12:06:24.768798\n",
            "Treinamento: batch 1187 terminou com 12:06:24.915254\n",
            "1188/1875 [==================>...........] - ETA: 1:42 - loss: 1.1283 - accuracy: 0.5527Treinamento: batch 1188 iniciou com 12:06:24.917118\n",
            "Treinamento: batch 1188 terminou com 12:06:25.065489\n",
            "1189/1875 [==================>...........] - ETA: 1:42 - loss: 1.1279 - accuracy: 0.5529Treinamento: batch 1189 iniciou com 12:06:25.067216\n",
            "Treinamento: batch 1189 terminou com 12:06:25.222740\n",
            "1190/1875 [==================>...........] - ETA: 1:41 - loss: 1.1274 - accuracy: 0.5531Treinamento: batch 1190 iniciou com 12:06:25.224967\n",
            "Treinamento: batch 1190 terminou com 12:06:25.368390\n",
            "1191/1875 [==================>...........] - ETA: 1:41 - loss: 1.1272 - accuracy: 0.5533Treinamento: batch 1191 iniciou com 12:06:25.370036\n",
            "Treinamento: batch 1191 terminou com 12:06:25.512795\n",
            "1192/1875 [==================>...........] - ETA: 1:41 - loss: 1.1273 - accuracy: 0.5533Treinamento: batch 1192 iniciou com 12:06:25.514420\n",
            "Treinamento: batch 1192 terminou com 12:06:25.659785\n",
            "1193/1875 [==================>...........] - ETA: 1:41 - loss: 1.1269 - accuracy: 0.5535Treinamento: batch 1193 iniciou com 12:06:25.661419\n",
            "Treinamento: batch 1193 terminou com 12:06:25.803175\n",
            "1194/1875 [==================>...........] - ETA: 1:41 - loss: 1.1267 - accuracy: 0.5536Treinamento: batch 1194 iniciou com 12:06:25.804924\n",
            "Treinamento: batch 1194 terminou com 12:06:25.949610\n",
            "1195/1875 [==================>...........] - ETA: 1:41 - loss: 1.1266 - accuracy: 0.5537Treinamento: batch 1195 iniciou com 12:06:25.951354\n",
            "Treinamento: batch 1195 terminou com 12:06:26.102303\n",
            "1196/1875 [==================>...........] - ETA: 1:40 - loss: 1.1264 - accuracy: 0.5538Treinamento: batch 1196 iniciou com 12:06:26.104800\n",
            "Treinamento: batch 1196 terminou com 12:06:26.262067\n",
            "1197/1875 [==================>...........] - ETA: 1:40 - loss: 1.1261 - accuracy: 0.5539Treinamento: batch 1197 iniciou com 12:06:26.263792\n",
            "Treinamento: batch 1197 terminou com 12:06:26.408415\n",
            "1198/1875 [==================>...........] - ETA: 1:40 - loss: 1.1259 - accuracy: 0.5541Treinamento: batch 1198 iniciou com 12:06:26.410225\n",
            "Treinamento: batch 1198 terminou com 12:06:26.552845\n",
            "1199/1875 [==================>...........] - ETA: 1:40 - loss: 1.1256 - accuracy: 0.5542Treinamento: batch 1199 iniciou com 12:06:26.554585\n",
            "Treinamento: batch 1199 terminou com 12:06:26.722740\n",
            "1200/1875 [==================>...........] - ETA: 1:40 - loss: 1.1253 - accuracy: 0.5544Treinamento: batch 1200 iniciou com 12:06:26.725411\n",
            "Treinamento: batch 1200 terminou com 12:06:26.868534\n",
            "1201/1875 [==================>...........] - ETA: 1:40 - loss: 1.1251 - accuracy: 0.5545Treinamento: batch 1201 iniciou com 12:06:26.870364\n",
            "Treinamento: batch 1201 terminou com 12:06:27.014123\n",
            "1202/1875 [==================>...........] - ETA: 1:40 - loss: 1.1248 - accuracy: 0.5546Treinamento: batch 1202 iniciou com 12:06:27.018786\n",
            "Treinamento: batch 1202 terminou com 12:06:27.171507\n",
            "1203/1875 [==================>...........] - ETA: 1:39 - loss: 1.1246 - accuracy: 0.5547Treinamento: batch 1203 iniciou com 12:06:27.173459\n",
            "Treinamento: batch 1203 terminou com 12:06:27.325375\n",
            "1204/1875 [==================>...........] - ETA: 1:39 - loss: 1.1243 - accuracy: 0.5548Treinamento: batch 1204 iniciou com 12:06:27.327479\n",
            "Treinamento: batch 1204 terminou com 12:06:27.472569\n",
            "1205/1875 [==================>...........] - ETA: 1:39 - loss: 1.1240 - accuracy: 0.5549Treinamento: batch 1205 iniciou com 12:06:27.475199\n",
            "Treinamento: batch 1205 terminou com 12:06:27.624993\n",
            "1206/1875 [==================>...........] - ETA: 1:39 - loss: 1.1240 - accuracy: 0.5548Treinamento: batch 1206 iniciou com 12:06:27.626917\n",
            "Treinamento: batch 1206 terminou com 12:06:27.771516\n",
            "1207/1875 [==================>...........] - ETA: 1:39 - loss: 1.1240 - accuracy: 0.5548Treinamento: batch 1207 iniciou com 12:06:27.773169\n",
            "Treinamento: batch 1207 terminou com 12:06:27.926666\n",
            "1208/1875 [==================>...........] - ETA: 1:39 - loss: 1.1238 - accuracy: 0.5548Treinamento: batch 1208 iniciou com 12:06:27.928371\n",
            "Treinamento: batch 1208 terminou com 12:06:28.076382\n",
            "1209/1875 [==================>...........] - ETA: 1:39 - loss: 1.1237 - accuracy: 0.5549Treinamento: batch 1209 iniciou com 12:06:28.078222\n",
            "Treinamento: batch 1209 terminou com 12:06:28.219645\n",
            "1210/1875 [==================>...........] - ETA: 1:38 - loss: 1.1235 - accuracy: 0.5550Treinamento: batch 1210 iniciou com 12:06:28.221440\n",
            "Treinamento: batch 1210 terminou com 12:06:28.386248\n",
            "1211/1875 [==================>...........] - ETA: 1:38 - loss: 1.1233 - accuracy: 0.5550Treinamento: batch 1211 iniciou com 12:06:28.388017\n",
            "Treinamento: batch 1211 terminou com 12:06:28.535601\n",
            "1212/1875 [==================>...........] - ETA: 1:38 - loss: 1.1232 - accuracy: 0.5551Treinamento: batch 1212 iniciou com 12:06:28.537355\n",
            "Treinamento: batch 1212 terminou com 12:06:28.676711\n",
            "1213/1875 [==================>...........] - ETA: 1:38 - loss: 1.1230 - accuracy: 0.5552Treinamento: batch 1213 iniciou com 12:06:28.678441\n",
            "Treinamento: batch 1213 terminou com 12:06:28.820191\n",
            "1214/1875 [==================>...........] - ETA: 1:38 - loss: 1.1229 - accuracy: 0.5553Treinamento: batch 1214 iniciou com 12:06:28.821908\n",
            "Treinamento: batch 1214 terminou com 12:06:28.966093\n",
            "1215/1875 [==================>...........] - ETA: 1:38 - loss: 1.1226 - accuracy: 0.5555Treinamento: batch 1215 iniciou com 12:06:28.967918\n",
            "Treinamento: batch 1215 terminou com 12:06:29.115482\n",
            "1216/1875 [==================>...........] - ETA: 1:38 - loss: 1.1224 - accuracy: 0.5555Treinamento: batch 1216 iniciou com 12:06:29.119695\n",
            "Treinamento: batch 1216 terminou com 12:06:29.264089\n",
            "1217/1875 [==================>...........] - ETA: 1:37 - loss: 1.1221 - accuracy: 0.5556Treinamento: batch 1217 iniciou com 12:06:29.265960\n",
            "Treinamento: batch 1217 terminou com 12:06:29.416856\n",
            "1218/1875 [==================>...........] - ETA: 1:37 - loss: 1.1219 - accuracy: 0.5558Treinamento: batch 1218 iniciou com 12:06:29.419443\n",
            "Treinamento: batch 1218 terminou com 12:06:29.571159\n",
            "1219/1875 [==================>...........] - ETA: 1:37 - loss: 1.1220 - accuracy: 0.5557Treinamento: batch 1219 iniciou com 12:06:29.572899\n",
            "Treinamento: batch 1219 terminou com 12:06:29.721292\n",
            "1220/1875 [==================>...........] - ETA: 1:37 - loss: 1.1221 - accuracy: 0.5557Treinamento: batch 1220 iniciou com 12:06:29.723046\n",
            "Treinamento: batch 1220 terminou com 12:06:29.868203\n",
            "1221/1875 [==================>...........] - ETA: 1:37 - loss: 1.1219 - accuracy: 0.5558Treinamento: batch 1221 iniciou com 12:06:29.869945\n",
            "Treinamento: batch 1221 terminou com 12:06:30.011482\n",
            "1222/1875 [==================>...........] - ETA: 1:37 - loss: 1.1218 - accuracy: 0.5559Treinamento: batch 1222 iniciou com 12:06:30.013763\n",
            "Treinamento: batch 1222 terminou com 12:06:30.159134\n",
            "1223/1875 [==================>...........] - ETA: 1:36 - loss: 1.1217 - accuracy: 0.5560Treinamento: batch 1223 iniciou com 12:06:30.160863\n",
            "Treinamento: batch 1223 terminou com 12:06:30.313469\n",
            "1224/1875 [==================>...........] - ETA: 1:36 - loss: 1.1213 - accuracy: 0.5561Treinamento: batch 1224 iniciou com 12:06:30.315187\n",
            "Treinamento: batch 1224 terminou com 12:06:30.458301\n",
            "1225/1875 [==================>...........] - ETA: 1:36 - loss: 1.1211 - accuracy: 0.5562Treinamento: batch 1225 iniciou com 12:06:30.460047\n",
            "Treinamento: batch 1225 terminou com 12:06:30.606247\n",
            "1226/1875 [==================>...........] - ETA: 1:36 - loss: 1.1209 - accuracy: 0.5564Treinamento: batch 1226 iniciou com 12:06:30.607977\n",
            "Treinamento: batch 1226 terminou com 12:06:30.747884\n",
            "1227/1875 [==================>...........] - ETA: 1:36 - loss: 1.1205 - accuracy: 0.5566Treinamento: batch 1227 iniciou com 12:06:30.749562\n",
            "Treinamento: batch 1227 terminou com 12:06:30.902472\n",
            "1228/1875 [==================>...........] - ETA: 1:36 - loss: 1.1205 - accuracy: 0.5566Treinamento: batch 1228 iniciou com 12:06:30.904928\n",
            "Treinamento: batch 1228 terminou com 12:06:31.050605\n",
            "1229/1875 [==================>...........] - ETA: 1:36 - loss: 1.1201 - accuracy: 0.5569Treinamento: batch 1229 iniciou com 12:06:31.052470\n",
            "Treinamento: batch 1229 terminou com 12:06:31.199149\n",
            "1230/1875 [==================>...........] - ETA: 1:35 - loss: 1.1198 - accuracy: 0.5570Treinamento: batch 1230 iniciou com 12:06:31.200830\n",
            "Treinamento: batch 1230 terminou com 12:06:31.359878\n",
            "1231/1875 [==================>...........] - ETA: 1:35 - loss: 1.1198 - accuracy: 0.5571Treinamento: batch 1231 iniciou com 12:06:31.361663\n",
            "Treinamento: batch 1231 terminou com 12:06:31.503129\n",
            "1232/1875 [==================>...........] - ETA: 1:35 - loss: 1.1195 - accuracy: 0.5572Treinamento: batch 1232 iniciou com 12:06:31.505172\n",
            "Treinamento: batch 1232 terminou com 12:06:31.654208\n",
            "1233/1875 [==================>...........] - ETA: 1:35 - loss: 1.1191 - accuracy: 0.5574Treinamento: batch 1233 iniciou com 12:06:31.656065\n",
            "Treinamento: batch 1233 terminou com 12:06:31.800797\n",
            "1234/1875 [==================>...........] - ETA: 1:35 - loss: 1.1187 - accuracy: 0.5576Treinamento: batch 1234 iniciou com 12:06:31.802491\n",
            "Treinamento: batch 1234 terminou com 12:06:31.948195\n",
            "1235/1875 [==================>...........] - ETA: 1:35 - loss: 1.1185 - accuracy: 0.5577Treinamento: batch 1235 iniciou com 12:06:31.950056\n",
            "Treinamento: batch 1235 terminou com 12:06:32.106126\n",
            "1236/1875 [==================>...........] - ETA: 1:35 - loss: 1.1180 - accuracy: 0.5579Treinamento: batch 1236 iniciou com 12:06:32.108195\n",
            "Treinamento: batch 1236 terminou com 12:06:32.266343\n",
            "1237/1875 [==================>...........] - ETA: 1:34 - loss: 1.1177 - accuracy: 0.5581Treinamento: batch 1237 iniciou com 12:06:32.268299\n",
            "Treinamento: batch 1237 terminou com 12:06:32.424825\n",
            "1238/1875 [==================>...........] - ETA: 1:34 - loss: 1.1173 - accuracy: 0.5583Treinamento: batch 1238 iniciou com 12:06:32.426691\n",
            "Treinamento: batch 1238 terminou com 12:06:32.579998\n",
            "1239/1875 [==================>...........] - ETA: 1:34 - loss: 1.1171 - accuracy: 0.5584Treinamento: batch 1239 iniciou com 12:06:32.581770\n",
            "Treinamento: batch 1239 terminou com 12:06:32.728109\n",
            "1240/1875 [==================>...........] - ETA: 1:34 - loss: 1.1168 - accuracy: 0.5586Treinamento: batch 1240 iniciou com 12:06:32.729972\n",
            "Treinamento: batch 1240 terminou com 12:06:32.875464\n",
            "1241/1875 [==================>...........] - ETA: 1:34 - loss: 1.1165 - accuracy: 0.5587Treinamento: batch 1241 iniciou com 12:06:32.877572\n",
            "Treinamento: batch 1241 terminou com 12:06:33.026963\n",
            "1242/1875 [==================>...........] - ETA: 1:34 - loss: 1.1160 - accuracy: 0.5589Treinamento: batch 1242 iniciou com 12:06:33.028805\n",
            "Treinamento: batch 1242 terminou com 12:06:33.176676\n",
            "1243/1875 [==================>...........] - ETA: 1:34 - loss: 1.1158 - accuracy: 0.5590Treinamento: batch 1243 iniciou com 12:06:33.178550\n",
            "Treinamento: batch 1243 terminou com 12:06:33.331017\n",
            "1244/1875 [==================>...........] - ETA: 1:33 - loss: 1.1157 - accuracy: 0.5591Treinamento: batch 1244 iniciou com 12:06:33.332758\n",
            "Treinamento: batch 1244 terminou com 12:06:33.483084\n",
            "1245/1875 [==================>...........] - ETA: 1:33 - loss: 1.1156 - accuracy: 0.5592Treinamento: batch 1245 iniciou com 12:06:33.485010\n",
            "Treinamento: batch 1245 terminou com 12:06:33.626026\n",
            "1246/1875 [==================>...........] - ETA: 1:33 - loss: 1.1154 - accuracy: 0.5593Treinamento: batch 1246 iniciou com 12:06:33.627760\n",
            "Treinamento: batch 1246 terminou com 12:06:33.775882\n",
            "1247/1875 [==================>...........] - ETA: 1:33 - loss: 1.1152 - accuracy: 0.5594Treinamento: batch 1247 iniciou com 12:06:33.777673\n",
            "Treinamento: batch 1247 terminou com 12:06:33.929902\n",
            "1248/1875 [==================>...........] - ETA: 1:33 - loss: 1.1151 - accuracy: 0.5595Treinamento: batch 1248 iniciou com 12:06:33.931675\n",
            "Treinamento: batch 1248 terminou com 12:06:34.080463\n",
            "1249/1875 [==================>...........] - ETA: 1:33 - loss: 1.1148 - accuracy: 0.5597Treinamento: batch 1249 iniciou com 12:06:34.082262\n",
            "Treinamento: batch 1249 terminou com 12:06:34.228572\n",
            "1250/1875 [===================>..........] - ETA: 1:33 - loss: 1.1145 - accuracy: 0.5598Treinamento: batch 1250 iniciou com 12:06:34.230354\n",
            "Treinamento: batch 1250 terminou com 12:06:34.371556\n",
            "1251/1875 [===================>..........] - ETA: 1:32 - loss: 1.1144 - accuracy: 0.5599Treinamento: batch 1251 iniciou com 12:06:34.373338\n",
            "Treinamento: batch 1251 terminou com 12:06:34.529464\n",
            "1252/1875 [===================>..........] - ETA: 1:32 - loss: 1.1141 - accuracy: 0.5601Treinamento: batch 1252 iniciou com 12:06:34.531286\n",
            "Treinamento: batch 1252 terminou com 12:06:34.674788\n",
            "1253/1875 [===================>..........] - ETA: 1:32 - loss: 1.1139 - accuracy: 0.5601Treinamento: batch 1253 iniciou com 12:06:34.676471\n",
            "Treinamento: batch 1253 terminou com 12:06:34.820138\n",
            "1254/1875 [===================>..........] - ETA: 1:32 - loss: 1.1138 - accuracy: 0.5602Treinamento: batch 1254 iniciou com 12:06:34.821939\n",
            "Treinamento: batch 1254 terminou com 12:06:34.961944\n",
            "1255/1875 [===================>..........] - ETA: 1:32 - loss: 1.1136 - accuracy: 0.5603Treinamento: batch 1255 iniciou com 12:06:34.963952\n",
            "Treinamento: batch 1255 terminou com 12:06:35.108227\n",
            "1256/1875 [===================>..........] - ETA: 1:32 - loss: 1.1136 - accuracy: 0.5603Treinamento: batch 1256 iniciou com 12:06:35.112048\n",
            "Treinamento: batch 1256 terminou com 12:06:35.263602\n",
            "1257/1875 [===================>..........] - ETA: 1:31 - loss: 1.1133 - accuracy: 0.5605Treinamento: batch 1257 iniciou com 12:06:35.265531\n",
            "Treinamento: batch 1257 terminou com 12:06:35.418957\n",
            "1258/1875 [===================>..........] - ETA: 1:31 - loss: 1.1131 - accuracy: 0.5606Treinamento: batch 1258 iniciou com 12:06:35.420662\n",
            "Treinamento: batch 1258 terminou com 12:06:35.561551\n",
            "1259/1875 [===================>..........] - ETA: 1:31 - loss: 1.1128 - accuracy: 0.5608Treinamento: batch 1259 iniciou com 12:06:35.563339\n",
            "Treinamento: batch 1259 terminou com 12:06:35.709456\n",
            "1260/1875 [===================>..........] - ETA: 1:31 - loss: 1.1125 - accuracy: 0.5609Treinamento: batch 1260 iniciou com 12:06:35.711170\n",
            "Treinamento: batch 1260 terminou com 12:06:35.853960\n",
            "1261/1875 [===================>..........] - ETA: 1:31 - loss: 1.1119 - accuracy: 0.5612Treinamento: batch 1261 iniciou com 12:06:35.855664\n",
            "Treinamento: batch 1261 terminou com 12:06:36.000326\n",
            "1262/1875 [===================>..........] - ETA: 1:31 - loss: 1.1117 - accuracy: 0.5613Treinamento: batch 1262 iniciou com 12:06:36.002431\n",
            "Treinamento: batch 1262 terminou com 12:06:36.148819\n",
            "1263/1875 [===================>..........] - ETA: 1:31 - loss: 1.1112 - accuracy: 0.5615Treinamento: batch 1263 iniciou com 12:06:36.150675\n",
            "Treinamento: batch 1263 terminou com 12:06:36.298258\n",
            "1264/1875 [===================>..........] - ETA: 1:30 - loss: 1.1110 - accuracy: 0.5615Treinamento: batch 1264 iniciou com 12:06:36.299967\n",
            "Treinamento: batch 1264 terminou com 12:06:36.454001\n",
            "1265/1875 [===================>..........] - ETA: 1:30 - loss: 1.1106 - accuracy: 0.5617Treinamento: batch 1265 iniciou com 12:06:36.455718\n",
            "Treinamento: batch 1265 terminou com 12:06:36.598853\n",
            "1266/1875 [===================>..........] - ETA: 1:30 - loss: 1.1106 - accuracy: 0.5617Treinamento: batch 1266 iniciou com 12:06:36.600576\n",
            "Treinamento: batch 1266 terminou com 12:06:36.749429\n",
            "1267/1875 [===================>..........] - ETA: 1:30 - loss: 1.1104 - accuracy: 0.5618Treinamento: batch 1267 iniciou com 12:06:36.751295\n",
            "Treinamento: batch 1267 terminou com 12:06:36.900024\n",
            "1268/1875 [===================>..........] - ETA: 1:30 - loss: 1.1101 - accuracy: 0.5619Treinamento: batch 1268 iniciou com 12:06:36.902248\n",
            "Treinamento: batch 1268 terminou com 12:06:37.090177\n",
            "1269/1875 [===================>..........] - ETA: 1:30 - loss: 1.1098 - accuracy: 0.5619Treinamento: batch 1269 iniciou com 12:06:37.092761\n",
            "Treinamento: batch 1269 terminou com 12:06:37.237026\n",
            "1270/1875 [===================>..........] - ETA: 1:30 - loss: 1.1096 - accuracy: 0.5620Treinamento: batch 1270 iniciou com 12:06:37.239024\n",
            "Treinamento: batch 1270 terminou com 12:06:37.386941\n",
            "1271/1875 [===================>..........] - ETA: 1:29 - loss: 1.1092 - accuracy: 0.5622Treinamento: batch 1271 iniciou com 12:06:37.388728\n",
            "Treinamento: batch 1271 terminou com 12:06:37.545606\n",
            "1272/1875 [===================>..........] - ETA: 1:29 - loss: 1.1087 - accuracy: 0.5623Treinamento: batch 1272 iniciou com 12:06:37.547452\n",
            "Treinamento: batch 1272 terminou com 12:06:37.697408\n",
            "1273/1875 [===================>..........] - ETA: 1:29 - loss: 1.1083 - accuracy: 0.5626Treinamento: batch 1273 iniciou com 12:06:37.699165\n",
            "Treinamento: batch 1273 terminou com 12:06:37.848341\n",
            "1274/1875 [===================>..........] - ETA: 1:29 - loss: 1.1082 - accuracy: 0.5627Treinamento: batch 1274 iniciou com 12:06:37.850158\n",
            "Treinamento: batch 1274 terminou com 12:06:38.001953\n",
            "1275/1875 [===================>..........] - ETA: 1:29 - loss: 1.1078 - accuracy: 0.5629Treinamento: batch 1275 iniciou com 12:06:38.005000\n",
            "Treinamento: batch 1275 terminou com 12:06:38.167854\n",
            "1276/1875 [===================>..........] - ETA: 1:29 - loss: 1.1074 - accuracy: 0.5630Treinamento: batch 1276 iniciou com 12:06:38.169738\n",
            "Treinamento: batch 1276 terminou com 12:06:38.326826\n",
            "1277/1875 [===================>..........] - ETA: 1:29 - loss: 1.1070 - accuracy: 0.5632Treinamento: batch 1277 iniciou com 12:06:38.330556\n",
            "Treinamento: batch 1277 terminou com 12:06:38.492366\n",
            "1278/1875 [===================>..........] - ETA: 1:28 - loss: 1.1068 - accuracy: 0.5632Treinamento: batch 1278 iniciou com 12:06:38.494357\n",
            "Treinamento: batch 1278 terminou com 12:06:38.653517\n",
            "1279/1875 [===================>..........] - ETA: 1:28 - loss: 1.1064 - accuracy: 0.5634Treinamento: batch 1279 iniciou com 12:06:38.655556\n",
            "Treinamento: batch 1279 terminou com 12:06:38.822015\n",
            "1280/1875 [===================>..........] - ETA: 1:28 - loss: 1.1062 - accuracy: 0.5634Treinamento: batch 1280 iniciou com 12:06:38.823755\n",
            "Treinamento: batch 1280 terminou com 12:06:38.986162\n",
            "1281/1875 [===================>..........] - ETA: 1:28 - loss: 1.1062 - accuracy: 0.5635Treinamento: batch 1281 iniciou com 12:06:38.988115\n",
            "Treinamento: batch 1281 terminou com 12:06:39.156976\n",
            "1282/1875 [===================>..........] - ETA: 1:28 - loss: 1.1064 - accuracy: 0.5634Treinamento: batch 1282 iniciou com 12:06:39.160067\n",
            "Treinamento: batch 1282 terminou com 12:06:39.321265\n",
            "1283/1875 [===================>..........] - ETA: 1:28 - loss: 1.1062 - accuracy: 0.5635Treinamento: batch 1283 iniciou com 12:06:39.323262\n",
            "Treinamento: batch 1283 terminou com 12:06:39.480284\n",
            "1284/1875 [===================>..........] - ETA: 1:28 - loss: 1.1058 - accuracy: 0.5636Treinamento: batch 1284 iniciou com 12:06:39.481989\n",
            "Treinamento: batch 1284 terminou com 12:06:39.648191\n",
            "1285/1875 [===================>..........] - ETA: 1:27 - loss: 1.1056 - accuracy: 0.5638Treinamento: batch 1285 iniciou com 12:06:39.650381\n",
            "Treinamento: batch 1285 terminou com 12:06:39.805710\n",
            "1286/1875 [===================>..........] - ETA: 1:27 - loss: 1.1054 - accuracy: 0.5638Treinamento: batch 1286 iniciou com 12:06:39.807666\n",
            "Treinamento: batch 1286 terminou com 12:06:39.960405\n",
            "1287/1875 [===================>..........] - ETA: 1:27 - loss: 1.1052 - accuracy: 0.5640Treinamento: batch 1287 iniciou com 12:06:39.962305\n",
            "Treinamento: batch 1287 terminou com 12:06:40.122295\n",
            "1288/1875 [===================>..........] - ETA: 1:27 - loss: 1.1049 - accuracy: 0.5641Treinamento: batch 1288 iniciou com 12:06:40.124328\n",
            "Treinamento: batch 1288 terminou com 12:06:40.281293\n",
            "1289/1875 [===================>..........] - ETA: 1:27 - loss: 1.1046 - accuracy: 0.5642Treinamento: batch 1289 iniciou com 12:06:40.283611\n",
            "Treinamento: batch 1289 terminou com 12:06:40.437439\n",
            "1290/1875 [===================>..........] - ETA: 1:27 - loss: 1.1044 - accuracy: 0.5643Treinamento: batch 1290 iniciou com 12:06:40.439161\n",
            "Treinamento: batch 1290 terminou com 12:06:40.598420\n",
            "1291/1875 [===================>..........] - ETA: 1:27 - loss: 1.1041 - accuracy: 0.5645Treinamento: batch 1291 iniciou com 12:06:40.600278\n",
            "Treinamento: batch 1291 terminou com 12:06:40.757321\n",
            "1292/1875 [===================>..........] - ETA: 1:26 - loss: 1.1038 - accuracy: 0.5646Treinamento: batch 1292 iniciou com 12:06:40.759531\n",
            "Treinamento: batch 1292 terminou com 12:06:40.913462\n",
            "1293/1875 [===================>..........] - ETA: 1:26 - loss: 1.1036 - accuracy: 0.5647Treinamento: batch 1293 iniciou com 12:06:40.915393\n",
            "Treinamento: batch 1293 terminou com 12:06:41.083798\n",
            "1294/1875 [===================>..........] - ETA: 1:26 - loss: 1.1031 - accuracy: 0.5649Treinamento: batch 1294 iniciou com 12:06:41.086143\n",
            "Treinamento: batch 1294 terminou com 12:06:41.251938\n",
            "1295/1875 [===================>..........] - ETA: 1:26 - loss: 1.1030 - accuracy: 0.5650Treinamento: batch 1295 iniciou com 12:06:41.253993\n",
            "Treinamento: batch 1295 terminou com 12:06:41.410099\n",
            "1296/1875 [===================>..........] - ETA: 1:26 - loss: 1.1028 - accuracy: 0.5651Treinamento: batch 1296 iniciou com 12:06:41.411937\n",
            "Treinamento: batch 1296 terminou com 12:06:41.574256\n",
            "1297/1875 [===================>..........] - ETA: 1:26 - loss: 1.1024 - accuracy: 0.5653Treinamento: batch 1297 iniciou com 12:06:41.576017\n",
            "Treinamento: batch 1297 terminou com 12:06:41.724460\n",
            "1298/1875 [===================>..........] - ETA: 1:26 - loss: 1.1020 - accuracy: 0.5655Treinamento: batch 1298 iniciou com 12:06:41.726393\n",
            "Treinamento: batch 1298 terminou com 12:06:41.879475\n",
            "1299/1875 [===================>..........] - ETA: 1:25 - loss: 1.1018 - accuracy: 0.5656Treinamento: batch 1299 iniciou com 12:06:41.881929\n",
            "Treinamento: batch 1299 terminou com 12:06:42.037004\n",
            "1300/1875 [===================>..........] - ETA: 1:25 - loss: 1.1017 - accuracy: 0.5657Treinamento: batch 1300 iniciou com 12:06:42.044317\n",
            "Treinamento: batch 1300 terminou com 12:06:42.211187\n",
            "1301/1875 [===================>..........] - ETA: 1:25 - loss: 1.1013 - accuracy: 0.5659Treinamento: batch 1301 iniciou com 12:06:42.213157\n",
            "Treinamento: batch 1301 terminou com 12:06:42.366340\n",
            "1302/1875 [===================>..........] - ETA: 1:25 - loss: 1.1010 - accuracy: 0.5660Treinamento: batch 1302 iniciou com 12:06:42.368101\n",
            "Treinamento: batch 1302 terminou com 12:06:42.512050\n",
            "1303/1875 [===================>..........] - ETA: 1:25 - loss: 1.1009 - accuracy: 0.5661Treinamento: batch 1303 iniciou com 12:06:42.514024\n",
            "Treinamento: batch 1303 terminou com 12:06:42.675305\n",
            "1304/1875 [===================>..........] - ETA: 1:25 - loss: 1.1008 - accuracy: 0.5661Treinamento: batch 1304 iniciou com 12:06:42.678139\n",
            "Treinamento: batch 1304 terminou com 12:06:42.830199\n",
            "1305/1875 [===================>..........] - ETA: 1:25 - loss: 1.1005 - accuracy: 0.5663Treinamento: batch 1305 iniciou com 12:06:42.832547\n",
            "Treinamento: batch 1305 terminou com 12:06:42.988013\n",
            "1306/1875 [===================>..........] - ETA: 1:24 - loss: 1.1001 - accuracy: 0.5664Treinamento: batch 1306 iniciou com 12:06:42.990158\n",
            "Treinamento: batch 1306 terminou com 12:06:43.147616\n",
            "1307/1875 [===================>..........] - ETA: 1:24 - loss: 1.0998 - accuracy: 0.5665Treinamento: batch 1307 iniciou com 12:06:43.149800\n",
            "Treinamento: batch 1307 terminou com 12:06:43.303287\n",
            "1308/1875 [===================>..........] - ETA: 1:24 - loss: 1.0994 - accuracy: 0.5668Treinamento: batch 1308 iniciou com 12:06:43.305469\n",
            "Treinamento: batch 1308 terminou com 12:06:43.460720\n",
            "1309/1875 [===================>..........] - ETA: 1:24 - loss: 1.0993 - accuracy: 0.5668Treinamento: batch 1309 iniciou com 12:06:43.462395\n",
            "Treinamento: batch 1309 terminou com 12:06:43.621137\n",
            "1310/1875 [===================>..........] - ETA: 1:24 - loss: 1.0992 - accuracy: 0.5669Treinamento: batch 1310 iniciou com 12:06:43.622943\n",
            "Treinamento: batch 1310 terminou com 12:06:43.776453\n",
            "1311/1875 [===================>..........] - ETA: 1:24 - loss: 1.0989 - accuracy: 0.5670Treinamento: batch 1311 iniciou com 12:06:43.779267\n",
            "Treinamento: batch 1311 terminou com 12:06:43.927497\n",
            "1312/1875 [===================>..........] - ETA: 1:23 - loss: 1.0986 - accuracy: 0.5672Treinamento: batch 1312 iniciou com 12:06:43.929294\n",
            "Treinamento: batch 1312 terminou com 12:06:44.083673\n",
            "1313/1875 [====================>.........] - ETA: 1:23 - loss: 1.0984 - accuracy: 0.5673Treinamento: batch 1313 iniciou com 12:06:44.085539\n",
            "Treinamento: batch 1313 terminou com 12:06:44.234831\n",
            "1314/1875 [====================>.........] - ETA: 1:23 - loss: 1.0982 - accuracy: 0.5674Treinamento: batch 1314 iniciou com 12:06:44.242390\n",
            "Treinamento: batch 1314 terminou com 12:06:44.388102\n",
            "1315/1875 [====================>.........] - ETA: 1:23 - loss: 1.0980 - accuracy: 0.5674Treinamento: batch 1315 iniciou com 12:06:44.389888\n",
            "Treinamento: batch 1315 terminou com 12:06:44.543975\n",
            "1316/1875 [====================>.........] - ETA: 1:23 - loss: 1.0978 - accuracy: 0.5676Treinamento: batch 1316 iniciou com 12:06:44.545696\n",
            "Treinamento: batch 1316 terminou com 12:06:44.694564\n",
            "1317/1875 [====================>.........] - ETA: 1:23 - loss: 1.0978 - accuracy: 0.5675Treinamento: batch 1317 iniciou com 12:06:44.700751\n",
            "Treinamento: batch 1317 terminou com 12:06:44.846730\n",
            "1318/1875 [====================>.........] - ETA: 1:23 - loss: 1.0977 - accuracy: 0.5676Treinamento: batch 1318 iniciou com 12:06:44.848467\n",
            "Treinamento: batch 1318 terminou com 12:06:44.993172\n",
            "1319/1875 [====================>.........] - ETA: 1:22 - loss: 1.0974 - accuracy: 0.5677Treinamento: batch 1319 iniciou com 12:06:44.995403\n",
            "Treinamento: batch 1319 terminou com 12:06:45.144238\n",
            "1320/1875 [====================>.........] - ETA: 1:22 - loss: 1.0970 - accuracy: 0.5679Treinamento: batch 1320 iniciou com 12:06:45.146684\n",
            "Treinamento: batch 1320 terminou com 12:06:45.293703\n",
            "1321/1875 [====================>.........] - ETA: 1:22 - loss: 1.0972 - accuracy: 0.5679Treinamento: batch 1321 iniciou com 12:06:45.295345\n",
            "Treinamento: batch 1321 terminou com 12:06:45.438655\n",
            "1322/1875 [====================>.........] - ETA: 1:22 - loss: 1.0968 - accuracy: 0.5680Treinamento: batch 1322 iniciou com 12:06:45.440684\n",
            "Treinamento: batch 1322 terminou com 12:06:45.581729\n",
            "1323/1875 [====================>.........] - ETA: 1:22 - loss: 1.0966 - accuracy: 0.5681Treinamento: batch 1323 iniciou com 12:06:45.583784\n",
            "Treinamento: batch 1323 terminou com 12:06:45.731748\n",
            "1324/1875 [====================>.........] - ETA: 1:22 - loss: 1.0964 - accuracy: 0.5683Treinamento: batch 1324 iniciou com 12:06:45.733536\n",
            "Treinamento: batch 1324 terminou com 12:06:45.883226\n",
            "1325/1875 [====================>.........] - ETA: 1:22 - loss: 1.0962 - accuracy: 0.5684Treinamento: batch 1325 iniciou com 12:06:45.885901\n",
            "Treinamento: batch 1325 terminou com 12:06:46.030273\n",
            "1326/1875 [====================>.........] - ETA: 1:21 - loss: 1.0960 - accuracy: 0.5685Treinamento: batch 1326 iniciou com 12:06:46.031993\n",
            "Treinamento: batch 1326 terminou com 12:06:46.182490\n",
            "1327/1875 [====================>.........] - ETA: 1:21 - loss: 1.0956 - accuracy: 0.5687Treinamento: batch 1327 iniciou com 12:06:46.184324\n",
            "Treinamento: batch 1327 terminou com 12:06:46.333311\n",
            "1328/1875 [====================>.........] - ETA: 1:21 - loss: 1.0953 - accuracy: 0.5687Treinamento: batch 1328 iniciou com 12:06:46.335320\n",
            "Treinamento: batch 1328 terminou com 12:06:46.481037\n",
            "1329/1875 [====================>.........] - ETA: 1:21 - loss: 1.0949 - accuracy: 0.5689Treinamento: batch 1329 iniciou com 12:06:46.482896\n",
            "Treinamento: batch 1329 terminou com 12:06:46.628663\n",
            "1330/1875 [====================>.........] - ETA: 1:21 - loss: 1.0947 - accuracy: 0.5690Treinamento: batch 1330 iniciou com 12:06:46.630333\n",
            "Treinamento: batch 1330 terminou com 12:06:46.785595\n",
            "1331/1875 [====================>.........] - ETA: 1:21 - loss: 1.0942 - accuracy: 0.5692Treinamento: batch 1331 iniciou com 12:06:46.787447\n",
            "Treinamento: batch 1331 terminou com 12:06:46.928762\n",
            "1332/1875 [====================>.........] - ETA: 1:21 - loss: 1.0940 - accuracy: 0.5693Treinamento: batch 1332 iniciou com 12:06:46.930461\n",
            "Treinamento: batch 1332 terminou com 12:06:47.074073\n",
            "1333/1875 [====================>.........] - ETA: 1:20 - loss: 1.0941 - accuracy: 0.5693Treinamento: batch 1333 iniciou com 12:06:47.075887\n",
            "Treinamento: batch 1333 terminou com 12:06:47.226074\n",
            "1334/1875 [====================>.........] - ETA: 1:20 - loss: 1.0941 - accuracy: 0.5692Treinamento: batch 1334 iniciou com 12:06:47.227867\n",
            "Treinamento: batch 1334 terminou com 12:06:47.395424\n",
            "1335/1875 [====================>.........] - ETA: 1:20 - loss: 1.0940 - accuracy: 0.5694Treinamento: batch 1335 iniciou com 12:06:47.398233\n",
            "Treinamento: batch 1335 terminou com 12:06:47.549184\n",
            "1336/1875 [====================>.........] - ETA: 1:20 - loss: 1.0938 - accuracy: 0.5694Treinamento: batch 1336 iniciou com 12:06:47.551355\n",
            "Treinamento: batch 1336 terminou com 12:06:47.705647\n",
            "1337/1875 [====================>.........] - ETA: 1:20 - loss: 1.0937 - accuracy: 0.5695Treinamento: batch 1337 iniciou com 12:06:47.707599\n",
            "Treinamento: batch 1337 terminou com 12:06:47.854429\n",
            "1338/1875 [====================>.........] - ETA: 1:20 - loss: 1.0938 - accuracy: 0.5695Treinamento: batch 1338 iniciou com 12:06:47.856404\n",
            "Treinamento: batch 1338 terminou com 12:06:48.000085\n",
            "1339/1875 [====================>.........] - ETA: 1:19 - loss: 1.0937 - accuracy: 0.5695Treinamento: batch 1339 iniciou com 12:06:48.001898\n",
            "Treinamento: batch 1339 terminou com 12:06:48.147844\n",
            "1340/1875 [====================>.........] - ETA: 1:19 - loss: 1.0934 - accuracy: 0.5697Treinamento: batch 1340 iniciou com 12:06:48.149611\n",
            "Treinamento: batch 1340 terminou com 12:06:48.300276\n",
            "1341/1875 [====================>.........] - ETA: 1:19 - loss: 1.0932 - accuracy: 0.5698Treinamento: batch 1341 iniciou com 12:06:48.302384\n",
            "Treinamento: batch 1341 terminou com 12:06:48.453996\n",
            "1342/1875 [====================>.........] - ETA: 1:19 - loss: 1.0929 - accuracy: 0.5699Treinamento: batch 1342 iniciou com 12:06:48.455756\n",
            "Treinamento: batch 1342 terminou com 12:06:48.599199\n",
            "1343/1875 [====================>.........] - ETA: 1:19 - loss: 1.0928 - accuracy: 0.5700Treinamento: batch 1343 iniciou com 12:06:48.600867\n",
            "Treinamento: batch 1343 terminou com 12:06:48.754790\n",
            "1344/1875 [====================>.........] - ETA: 1:19 - loss: 1.0929 - accuracy: 0.5700Treinamento: batch 1344 iniciou com 12:06:48.759257\n",
            "Treinamento: batch 1344 terminou com 12:06:48.905760\n",
            "1345/1875 [====================>.........] - ETA: 1:19 - loss: 1.0926 - accuracy: 0.5702Treinamento: batch 1345 iniciou com 12:06:48.907982\n",
            "Treinamento: batch 1345 terminou com 12:06:49.055191\n",
            "1346/1875 [====================>.........] - ETA: 1:18 - loss: 1.0925 - accuracy: 0.5703Treinamento: batch 1346 iniciou com 12:06:49.057011\n",
            "Treinamento: batch 1346 terminou com 12:06:49.200543\n",
            "1347/1875 [====================>.........] - ETA: 1:18 - loss: 1.0924 - accuracy: 0.5704Treinamento: batch 1347 iniciou com 12:06:49.202289\n",
            "Treinamento: batch 1347 terminou com 12:06:49.353134\n",
            "1348/1875 [====================>.........] - ETA: 1:18 - loss: 1.0922 - accuracy: 0.5705Treinamento: batch 1348 iniciou com 12:06:49.355032\n",
            "Treinamento: batch 1348 terminou com 12:06:49.499236\n",
            "1349/1875 [====================>.........] - ETA: 1:18 - loss: 1.0919 - accuracy: 0.5707Treinamento: batch 1349 iniciou com 12:06:49.501374\n",
            "Treinamento: batch 1349 terminou com 12:06:49.650791\n",
            "1350/1875 [====================>.........] - ETA: 1:18 - loss: 1.0917 - accuracy: 0.5708Treinamento: batch 1350 iniciou com 12:06:49.652467\n",
            "Treinamento: batch 1350 terminou com 12:06:49.806164\n",
            "1351/1875 [====================>.........] - ETA: 1:18 - loss: 1.0918 - accuracy: 0.5707Treinamento: batch 1351 iniciou com 12:06:49.808399\n",
            "Treinamento: batch 1351 terminou com 12:06:49.954702\n",
            "1352/1875 [====================>.........] - ETA: 1:18 - loss: 1.0916 - accuracy: 0.5708Treinamento: batch 1352 iniciou com 12:06:49.956405\n",
            "Treinamento: batch 1352 terminou com 12:06:50.105957\n",
            "1353/1875 [====================>.........] - ETA: 1:17 - loss: 1.0914 - accuracy: 0.5709Treinamento: batch 1353 iniciou com 12:06:50.107777\n",
            "Treinamento: batch 1353 terminou com 12:06:50.260066\n",
            "1354/1875 [====================>.........] - ETA: 1:17 - loss: 1.0910 - accuracy: 0.5710Treinamento: batch 1354 iniciou com 12:06:50.261844\n",
            "Treinamento: batch 1354 terminou com 12:06:50.407971\n",
            "1355/1875 [====================>.........] - ETA: 1:17 - loss: 1.0909 - accuracy: 0.5711Treinamento: batch 1355 iniciou com 12:06:50.409734\n",
            "Treinamento: batch 1355 terminou com 12:06:50.556103\n",
            "1356/1875 [====================>.........] - ETA: 1:17 - loss: 1.0906 - accuracy: 0.5712Treinamento: batch 1356 iniciou com 12:06:50.557845\n",
            "Treinamento: batch 1356 terminou com 12:06:50.706396\n",
            "1357/1875 [====================>.........] - ETA: 1:17 - loss: 1.0903 - accuracy: 0.5713Treinamento: batch 1357 iniciou com 12:06:50.710535\n",
            "Treinamento: batch 1357 terminou com 12:06:50.865224\n",
            "1358/1875 [====================>.........] - ETA: 1:17 - loss: 1.0900 - accuracy: 0.5714Treinamento: batch 1358 iniciou com 12:06:50.867519\n",
            "Treinamento: batch 1358 terminou com 12:06:51.020916\n",
            "1359/1875 [====================>.........] - ETA: 1:17 - loss: 1.0901 - accuracy: 0.5714Treinamento: batch 1359 iniciou com 12:06:51.022767\n",
            "Treinamento: batch 1359 terminou com 12:06:51.171489\n",
            "1360/1875 [====================>.........] - ETA: 1:16 - loss: 1.0900 - accuracy: 0.5715Treinamento: batch 1360 iniciou com 12:06:51.173191\n",
            "Treinamento: batch 1360 terminou com 12:06:51.318163\n",
            "1361/1875 [====================>.........] - ETA: 1:16 - loss: 1.0897 - accuracy: 0.5716Treinamento: batch 1361 iniciou com 12:06:51.319931\n",
            "Treinamento: batch 1361 terminou com 12:06:51.467389\n",
            "1362/1875 [====================>.........] - ETA: 1:16 - loss: 1.0897 - accuracy: 0.5716Treinamento: batch 1362 iniciou com 12:06:51.469084\n",
            "Treinamento: batch 1362 terminou com 12:06:51.613762\n",
            "1363/1875 [====================>.........] - ETA: 1:16 - loss: 1.0895 - accuracy: 0.5716Treinamento: batch 1363 iniciou com 12:06:51.615430\n",
            "Treinamento: batch 1363 terminou com 12:06:51.771710\n",
            "1364/1875 [====================>.........] - ETA: 1:16 - loss: 1.0892 - accuracy: 0.5718Treinamento: batch 1364 iniciou com 12:06:51.773346\n",
            "Treinamento: batch 1364 terminou com 12:06:51.918393\n",
            "1365/1875 [====================>.........] - ETA: 1:16 - loss: 1.0891 - accuracy: 0.5718Treinamento: batch 1365 iniciou com 12:06:51.920771\n",
            "Treinamento: batch 1365 terminou com 12:06:52.069380\n",
            "1366/1875 [====================>.........] - ETA: 1:15 - loss: 1.0887 - accuracy: 0.5721Treinamento: batch 1366 iniciou com 12:06:52.071356\n",
            "Treinamento: batch 1366 terminou com 12:06:52.221393\n",
            "1367/1875 [====================>.........] - ETA: 1:15 - loss: 1.0886 - accuracy: 0.5723Treinamento: batch 1367 iniciou com 12:06:52.223289\n",
            "Treinamento: batch 1367 terminou com 12:06:52.378867\n",
            "1368/1875 [====================>.........] - ETA: 1:15 - loss: 1.0882 - accuracy: 0.5724Treinamento: batch 1368 iniciou com 12:06:52.381300\n",
            "Treinamento: batch 1368 terminou com 12:06:52.529389\n",
            "1369/1875 [====================>.........] - ETA: 1:15 - loss: 1.0878 - accuracy: 0.5726Treinamento: batch 1369 iniciou com 12:06:52.531163\n",
            "Treinamento: batch 1369 terminou com 12:06:52.673034\n",
            "1370/1875 [====================>.........] - ETA: 1:15 - loss: 1.0876 - accuracy: 0.5727Treinamento: batch 1370 iniciou com 12:06:52.675060\n",
            "Treinamento: batch 1370 terminou com 12:06:52.831739\n",
            "1371/1875 [====================>.........] - ETA: 1:15 - loss: 1.0875 - accuracy: 0.5728Treinamento: batch 1371 iniciou com 12:06:52.833688\n",
            "Treinamento: batch 1371 terminou com 12:06:52.986177\n",
            "1372/1875 [====================>.........] - ETA: 1:15 - loss: 1.0873 - accuracy: 0.5730Treinamento: batch 1372 iniciou com 12:06:52.987962\n",
            "Treinamento: batch 1372 terminou com 12:06:53.141744\n",
            "1373/1875 [====================>.........] - ETA: 1:14 - loss: 1.0870 - accuracy: 0.5731Treinamento: batch 1373 iniciou com 12:06:53.143549\n",
            "Treinamento: batch 1373 terminou com 12:06:53.299779\n",
            "1374/1875 [====================>.........] - ETA: 1:14 - loss: 1.0868 - accuracy: 0.5732Treinamento: batch 1374 iniciou com 12:06:53.301581\n",
            "Treinamento: batch 1374 terminou com 12:06:53.457289\n",
            "1375/1875 [=====================>........] - ETA: 1:14 - loss: 1.0864 - accuracy: 0.5734Treinamento: batch 1375 iniciou com 12:06:53.459283\n",
            "Treinamento: batch 1375 terminou com 12:06:53.611059\n",
            "1376/1875 [=====================>........] - ETA: 1:14 - loss: 1.0864 - accuracy: 0.5734Treinamento: batch 1376 iniciou com 12:06:53.612901\n",
            "Treinamento: batch 1376 terminou com 12:06:53.762160\n",
            "1377/1875 [=====================>........] - ETA: 1:14 - loss: 1.0861 - accuracy: 0.5736Treinamento: batch 1377 iniciou com 12:06:53.763792\n",
            "Treinamento: batch 1377 terminou com 12:06:53.921159\n",
            "1378/1875 [=====================>........] - ETA: 1:14 - loss: 1.0858 - accuracy: 0.5738Treinamento: batch 1378 iniciou com 12:06:53.922831\n",
            "Treinamento: batch 1378 terminou com 12:06:54.069782\n",
            "1379/1875 [=====================>........] - ETA: 1:14 - loss: 1.0856 - accuracy: 0.5739Treinamento: batch 1379 iniciou com 12:06:54.071492\n",
            "Treinamento: batch 1379 terminou com 12:06:54.220428\n",
            "1380/1875 [=====================>........] - ETA: 1:13 - loss: 1.0855 - accuracy: 0.5740Treinamento: batch 1380 iniciou com 12:06:54.222746\n",
            "Treinamento: batch 1380 terminou com 12:06:54.363966\n",
            "1381/1875 [=====================>........] - ETA: 1:13 - loss: 1.0851 - accuracy: 0.5741Treinamento: batch 1381 iniciou com 12:06:54.365601\n",
            "Treinamento: batch 1381 terminou com 12:06:54.515147\n",
            "1382/1875 [=====================>........] - ETA: 1:13 - loss: 1.0849 - accuracy: 0.5743Treinamento: batch 1382 iniciou com 12:06:54.516940\n",
            "Treinamento: batch 1382 terminou com 12:06:54.662477\n",
            "1383/1875 [=====================>........] - ETA: 1:13 - loss: 1.0845 - accuracy: 0.5745Treinamento: batch 1383 iniciou com 12:06:54.664205\n",
            "Treinamento: batch 1383 terminou com 12:06:54.814903\n",
            "1384/1875 [=====================>........] - ETA: 1:13 - loss: 1.0840 - accuracy: 0.5747Treinamento: batch 1384 iniciou com 12:06:54.817540\n",
            "Treinamento: batch 1384 terminou com 12:06:54.975571\n",
            "1385/1875 [=====================>........] - ETA: 1:13 - loss: 1.0838 - accuracy: 0.5749Treinamento: batch 1385 iniciou com 12:06:54.977865\n",
            "Treinamento: batch 1385 terminou com 12:06:55.132764\n",
            "1386/1875 [=====================>........] - ETA: 1:13 - loss: 1.0836 - accuracy: 0.5750Treinamento: batch 1386 iniciou com 12:06:55.134887\n",
            "Treinamento: batch 1386 terminou com 12:06:55.275914\n",
            "1387/1875 [=====================>........] - ETA: 1:12 - loss: 1.0834 - accuracy: 0.5751Treinamento: batch 1387 iniciou com 12:06:55.277609\n",
            "Treinamento: batch 1387 terminou com 12:06:55.423269\n",
            "1388/1875 [=====================>........] - ETA: 1:12 - loss: 1.0831 - accuracy: 0.5752Treinamento: batch 1388 iniciou com 12:06:55.425892\n",
            "Treinamento: batch 1388 terminou com 12:06:55.569173\n",
            "1389/1875 [=====================>........] - ETA: 1:12 - loss: 1.0828 - accuracy: 0.5753Treinamento: batch 1389 iniciou com 12:06:55.570943\n",
            "Treinamento: batch 1389 terminou com 12:06:55.711136\n",
            "1390/1875 [=====================>........] - ETA: 1:12 - loss: 1.0828 - accuracy: 0.5754Treinamento: batch 1390 iniciou com 12:06:55.712871\n",
            "Treinamento: batch 1390 terminou com 12:06:55.868763\n",
            "1391/1875 [=====================>........] - ETA: 1:12 - loss: 1.0827 - accuracy: 0.5754Treinamento: batch 1391 iniciou com 12:06:55.870367\n",
            "Treinamento: batch 1391 terminou com 12:06:56.017853\n",
            "1392/1875 [=====================>........] - ETA: 1:12 - loss: 1.0824 - accuracy: 0.5755Treinamento: batch 1392 iniciou com 12:06:56.021209\n",
            "Treinamento: batch 1392 terminou com 12:06:56.173087\n",
            "1393/1875 [=====================>........] - ETA: 1:11 - loss: 1.0822 - accuracy: 0.5757Treinamento: batch 1393 iniciou com 12:06:56.174825\n",
            "Treinamento: batch 1393 terminou com 12:06:56.320985\n",
            "1394/1875 [=====================>........] - ETA: 1:11 - loss: 1.0819 - accuracy: 0.5758Treinamento: batch 1394 iniciou com 12:06:56.322721\n",
            "Treinamento: batch 1394 terminou com 12:06:56.472787\n",
            "1395/1875 [=====================>........] - ETA: 1:11 - loss: 1.0816 - accuracy: 0.5760Treinamento: batch 1395 iniciou com 12:06:56.474511\n",
            "Treinamento: batch 1395 terminou com 12:06:56.628612\n",
            "1396/1875 [=====================>........] - ETA: 1:11 - loss: 1.0814 - accuracy: 0.5760Treinamento: batch 1396 iniciou com 12:06:56.630332\n",
            "Treinamento: batch 1396 terminou com 12:06:56.776471\n",
            "1397/1875 [=====================>........] - ETA: 1:11 - loss: 1.0810 - accuracy: 0.5763Treinamento: batch 1397 iniciou com 12:06:56.778285\n",
            "Treinamento: batch 1397 terminou com 12:06:56.935274\n",
            "1398/1875 [=====================>........] - ETA: 1:11 - loss: 1.0807 - accuracy: 0.5764Treinamento: batch 1398 iniciou com 12:06:56.937027\n",
            "Treinamento: batch 1398 terminou com 12:06:57.085306\n",
            "1399/1875 [=====================>........] - ETA: 1:11 - loss: 1.0804 - accuracy: 0.5765Treinamento: batch 1399 iniciou com 12:06:57.087140\n",
            "Treinamento: batch 1399 terminou com 12:06:57.228088\n",
            "1400/1875 [=====================>........] - ETA: 1:10 - loss: 1.0803 - accuracy: 0.5766Treinamento: batch 1400 iniciou com 12:06:57.229917\n",
            "Treinamento: batch 1400 terminou com 12:06:57.378019\n",
            "1401/1875 [=====================>........] - ETA: 1:10 - loss: 1.0802 - accuracy: 0.5767Treinamento: batch 1401 iniciou com 12:06:57.379701\n",
            "Treinamento: batch 1401 terminou com 12:06:57.523053\n",
            "1402/1875 [=====================>........] - ETA: 1:10 - loss: 1.0799 - accuracy: 0.5767Treinamento: batch 1402 iniciou com 12:06:57.524777\n",
            "Treinamento: batch 1402 terminou com 12:06:57.675558\n",
            "1403/1875 [=====================>........] - ETA: 1:10 - loss: 1.0796 - accuracy: 0.5769Treinamento: batch 1403 iniciou com 12:06:57.677227\n",
            "Treinamento: batch 1403 terminou com 12:06:57.859667\n",
            "1404/1875 [=====================>........] - ETA: 1:10 - loss: 1.0792 - accuracy: 0.5770Treinamento: batch 1404 iniciou com 12:06:57.861398\n",
            "Treinamento: batch 1404 terminou com 12:06:58.022365\n",
            "1405/1875 [=====================>........] - ETA: 1:10 - loss: 1.0789 - accuracy: 0.5771Treinamento: batch 1405 iniciou com 12:06:58.024208\n",
            "Treinamento: batch 1405 terminou com 12:06:58.166605\n",
            "1406/1875 [=====================>........] - ETA: 1:10 - loss: 1.0786 - accuracy: 0.5773Treinamento: batch 1406 iniciou com 12:06:58.168695\n",
            "Treinamento: batch 1406 terminou com 12:06:58.328615\n",
            "1407/1875 [=====================>........] - ETA: 1:09 - loss: 1.0783 - accuracy: 0.5774Treinamento: batch 1407 iniciou com 12:06:58.330467\n",
            "Treinamento: batch 1407 terminou com 12:06:58.478762\n",
            "1408/1875 [=====================>........] - ETA: 1:09 - loss: 1.0779 - accuracy: 0.5775Treinamento: batch 1408 iniciou com 12:06:58.480521\n",
            "Treinamento: batch 1408 terminou com 12:06:58.625206\n",
            "1409/1875 [=====================>........] - ETA: 1:09 - loss: 1.0776 - accuracy: 0.5776Treinamento: batch 1409 iniciou com 12:06:58.627983\n",
            "Treinamento: batch 1409 terminou com 12:06:58.772665\n",
            "1410/1875 [=====================>........] - ETA: 1:09 - loss: 1.0774 - accuracy: 0.5778Treinamento: batch 1410 iniciou com 12:06:58.774378\n",
            "Treinamento: batch 1410 terminou com 12:06:58.933329\n",
            "1411/1875 [=====================>........] - ETA: 1:09 - loss: 1.0770 - accuracy: 0.5779Treinamento: batch 1411 iniciou com 12:06:58.935110\n",
            "Treinamento: batch 1411 terminou com 12:06:59.083073\n",
            "1412/1875 [=====================>........] - ETA: 1:09 - loss: 1.0767 - accuracy: 0.5781Treinamento: batch 1412 iniciou com 12:06:59.084813\n",
            "Treinamento: batch 1412 terminou com 12:06:59.232858\n",
            "1413/1875 [=====================>........] - ETA: 1:08 - loss: 1.0764 - accuracy: 0.5782Treinamento: batch 1413 iniciou com 12:06:59.234447\n",
            "Treinamento: batch 1413 terminou com 12:06:59.383749\n",
            "1414/1875 [=====================>........] - ETA: 1:08 - loss: 1.0763 - accuracy: 0.5782Treinamento: batch 1414 iniciou com 12:06:59.385426\n",
            "Treinamento: batch 1414 terminou com 12:06:59.535234\n",
            "1415/1875 [=====================>........] - ETA: 1:08 - loss: 1.0761 - accuracy: 0.5782Treinamento: batch 1415 iniciou com 12:06:59.537282\n",
            "Treinamento: batch 1415 terminou com 12:06:59.681508\n",
            "1416/1875 [=====================>........] - ETA: 1:08 - loss: 1.0761 - accuracy: 0.5783Treinamento: batch 1416 iniciou com 12:06:59.683136\n",
            "Treinamento: batch 1416 terminou com 12:06:59.827862\n",
            "1417/1875 [=====================>........] - ETA: 1:08 - loss: 1.0759 - accuracy: 0.5784Treinamento: batch 1417 iniciou com 12:06:59.829529\n",
            "Treinamento: batch 1417 terminou com 12:06:59.980903\n",
            "1418/1875 [=====================>........] - ETA: 1:08 - loss: 1.0756 - accuracy: 0.5786Treinamento: batch 1418 iniciou com 12:06:59.982623\n",
            "Treinamento: batch 1418 terminou com 12:07:00.131055\n",
            "1419/1875 [=====================>........] - ETA: 1:08 - loss: 1.0754 - accuracy: 0.5787Treinamento: batch 1419 iniciou com 12:07:00.132787\n",
            "Treinamento: batch 1419 terminou com 12:07:00.284205\n",
            "1420/1875 [=====================>........] - ETA: 1:07 - loss: 1.0750 - accuracy: 0.5789Treinamento: batch 1420 iniciou com 12:07:00.286203\n",
            "Treinamento: batch 1420 terminou com 12:07:00.435442\n",
            "1421/1875 [=====================>........] - ETA: 1:07 - loss: 1.0748 - accuracy: 0.5790Treinamento: batch 1421 iniciou com 12:07:00.437477\n",
            "Treinamento: batch 1421 terminou com 12:07:00.582506\n",
            "1422/1875 [=====================>........] - ETA: 1:07 - loss: 1.0745 - accuracy: 0.5791Treinamento: batch 1422 iniciou com 12:07:00.584099\n",
            "Treinamento: batch 1422 terminou com 12:07:00.736610\n",
            "1423/1875 [=====================>........] - ETA: 1:07 - loss: 1.0742 - accuracy: 0.5793Treinamento: batch 1423 iniciou com 12:07:00.738835\n",
            "Treinamento: batch 1423 terminou com 12:07:00.902139\n",
            "1424/1875 [=====================>........] - ETA: 1:07 - loss: 1.0744 - accuracy: 0.5793Treinamento: batch 1424 iniciou com 12:07:00.903925\n",
            "Treinamento: batch 1424 terminou com 12:07:01.071322\n",
            "1425/1875 [=====================>........] - ETA: 1:07 - loss: 1.0740 - accuracy: 0.5794Treinamento: batch 1425 iniciou com 12:07:01.073050\n",
            "Treinamento: batch 1425 terminou com 12:07:01.228465\n",
            "1426/1875 [=====================>........] - ETA: 1:07 - loss: 1.0737 - accuracy: 0.5796Treinamento: batch 1426 iniciou com 12:07:01.231380\n",
            "Treinamento: batch 1426 terminou com 12:07:01.391697\n",
            "1427/1875 [=====================>........] - ETA: 1:06 - loss: 1.0736 - accuracy: 0.5797Treinamento: batch 1427 iniciou com 12:07:01.393425\n",
            "Treinamento: batch 1427 terminou com 12:07:01.560002\n",
            "1428/1875 [=====================>........] - ETA: 1:06 - loss: 1.0734 - accuracy: 0.5798Treinamento: batch 1428 iniciou com 12:07:01.562295\n",
            "Treinamento: batch 1428 terminou com 12:07:01.728988\n",
            "1429/1875 [=====================>........] - ETA: 1:06 - loss: 1.0736 - accuracy: 0.5798Treinamento: batch 1429 iniciou com 12:07:01.734344\n",
            "Treinamento: batch 1429 terminou com 12:07:01.884173\n",
            "1430/1875 [=====================>........] - ETA: 1:06 - loss: 1.0738 - accuracy: 0.5798Treinamento: batch 1430 iniciou com 12:07:01.885966\n",
            "Treinamento: batch 1430 terminou com 12:07:02.060435\n",
            "1431/1875 [=====================>........] - ETA: 1:06 - loss: 1.0734 - accuracy: 0.5799Treinamento: batch 1431 iniciou com 12:07:02.062696\n",
            "Treinamento: batch 1431 terminou com 12:07:02.217355\n",
            "1432/1875 [=====================>........] - ETA: 1:06 - loss: 1.0730 - accuracy: 0.5801Treinamento: batch 1432 iniciou com 12:07:02.219332\n",
            "Treinamento: batch 1432 terminou com 12:07:02.370127\n",
            "1433/1875 [=====================>........] - ETA: 1:06 - loss: 1.0728 - accuracy: 0.5802Treinamento: batch 1433 iniciou com 12:07:02.372974\n",
            "Treinamento: batch 1433 terminou com 12:07:02.524023\n",
            "1434/1875 [=====================>........] - ETA: 1:05 - loss: 1.0723 - accuracy: 0.5804Treinamento: batch 1434 iniciou com 12:07:02.525860\n",
            "Treinamento: batch 1434 terminou com 12:07:02.690227\n",
            "1435/1875 [=====================>........] - ETA: 1:05 - loss: 1.0723 - accuracy: 0.5804Treinamento: batch 1435 iniciou com 12:07:02.692858\n",
            "Treinamento: batch 1435 terminou com 12:07:02.852085\n",
            "1436/1875 [=====================>........] - ETA: 1:05 - loss: 1.0721 - accuracy: 0.5805Treinamento: batch 1436 iniciou com 12:07:02.853813\n",
            "Treinamento: batch 1436 terminou com 12:07:03.018509\n",
            "1437/1875 [=====================>........] - ETA: 1:05 - loss: 1.0719 - accuracy: 0.5805Treinamento: batch 1437 iniciou com 12:07:03.023279\n",
            "Treinamento: batch 1437 terminou com 12:07:03.176455\n",
            "1438/1875 [======================>.......] - ETA: 1:05 - loss: 1.0717 - accuracy: 0.5806Treinamento: batch 1438 iniciou com 12:07:03.178613\n",
            "Treinamento: batch 1438 terminou com 12:07:03.341669\n",
            "1439/1875 [======================>.......] - ETA: 1:05 - loss: 1.0715 - accuracy: 0.5806Treinamento: batch 1439 iniciou com 12:07:03.344119\n",
            "Treinamento: batch 1439 terminou com 12:07:03.507877\n",
            "1440/1875 [======================>.......] - ETA: 1:05 - loss: 1.0717 - accuracy: 0.5807Treinamento: batch 1440 iniciou com 12:07:03.509772\n",
            "Treinamento: batch 1440 terminou com 12:07:03.659332\n",
            "1441/1875 [======================>.......] - ETA: 1:04 - loss: 1.0715 - accuracy: 0.5808Treinamento: batch 1441 iniciou com 12:07:03.661158\n",
            "Treinamento: batch 1441 terminou com 12:07:03.816710\n",
            "1442/1875 [======================>.......] - ETA: 1:04 - loss: 1.0712 - accuracy: 0.5809Treinamento: batch 1442 iniciou com 12:07:03.818961\n",
            "Treinamento: batch 1442 terminou com 12:07:03.968796\n",
            "1443/1875 [======================>.......] - ETA: 1:04 - loss: 1.0709 - accuracy: 0.5811Treinamento: batch 1443 iniciou com 12:07:03.970578\n",
            "Treinamento: batch 1443 terminou com 12:07:04.137558\n",
            "1444/1875 [======================>.......] - ETA: 1:04 - loss: 1.0708 - accuracy: 0.5812Treinamento: batch 1444 iniciou com 12:07:04.139181\n",
            "Treinamento: batch 1444 terminou com 12:07:04.295620\n",
            "1445/1875 [======================>.......] - ETA: 1:04 - loss: 1.0706 - accuracy: 0.5813Treinamento: batch 1445 iniciou com 12:07:04.297992\n",
            "Treinamento: batch 1445 terminou com 12:07:04.466608\n",
            "1446/1875 [======================>.......] - ETA: 1:04 - loss: 1.0703 - accuracy: 0.5814Treinamento: batch 1446 iniciou com 12:07:04.468531\n",
            "Treinamento: batch 1446 terminou com 12:07:04.616255\n",
            "1447/1875 [======================>.......] - ETA: 1:04 - loss: 1.0701 - accuracy: 0.5815Treinamento: batch 1447 iniciou com 12:07:04.618342\n",
            "Treinamento: batch 1447 terminou com 12:07:04.769216\n",
            "1448/1875 [======================>.......] - ETA: 1:03 - loss: 1.0700 - accuracy: 0.5816Treinamento: batch 1448 iniciou com 12:07:04.771045\n",
            "Treinamento: batch 1448 terminou com 12:07:04.920393\n",
            "1449/1875 [======================>.......] - ETA: 1:03 - loss: 1.0697 - accuracy: 0.5817Treinamento: batch 1449 iniciou com 12:07:04.922211\n",
            "Treinamento: batch 1449 terminou com 12:07:05.080783\n",
            "1450/1875 [======================>.......] - ETA: 1:03 - loss: 1.0696 - accuracy: 0.5817Treinamento: batch 1450 iniciou com 12:07:05.082522\n",
            "Treinamento: batch 1450 terminou com 12:07:05.238025\n",
            "1451/1875 [======================>.......] - ETA: 1:03 - loss: 1.0694 - accuracy: 0.5817Treinamento: batch 1451 iniciou com 12:07:05.239911\n",
            "Treinamento: batch 1451 terminou com 12:07:05.389022\n",
            "1452/1875 [======================>.......] - ETA: 1:03 - loss: 1.0690 - accuracy: 0.5819Treinamento: batch 1452 iniciou com 12:07:05.390767\n",
            "Treinamento: batch 1452 terminou com 12:07:05.539158\n",
            "1453/1875 [======================>.......] - ETA: 1:03 - loss: 1.0688 - accuracy: 0.5821Treinamento: batch 1453 iniciou com 12:07:05.540824\n",
            "Treinamento: batch 1453 terminou com 12:07:05.698468\n",
            "1454/1875 [======================>.......] - ETA: 1:02 - loss: 1.0686 - accuracy: 0.5822Treinamento: batch 1454 iniciou com 12:07:05.700779\n",
            "Treinamento: batch 1454 terminou com 12:07:05.851566\n",
            "1455/1875 [======================>.......] - ETA: 1:02 - loss: 1.0685 - accuracy: 0.5823Treinamento: batch 1455 iniciou com 12:07:05.853333\n",
            "Treinamento: batch 1455 terminou com 12:07:06.010361\n",
            "1456/1875 [======================>.......] - ETA: 1:02 - loss: 1.0681 - accuracy: 0.5825Treinamento: batch 1456 iniciou com 12:07:06.018407\n",
            "Treinamento: batch 1456 terminou com 12:07:06.188380\n",
            "1457/1875 [======================>.......] - ETA: 1:02 - loss: 1.0678 - accuracy: 0.5826Treinamento: batch 1457 iniciou com 12:07:06.190706\n",
            "Treinamento: batch 1457 terminou com 12:07:06.346538\n",
            "1458/1875 [======================>.......] - ETA: 1:02 - loss: 1.0677 - accuracy: 0.5826Treinamento: batch 1458 iniciou com 12:07:06.348518\n",
            "Treinamento: batch 1458 terminou com 12:07:06.495522\n",
            "1459/1875 [======================>.......] - ETA: 1:02 - loss: 1.0674 - accuracy: 0.5828Treinamento: batch 1459 iniciou com 12:07:06.497446\n",
            "Treinamento: batch 1459 terminou com 12:07:06.651609\n",
            "1460/1875 [======================>.......] - ETA: 1:02 - loss: 1.0671 - accuracy: 0.5828Treinamento: batch 1460 iniciou com 12:07:06.653473\n",
            "Treinamento: batch 1460 terminou com 12:07:06.802189\n",
            "1461/1875 [======================>.......] - ETA: 1:01 - loss: 1.0670 - accuracy: 0.5828Treinamento: batch 1461 iniciou com 12:07:06.803936\n",
            "Treinamento: batch 1461 terminou com 12:07:06.961560\n",
            "1462/1875 [======================>.......] - ETA: 1:01 - loss: 1.0667 - accuracy: 0.5830Treinamento: batch 1462 iniciou com 12:07:06.963083\n",
            "Treinamento: batch 1462 terminou com 12:07:07.114698\n",
            "1463/1875 [======================>.......] - ETA: 1:01 - loss: 1.0664 - accuracy: 0.5831Treinamento: batch 1463 iniciou com 12:07:07.116187\n",
            "Treinamento: batch 1463 terminou com 12:07:07.266171\n",
            "1464/1875 [======================>.......] - ETA: 1:01 - loss: 1.0661 - accuracy: 0.5832Treinamento: batch 1464 iniciou com 12:07:07.267801\n",
            "Treinamento: batch 1464 terminou com 12:07:07.406704\n",
            "1465/1875 [======================>.......] - ETA: 1:01 - loss: 1.0659 - accuracy: 0.5833Treinamento: batch 1465 iniciou com 12:07:07.408097\n",
            "Treinamento: batch 1465 terminou com 12:07:07.550781\n",
            "1466/1875 [======================>.......] - ETA: 1:01 - loss: 1.0655 - accuracy: 0.5834Treinamento: batch 1466 iniciou com 12:07:07.552350\n",
            "Treinamento: batch 1466 terminou com 12:07:07.696380\n",
            "1467/1875 [======================>.......] - ETA: 1:01 - loss: 1.0652 - accuracy: 0.5835Treinamento: batch 1467 iniciou com 12:07:07.697978\n",
            "Treinamento: batch 1467 terminou com 12:07:07.837567\n",
            "1468/1875 [======================>.......] - ETA: 1:00 - loss: 1.0651 - accuracy: 0.5836Treinamento: batch 1468 iniciou com 12:07:07.839175\n",
            "Treinamento: batch 1468 terminou com 12:07:07.981202\n",
            "1469/1875 [======================>.......] - ETA: 1:00 - loss: 1.0649 - accuracy: 0.5837Treinamento: batch 1469 iniciou com 12:07:07.983026\n",
            "Treinamento: batch 1469 terminou com 12:07:08.175403\n",
            "1470/1875 [======================>.......] - ETA: 1:00 - loss: 1.0649 - accuracy: 0.5838Treinamento: batch 1470 iniciou com 12:07:08.177680\n",
            "Treinamento: batch 1470 terminou com 12:07:08.333008\n",
            "1471/1875 [======================>.......] - ETA: 1:00 - loss: 1.0645 - accuracy: 0.5839Treinamento: batch 1471 iniciou com 12:07:08.334593\n",
            "Treinamento: batch 1471 terminou com 12:07:08.492176\n",
            "1472/1875 [======================>.......] - ETA: 1:00 - loss: 1.0644 - accuracy: 0.5840Treinamento: batch 1472 iniciou com 12:07:08.493886\n",
            "Treinamento: batch 1472 terminou com 12:07:08.649980\n",
            "1473/1875 [======================>.......] - ETA: 1:00 - loss: 1.0640 - accuracy: 0.5841Treinamento: batch 1473 iniciou com 12:07:08.652851\n",
            "Treinamento: batch 1473 terminou com 12:07:08.801966\n",
            "1474/1875 [======================>.......] - ETA: 1:00 - loss: 1.0637 - accuracy: 0.5842Treinamento: batch 1474 iniciou com 12:07:08.803725\n",
            "Treinamento: batch 1474 terminou com 12:07:08.954744\n",
            "1475/1875 [======================>.......] - ETA: 59s - loss: 1.0634 - accuracy: 0.5844 Treinamento: batch 1475 iniciou com 12:07:08.956539\n",
            "Treinamento: batch 1475 terminou com 12:07:09.104698\n",
            "1476/1875 [======================>.......] - ETA: 59s - loss: 1.0634 - accuracy: 0.5844Treinamento: batch 1476 iniciou com 12:07:09.106442\n",
            "Treinamento: batch 1476 terminou com 12:07:09.268190\n",
            "1477/1875 [======================>.......] - ETA: 59s - loss: 1.0631 - accuracy: 0.5845Treinamento: batch 1477 iniciou com 12:07:09.270310\n",
            "Treinamento: batch 1477 terminou com 12:07:09.420248\n",
            "1478/1875 [======================>.......] - ETA: 59s - loss: 1.0630 - accuracy: 0.5846Treinamento: batch 1478 iniciou com 12:07:09.422188\n",
            "Treinamento: batch 1478 terminou com 12:07:09.573476\n",
            "1479/1875 [======================>.......] - ETA: 59s - loss: 1.0627 - accuracy: 0.5847Treinamento: batch 1479 iniciou com 12:07:09.575163\n",
            "Treinamento: batch 1479 terminou com 12:07:09.722611\n",
            "1480/1875 [======================>.......] - ETA: 59s - loss: 1.0624 - accuracy: 0.5849Treinamento: batch 1480 iniciou com 12:07:09.724284\n",
            "Treinamento: batch 1480 terminou com 12:07:09.874768\n",
            "1481/1875 [======================>.......] - ETA: 58s - loss: 1.0622 - accuracy: 0.5849Treinamento: batch 1481 iniciou com 12:07:09.876365\n",
            "Treinamento: batch 1481 terminou com 12:07:10.024877\n",
            "1482/1875 [======================>.......] - ETA: 58s - loss: 1.0619 - accuracy: 0.5850Treinamento: batch 1482 iniciou com 12:07:10.026622\n",
            "Treinamento: batch 1482 terminou com 12:07:10.191562\n",
            "1483/1875 [======================>.......] - ETA: 58s - loss: 1.0616 - accuracy: 0.5852Treinamento: batch 1483 iniciou com 12:07:10.193159\n",
            "Treinamento: batch 1483 terminou com 12:07:10.338932\n",
            "1484/1875 [======================>.......] - ETA: 58s - loss: 1.0613 - accuracy: 0.5853Treinamento: batch 1484 iniciou com 12:07:10.340690\n",
            "Treinamento: batch 1484 terminou com 12:07:10.487126\n",
            "1485/1875 [======================>.......] - ETA: 58s - loss: 1.0609 - accuracy: 0.5855Treinamento: batch 1485 iniciou com 12:07:10.488750\n",
            "Treinamento: batch 1485 terminou com 12:07:10.646790\n",
            "1486/1875 [======================>.......] - ETA: 58s - loss: 1.0609 - accuracy: 0.5855Treinamento: batch 1486 iniciou com 12:07:10.648511\n",
            "Treinamento: batch 1486 terminou com 12:07:10.797093\n",
            "1487/1875 [======================>.......] - ETA: 58s - loss: 1.0608 - accuracy: 0.5856Treinamento: batch 1487 iniciou com 12:07:10.798676\n",
            "Treinamento: batch 1487 terminou com 12:07:10.949908\n",
            "1488/1875 [======================>.......] - ETA: 57s - loss: 1.0609 - accuracy: 0.5856Treinamento: batch 1488 iniciou com 12:07:10.951578\n",
            "Treinamento: batch 1488 terminou com 12:07:11.105759\n",
            "1489/1875 [======================>.......] - ETA: 57s - loss: 1.0608 - accuracy: 0.5857Treinamento: batch 1489 iniciou com 12:07:11.107346\n",
            "Treinamento: batch 1489 terminou com 12:07:11.261289\n",
            "1490/1875 [======================>.......] - ETA: 57s - loss: 1.0604 - accuracy: 0.5859Treinamento: batch 1490 iniciou com 12:07:11.262860\n",
            "Treinamento: batch 1490 terminou com 12:07:11.397498\n",
            "1491/1875 [======================>.......] - ETA: 57s - loss: 1.0605 - accuracy: 0.5858Treinamento: batch 1491 iniciou com 12:07:11.398902\n",
            "Treinamento: batch 1491 terminou com 12:07:11.533334\n",
            "1492/1875 [======================>.......] - ETA: 57s - loss: 1.0605 - accuracy: 0.5858Treinamento: batch 1492 iniciou com 12:07:11.534776\n",
            "Treinamento: batch 1492 terminou com 12:07:11.678601\n",
            "1493/1875 [======================>.......] - ETA: 57s - loss: 1.0601 - accuracy: 0.5860Treinamento: batch 1493 iniciou com 12:07:11.680028\n",
            "Treinamento: batch 1493 terminou com 12:07:11.813096\n",
            "1494/1875 [======================>.......] - ETA: 57s - loss: 1.0597 - accuracy: 0.5861Treinamento: batch 1494 iniciou com 12:07:11.814453\n",
            "Treinamento: batch 1494 terminou com 12:07:11.953724\n",
            "1495/1875 [======================>.......] - ETA: 56s - loss: 1.0595 - accuracy: 0.5862Treinamento: batch 1495 iniciou com 12:07:11.955017\n",
            "Treinamento: batch 1495 terminou com 12:07:12.107832\n",
            "1496/1875 [======================>.......] - ETA: 56s - loss: 1.0594 - accuracy: 0.5863Treinamento: batch 1496 iniciou com 12:07:12.110422\n",
            "Treinamento: batch 1496 terminou com 12:07:12.285381\n",
            "1497/1875 [======================>.......] - ETA: 56s - loss: 1.0591 - accuracy: 0.5864Treinamento: batch 1497 iniciou com 12:07:12.287456\n",
            "Treinamento: batch 1497 terminou com 12:07:12.445541\n",
            "1498/1875 [======================>.......] - ETA: 56s - loss: 1.0589 - accuracy: 0.5865Treinamento: batch 1498 iniciou com 12:07:12.448773\n",
            "Treinamento: batch 1498 terminou com 12:07:12.604619\n",
            "1499/1875 [======================>.......] - ETA: 56s - loss: 1.0586 - accuracy: 0.5866Treinamento: batch 1499 iniciou com 12:07:12.606452\n",
            "Treinamento: batch 1499 terminou com 12:07:12.765803\n",
            "1500/1875 [=======================>......] - ETA: 56s - loss: 1.0583 - accuracy: 0.5868Treinamento: batch 1500 iniciou com 12:07:12.767680\n",
            "Treinamento: batch 1500 terminou com 12:07:12.916604\n",
            "1501/1875 [=======================>......] - ETA: 55s - loss: 1.0579 - accuracy: 0.5869Treinamento: batch 1501 iniciou com 12:07:12.920204\n",
            "Treinamento: batch 1501 terminou com 12:07:13.085672\n",
            "1502/1875 [=======================>......] - ETA: 55s - loss: 1.0578 - accuracy: 0.5870Treinamento: batch 1502 iniciou com 12:07:13.092396\n",
            "Treinamento: batch 1502 terminou com 12:07:13.261191\n",
            "1503/1875 [=======================>......] - ETA: 55s - loss: 1.0577 - accuracy: 0.5871Treinamento: batch 1503 iniciou com 12:07:13.263794\n",
            "Treinamento: batch 1503 terminou com 12:07:13.413319\n",
            "1504/1875 [=======================>......] - ETA: 55s - loss: 1.0573 - accuracy: 0.5872Treinamento: batch 1504 iniciou com 12:07:13.415054\n",
            "Treinamento: batch 1504 terminou com 12:07:13.569561\n",
            "1505/1875 [=======================>......] - ETA: 55s - loss: 1.0571 - accuracy: 0.5873Treinamento: batch 1505 iniciou com 12:07:13.571461\n",
            "Treinamento: batch 1505 terminou com 12:07:13.719677\n",
            "1506/1875 [=======================>......] - ETA: 55s - loss: 1.0568 - accuracy: 0.5873Treinamento: batch 1506 iniciou com 12:07:13.722124\n",
            "Treinamento: batch 1506 terminou com 12:07:13.877519\n",
            "1507/1875 [=======================>......] - ETA: 55s - loss: 1.0564 - accuracy: 0.5875Treinamento: batch 1507 iniciou com 12:07:13.879408\n",
            "Treinamento: batch 1507 terminou com 12:07:14.032976\n",
            "1508/1875 [=======================>......] - ETA: 54s - loss: 1.0562 - accuracy: 0.5875Treinamento: batch 1508 iniciou com 12:07:14.034832\n",
            "Treinamento: batch 1508 terminou com 12:07:14.179325\n",
            "1509/1875 [=======================>......] - ETA: 54s - loss: 1.0559 - accuracy: 0.5877Treinamento: batch 1509 iniciou com 12:07:14.180717\n",
            "Treinamento: batch 1509 terminou com 12:07:14.336345\n",
            "1510/1875 [=======================>......] - ETA: 54s - loss: 1.0556 - accuracy: 0.5877Treinamento: batch 1510 iniciou com 12:07:14.338267\n",
            "Treinamento: batch 1510 terminou com 12:07:14.480691\n",
            "1511/1875 [=======================>......] - ETA: 54s - loss: 1.0555 - accuracy: 0.5878Treinamento: batch 1511 iniciou com 12:07:14.485924\n",
            "Treinamento: batch 1511 terminou com 12:07:14.634682\n",
            "1512/1875 [=======================>......] - ETA: 54s - loss: 1.0552 - accuracy: 0.5879Treinamento: batch 1512 iniciou com 12:07:14.636156\n",
            "Treinamento: batch 1512 terminou com 12:07:14.781764\n",
            "1513/1875 [=======================>......] - ETA: 54s - loss: 1.0549 - accuracy: 0.5880Treinamento: batch 1513 iniciou com 12:07:14.783352\n",
            "Treinamento: batch 1513 terminou com 12:07:14.923423\n",
            "1514/1875 [=======================>......] - ETA: 54s - loss: 1.0547 - accuracy: 0.5881Treinamento: batch 1514 iniciou com 12:07:14.925330\n",
            "Treinamento: batch 1514 terminou com 12:07:15.081673\n",
            "1515/1875 [=======================>......] - ETA: 53s - loss: 1.0546 - accuracy: 0.5881Treinamento: batch 1515 iniciou com 12:07:15.083415\n",
            "Treinamento: batch 1515 terminou com 12:07:15.239126\n",
            "1516/1875 [=======================>......] - ETA: 53s - loss: 1.0544 - accuracy: 0.5882Treinamento: batch 1516 iniciou com 12:07:15.241224\n",
            "Treinamento: batch 1516 terminou com 12:07:15.395572\n",
            "1517/1875 [=======================>......] - ETA: 53s - loss: 1.0542 - accuracy: 0.5882Treinamento: batch 1517 iniciou com 12:07:15.397782\n",
            "Treinamento: batch 1517 terminou com 12:07:15.574950\n",
            "1518/1875 [=======================>......] - ETA: 53s - loss: 1.0539 - accuracy: 0.5884Treinamento: batch 1518 iniciou com 12:07:15.576866\n",
            "Treinamento: batch 1518 terminou com 12:07:15.745707\n",
            "1519/1875 [=======================>......] - ETA: 53s - loss: 1.0539 - accuracy: 0.5884Treinamento: batch 1519 iniciou com 12:07:15.747377\n",
            "Treinamento: batch 1519 terminou com 12:07:15.897050\n",
            "1520/1875 [=======================>......] - ETA: 53s - loss: 1.0541 - accuracy: 0.5884Treinamento: batch 1520 iniciou com 12:07:15.898845\n",
            "Treinamento: batch 1520 terminou com 12:07:16.051295\n",
            "1521/1875 [=======================>......] - ETA: 53s - loss: 1.0538 - accuracy: 0.5886Treinamento: batch 1521 iniciou com 12:07:16.053059\n",
            "Treinamento: batch 1521 terminou com 12:07:16.199812\n",
            "1522/1875 [=======================>......] - ETA: 52s - loss: 1.0537 - accuracy: 0.5886Treinamento: batch 1522 iniciou com 12:07:16.201526\n",
            "Treinamento: batch 1522 terminou com 12:07:16.363174\n",
            "1523/1875 [=======================>......] - ETA: 52s - loss: 1.0536 - accuracy: 0.5887Treinamento: batch 1523 iniciou com 12:07:16.364838\n",
            "Treinamento: batch 1523 terminou com 12:07:16.517159\n",
            "1524/1875 [=======================>......] - ETA: 52s - loss: 1.0533 - accuracy: 0.5888Treinamento: batch 1524 iniciou com 12:07:16.519402\n",
            "Treinamento: batch 1524 terminou com 12:07:16.674371\n",
            "1525/1875 [=======================>......] - ETA: 52s - loss: 1.0531 - accuracy: 0.5889Treinamento: batch 1525 iniciou com 12:07:16.676207\n",
            "Treinamento: batch 1525 terminou com 12:07:16.827169\n",
            "1526/1875 [=======================>......] - ETA: 52s - loss: 1.0529 - accuracy: 0.5890Treinamento: batch 1526 iniciou com 12:07:16.828875\n",
            "Treinamento: batch 1526 terminou com 12:07:16.975486\n",
            "1527/1875 [=======================>......] - ETA: 52s - loss: 1.0526 - accuracy: 0.5891Treinamento: batch 1527 iniciou com 12:07:16.977421\n",
            "Treinamento: batch 1527 terminou com 12:07:17.145699\n",
            "1528/1875 [=======================>......] - ETA: 51s - loss: 1.0525 - accuracy: 0.5891Treinamento: batch 1528 iniciou com 12:07:17.147452\n",
            "Treinamento: batch 1528 terminou com 12:07:17.303206\n",
            "1529/1875 [=======================>......] - ETA: 51s - loss: 1.0521 - accuracy: 0.5894Treinamento: batch 1529 iniciou com 12:07:17.305441\n",
            "Treinamento: batch 1529 terminou com 12:07:17.463307\n",
            "1530/1875 [=======================>......] - ETA: 51s - loss: 1.0521 - accuracy: 0.5894Treinamento: batch 1530 iniciou com 12:07:17.464918\n",
            "Treinamento: batch 1530 terminou com 12:07:17.608743\n",
            "1531/1875 [=======================>......] - ETA: 51s - loss: 1.0519 - accuracy: 0.5895Treinamento: batch 1531 iniciou com 12:07:17.610445\n",
            "Treinamento: batch 1531 terminou com 12:07:17.756900\n",
            "1532/1875 [=======================>......] - ETA: 51s - loss: 1.0517 - accuracy: 0.5896Treinamento: batch 1532 iniciou com 12:07:17.758397\n",
            "Treinamento: batch 1532 terminou com 12:07:17.907907\n",
            "1533/1875 [=======================>......] - ETA: 51s - loss: 1.0515 - accuracy: 0.5896Treinamento: batch 1533 iniciou com 12:07:17.909503\n",
            "Treinamento: batch 1533 terminou com 12:07:18.063121\n",
            "1534/1875 [=======================>......] - ETA: 51s - loss: 1.0514 - accuracy: 0.5896Treinamento: batch 1534 iniciou com 12:07:18.065695\n",
            "Treinamento: batch 1534 terminou com 12:07:18.211860\n",
            "1535/1875 [=======================>......] - ETA: 50s - loss: 1.0512 - accuracy: 0.5896Treinamento: batch 1535 iniciou com 12:07:18.213700\n",
            "Treinamento: batch 1535 terminou com 12:07:18.387814\n",
            "1536/1875 [=======================>......] - ETA: 50s - loss: 1.0509 - accuracy: 0.5897Treinamento: batch 1536 iniciou com 12:07:18.390746\n",
            "Treinamento: batch 1536 terminou com 12:07:18.567426\n",
            "1537/1875 [=======================>......] - ETA: 50s - loss: 1.0507 - accuracy: 0.5898Treinamento: batch 1537 iniciou com 12:07:18.569712\n",
            "Treinamento: batch 1537 terminou com 12:07:18.720161\n",
            "1538/1875 [=======================>......] - ETA: 50s - loss: 1.0505 - accuracy: 0.5899Treinamento: batch 1538 iniciou com 12:07:18.721690\n",
            "Treinamento: batch 1538 terminou com 12:07:18.865023\n",
            "1539/1875 [=======================>......] - ETA: 50s - loss: 1.0502 - accuracy: 0.5900Treinamento: batch 1539 iniciou com 12:07:18.866807\n",
            "Treinamento: batch 1539 terminou com 12:07:19.015021\n",
            "1540/1875 [=======================>......] - ETA: 50s - loss: 1.0501 - accuracy: 0.5900Treinamento: batch 1540 iniciou com 12:07:19.018991\n",
            "Treinamento: batch 1540 terminou com 12:07:19.167462\n",
            "1541/1875 [=======================>......] - ETA: 50s - loss: 1.0498 - accuracy: 0.5901Treinamento: batch 1541 iniciou com 12:07:19.169199\n",
            "Treinamento: batch 1541 terminou com 12:07:19.318311\n",
            "1542/1875 [=======================>......] - ETA: 49s - loss: 1.0495 - accuracy: 0.5902Treinamento: batch 1542 iniciou com 12:07:19.320130\n",
            "Treinamento: batch 1542 terminou com 12:07:19.481837\n",
            "1543/1875 [=======================>......] - ETA: 49s - loss: 1.0493 - accuracy: 0.5903Treinamento: batch 1543 iniciou com 12:07:19.483776\n",
            "Treinamento: batch 1543 terminou com 12:07:19.636418\n",
            "1544/1875 [=======================>......] - ETA: 49s - loss: 1.0489 - accuracy: 0.5905Treinamento: batch 1544 iniciou com 12:07:19.638219\n",
            "Treinamento: batch 1544 terminou com 12:07:19.785164\n",
            "1545/1875 [=======================>......] - ETA: 49s - loss: 1.0486 - accuracy: 0.5906Treinamento: batch 1545 iniciou com 12:07:19.787013\n",
            "Treinamento: batch 1545 terminou com 12:07:19.933079\n",
            "1546/1875 [=======================>......] - ETA: 49s - loss: 1.0486 - accuracy: 0.5906Treinamento: batch 1546 iniciou com 12:07:19.935021\n",
            "Treinamento: batch 1546 terminou com 12:07:20.076922\n",
            "1547/1875 [=======================>......] - ETA: 49s - loss: 1.0485 - accuracy: 0.5907Treinamento: batch 1547 iniciou com 12:07:20.078658\n",
            "Treinamento: batch 1547 terminou com 12:07:20.225272\n",
            "1548/1875 [=======================>......] - ETA: 49s - loss: 1.0483 - accuracy: 0.5908Treinamento: batch 1548 iniciou com 12:07:20.227063\n",
            "Treinamento: batch 1548 terminou com 12:07:20.375255\n",
            "1549/1875 [=======================>......] - ETA: 48s - loss: 1.0480 - accuracy: 0.5909Treinamento: batch 1549 iniciou com 12:07:20.377750\n",
            "Treinamento: batch 1549 terminou com 12:07:20.526033\n",
            "1550/1875 [=======================>......] - ETA: 48s - loss: 1.0477 - accuracy: 0.5911Treinamento: batch 1550 iniciou com 12:07:20.528113\n",
            "Treinamento: batch 1550 terminou com 12:07:20.671855\n",
            "1551/1875 [=======================>......] - ETA: 48s - loss: 1.0476 - accuracy: 0.5911Treinamento: batch 1551 iniciou com 12:07:20.673592\n",
            "Treinamento: batch 1551 terminou com 12:07:20.833515\n",
            "1552/1875 [=======================>......] - ETA: 48s - loss: 1.0474 - accuracy: 0.5912Treinamento: batch 1552 iniciou com 12:07:20.835224\n",
            "Treinamento: batch 1552 terminou com 12:07:20.984340\n",
            "1553/1875 [=======================>......] - ETA: 48s - loss: 1.0473 - accuracy: 0.5913Treinamento: batch 1553 iniciou com 12:07:20.986486\n",
            "Treinamento: batch 1553 terminou com 12:07:21.144064\n",
            "1554/1875 [=======================>......] - ETA: 48s - loss: 1.0473 - accuracy: 0.5913Treinamento: batch 1554 iniciou com 12:07:21.145749\n",
            "Treinamento: batch 1554 terminou com 12:07:21.293571\n",
            "1555/1875 [=======================>......] - ETA: 47s - loss: 1.0471 - accuracy: 0.5914Treinamento: batch 1555 iniciou com 12:07:21.295235\n",
            "Treinamento: batch 1555 terminou com 12:07:21.449452\n",
            "1556/1875 [=======================>......] - ETA: 47s - loss: 1.0470 - accuracy: 0.5914Treinamento: batch 1556 iniciou com 12:07:21.451162\n",
            "Treinamento: batch 1556 terminou com 12:07:21.608538\n",
            "1557/1875 [=======================>......] - ETA: 47s - loss: 1.0467 - accuracy: 0.5916Treinamento: batch 1557 iniciou com 12:07:21.610453\n",
            "Treinamento: batch 1557 terminou com 12:07:21.756050\n",
            "1558/1875 [=======================>......] - ETA: 47s - loss: 1.0465 - accuracy: 0.5918Treinamento: batch 1558 iniciou com 12:07:21.757941\n",
            "Treinamento: batch 1558 terminou com 12:07:21.912446\n",
            "1559/1875 [=======================>......] - ETA: 47s - loss: 1.0462 - accuracy: 0.5919Treinamento: batch 1559 iniciou com 12:07:21.914279\n",
            "Treinamento: batch 1559 terminou com 12:07:22.062127\n",
            "1560/1875 [=======================>......] - ETA: 47s - loss: 1.0461 - accuracy: 0.5920Treinamento: batch 1560 iniciou com 12:07:22.063738\n",
            "Treinamento: batch 1560 terminou com 12:07:22.210330\n",
            "1561/1875 [=======================>......] - ETA: 47s - loss: 1.0461 - accuracy: 0.5920Treinamento: batch 1561 iniciou com 12:07:22.211915\n",
            "Treinamento: batch 1561 terminou com 12:07:22.371891\n",
            "1562/1875 [=======================>......] - ETA: 46s - loss: 1.0461 - accuracy: 0.5921Treinamento: batch 1562 iniciou com 12:07:22.374170\n",
            "Treinamento: batch 1562 terminou com 12:07:22.532683\n",
            "1563/1875 [========================>.....] - ETA: 46s - loss: 1.0460 - accuracy: 0.5921Treinamento: batch 1563 iniciou com 12:07:22.534391\n",
            "Treinamento: batch 1563 terminou com 12:07:22.689426\n",
            "1564/1875 [========================>.....] - ETA: 46s - loss: 1.0459 - accuracy: 0.5922Treinamento: batch 1564 iniciou com 12:07:22.691237\n",
            "Treinamento: batch 1564 terminou com 12:07:22.845037\n",
            "1565/1875 [========================>.....] - ETA: 46s - loss: 1.0457 - accuracy: 0.5924Treinamento: batch 1565 iniciou com 12:07:22.846683\n",
            "Treinamento: batch 1565 terminou com 12:07:22.989186\n",
            "1566/1875 [========================>.....] - ETA: 46s - loss: 1.0457 - accuracy: 0.5924Treinamento: batch 1566 iniciou com 12:07:22.990792\n",
            "Treinamento: batch 1566 terminou com 12:07:23.140281\n",
            "1567/1875 [========================>.....] - ETA: 46s - loss: 1.0454 - accuracy: 0.5925Treinamento: batch 1567 iniciou com 12:07:23.141965\n",
            "Treinamento: batch 1567 terminou com 12:07:23.299283\n",
            "1568/1875 [========================>.....] - ETA: 46s - loss: 1.0451 - accuracy: 0.5926Treinamento: batch 1568 iniciou com 12:07:23.300928\n",
            "Treinamento: batch 1568 terminou com 12:07:23.457962\n",
            "1569/1875 [========================>.....] - ETA: 45s - loss: 1.0448 - accuracy: 0.5927Treinamento: batch 1569 iniciou com 12:07:23.459577\n",
            "Treinamento: batch 1569 terminou com 12:07:23.602087\n",
            "1570/1875 [========================>.....] - ETA: 45s - loss: 1.0446 - accuracy: 0.5928Treinamento: batch 1570 iniciou com 12:07:23.603706\n",
            "Treinamento: batch 1570 terminou com 12:07:23.746968\n",
            "1571/1875 [========================>.....] - ETA: 45s - loss: 1.0442 - accuracy: 0.5930Treinamento: batch 1571 iniciou com 12:07:23.748461\n",
            "Treinamento: batch 1571 terminou com 12:07:23.893084\n",
            "1572/1875 [========================>.....] - ETA: 45s - loss: 1.0442 - accuracy: 0.5930Treinamento: batch 1572 iniciou com 12:07:23.894734\n",
            "Treinamento: batch 1572 terminou com 12:07:24.033031\n",
            "1573/1875 [========================>.....] - ETA: 45s - loss: 1.0442 - accuracy: 0.5930Treinamento: batch 1573 iniciou com 12:07:24.034786\n",
            "Treinamento: batch 1573 terminou com 12:07:24.184486\n",
            "1574/1875 [========================>.....] - ETA: 45s - loss: 1.0440 - accuracy: 0.5931Treinamento: batch 1574 iniciou com 12:07:24.186036\n",
            "Treinamento: batch 1574 terminou com 12:07:24.327829\n",
            "1575/1875 [========================>.....] - ETA: 44s - loss: 1.0437 - accuracy: 0.5932Treinamento: batch 1575 iniciou com 12:07:24.329552\n",
            "Treinamento: batch 1575 terminou com 12:07:24.493564\n",
            "1576/1875 [========================>.....] - ETA: 44s - loss: 1.0436 - accuracy: 0.5932Treinamento: batch 1576 iniciou com 12:07:24.495394\n",
            "Treinamento: batch 1576 terminou com 12:07:24.651106\n",
            "1577/1875 [========================>.....] - ETA: 44s - loss: 1.0434 - accuracy: 0.5933Treinamento: batch 1577 iniciou com 12:07:24.652740\n",
            "Treinamento: batch 1577 terminou com 12:07:24.795075\n",
            "1578/1875 [========================>.....] - ETA: 44s - loss: 1.0431 - accuracy: 0.5934Treinamento: batch 1578 iniciou com 12:07:24.796697\n",
            "Treinamento: batch 1578 terminou com 12:07:24.943089\n",
            "1579/1875 [========================>.....] - ETA: 44s - loss: 1.0429 - accuracy: 0.5935Treinamento: batch 1579 iniciou com 12:07:24.944687\n",
            "Treinamento: batch 1579 terminou com 12:07:25.091041\n",
            "1580/1875 [========================>.....] - ETA: 44s - loss: 1.0425 - accuracy: 0.5937Treinamento: batch 1580 iniciou com 12:07:25.092608\n",
            "Treinamento: batch 1580 terminou com 12:07:25.237167\n",
            "1581/1875 [========================>.....] - ETA: 44s - loss: 1.0422 - accuracy: 0.5938Treinamento: batch 1581 iniciou com 12:07:25.238757\n",
            "Treinamento: batch 1581 terminou com 12:07:25.383914\n",
            "1582/1875 [========================>.....] - ETA: 43s - loss: 1.0420 - accuracy: 0.5939Treinamento: batch 1582 iniciou com 12:07:25.385546\n",
            "Treinamento: batch 1582 terminou com 12:07:25.536507\n",
            "1583/1875 [========================>.....] - ETA: 43s - loss: 1.0419 - accuracy: 0.5940Treinamento: batch 1583 iniciou com 12:07:25.538029\n",
            "Treinamento: batch 1583 terminou com 12:07:25.693010\n",
            "1584/1875 [========================>.....] - ETA: 43s - loss: 1.0417 - accuracy: 0.5940Treinamento: batch 1584 iniciou com 12:07:25.694773\n",
            "Treinamento: batch 1584 terminou com 12:07:25.841450\n",
            "1585/1875 [========================>.....] - ETA: 43s - loss: 1.0415 - accuracy: 0.5940Treinamento: batch 1585 iniciou com 12:07:25.843070\n",
            "Treinamento: batch 1585 terminou com 12:07:25.982554\n",
            "1586/1875 [========================>.....] - ETA: 43s - loss: 1.0416 - accuracy: 0.5940Treinamento: batch 1586 iniciou com 12:07:25.984123\n",
            "Treinamento: batch 1586 terminou com 12:07:26.131068\n",
            "1587/1875 [========================>.....] - ETA: 43s - loss: 1.0415 - accuracy: 0.5940Treinamento: batch 1587 iniciou com 12:07:26.132895\n",
            "Treinamento: batch 1587 terminou com 12:07:26.276654\n",
            "1588/1875 [========================>.....] - ETA: 43s - loss: 1.0413 - accuracy: 0.5941Treinamento: batch 1588 iniciou com 12:07:26.278559\n",
            "Treinamento: batch 1588 terminou com 12:07:26.425615\n",
            "1589/1875 [========================>.....] - ETA: 42s - loss: 1.0409 - accuracy: 0.5942Treinamento: batch 1589 iniciou com 12:07:26.427258\n",
            "Treinamento: batch 1589 terminou com 12:07:26.580911\n",
            "1590/1875 [========================>.....] - ETA: 42s - loss: 1.0408 - accuracy: 0.5943Treinamento: batch 1590 iniciou com 12:07:26.582400\n",
            "Treinamento: batch 1590 terminou com 12:07:26.724309\n",
            "1591/1875 [========================>.....] - ETA: 42s - loss: 1.0407 - accuracy: 0.5944Treinamento: batch 1591 iniciou com 12:07:26.727277\n",
            "Treinamento: batch 1591 terminou com 12:07:26.872272\n",
            "1592/1875 [========================>.....] - ETA: 42s - loss: 1.0404 - accuracy: 0.5944Treinamento: batch 1592 iniciou com 12:07:26.874217\n",
            "Treinamento: batch 1592 terminou com 12:07:27.026387\n",
            "1593/1875 [========================>.....] - ETA: 42s - loss: 1.0402 - accuracy: 0.5945Treinamento: batch 1593 iniciou com 12:07:27.028002\n",
            "Treinamento: batch 1593 terminou com 12:07:27.180185\n",
            "1594/1875 [========================>.....] - ETA: 42s - loss: 1.0400 - accuracy: 0.5946Treinamento: batch 1594 iniciou com 12:07:27.182455\n",
            "Treinamento: batch 1594 terminou com 12:07:27.330268\n",
            "1595/1875 [========================>.....] - ETA: 41s - loss: 1.0399 - accuracy: 0.5947Treinamento: batch 1595 iniciou com 12:07:27.332280\n",
            "Treinamento: batch 1595 terminou com 12:07:27.481802\n",
            "1596/1875 [========================>.....] - ETA: 41s - loss: 1.0396 - accuracy: 0.5948Treinamento: batch 1596 iniciou com 12:07:27.483353\n",
            "Treinamento: batch 1596 terminou com 12:07:27.639297\n",
            "1597/1875 [========================>.....] - ETA: 41s - loss: 1.0394 - accuracy: 0.5949Treinamento: batch 1597 iniciou com 12:07:27.640872\n",
            "Treinamento: batch 1597 terminou com 12:07:27.786309\n",
            "1598/1875 [========================>.....] - ETA: 41s - loss: 1.0393 - accuracy: 0.5950Treinamento: batch 1598 iniciou com 12:07:27.787937\n",
            "Treinamento: batch 1598 terminou com 12:07:27.929579\n",
            "1599/1875 [========================>.....] - ETA: 41s - loss: 1.0393 - accuracy: 0.5951Treinamento: batch 1599 iniciou com 12:07:27.931189\n",
            "Treinamento: batch 1599 terminou com 12:07:28.090183\n",
            "1600/1875 [========================>.....] - ETA: 41s - loss: 1.0392 - accuracy: 0.5951Treinamento: batch 1600 iniciou com 12:07:28.091882\n",
            "Treinamento: batch 1600 terminou com 12:07:28.244794\n",
            "1601/1875 [========================>.....] - ETA: 41s - loss: 1.0390 - accuracy: 0.5951Treinamento: batch 1601 iniciou com 12:07:28.246458\n",
            "Treinamento: batch 1601 terminou com 12:07:28.386490\n",
            "1602/1875 [========================>.....] - ETA: 40s - loss: 1.0390 - accuracy: 0.5951Treinamento: batch 1602 iniciou com 12:07:28.388336\n",
            "Treinamento: batch 1602 terminou com 12:07:28.537057\n",
            "1603/1875 [========================>.....] - ETA: 40s - loss: 1.0388 - accuracy: 0.5952Treinamento: batch 1603 iniciou com 12:07:28.539937\n",
            "Treinamento: batch 1603 terminou com 12:07:28.690422\n",
            "1604/1875 [========================>.....] - ETA: 40s - loss: 1.0387 - accuracy: 0.5952Treinamento: batch 1604 iniciou com 12:07:28.692067\n",
            "Treinamento: batch 1604 terminou com 12:07:28.851843\n",
            "1605/1875 [========================>.....] - ETA: 40s - loss: 1.0385 - accuracy: 0.5953Treinamento: batch 1605 iniciou com 12:07:28.854195\n",
            "Treinamento: batch 1605 terminou com 12:07:29.009010\n",
            "1606/1875 [========================>.....] - ETA: 40s - loss: 1.0384 - accuracy: 0.5953Treinamento: batch 1606 iniciou com 12:07:29.010603\n",
            "Treinamento: batch 1606 terminou com 12:07:29.160788\n",
            "1607/1875 [========================>.....] - ETA: 40s - loss: 1.0383 - accuracy: 0.5953Treinamento: batch 1607 iniciou com 12:07:29.163328\n",
            "Treinamento: batch 1607 terminou com 12:07:29.304380\n",
            "1608/1875 [========================>.....] - ETA: 40s - loss: 1.0381 - accuracy: 0.5954Treinamento: batch 1608 iniciou com 12:07:29.306014\n",
            "Treinamento: batch 1608 terminou com 12:07:29.446355\n",
            "1609/1875 [========================>.....] - ETA: 39s - loss: 1.0379 - accuracy: 0.5955Treinamento: batch 1609 iniciou com 12:07:29.448777\n",
            "Treinamento: batch 1609 terminou com 12:07:29.595332\n",
            "1610/1875 [========================>.....] - ETA: 39s - loss: 1.0377 - accuracy: 0.5956Treinamento: batch 1610 iniciou com 12:07:29.596902\n",
            "Treinamento: batch 1610 terminou com 12:07:29.735275\n",
            "1611/1875 [========================>.....] - ETA: 39s - loss: 1.0374 - accuracy: 0.5957Treinamento: batch 1611 iniciou com 12:07:29.736604\n",
            "Treinamento: batch 1611 terminou com 12:07:29.870297\n",
            "1612/1875 [========================>.....] - ETA: 39s - loss: 1.0371 - accuracy: 0.5958Treinamento: batch 1612 iniciou com 12:07:29.871852\n",
            "Treinamento: batch 1612 terminou com 12:07:30.006069\n",
            "1613/1875 [========================>.....] - ETA: 39s - loss: 1.0369 - accuracy: 0.5959Treinamento: batch 1613 iniciou com 12:07:30.007540\n",
            "Treinamento: batch 1613 terminou com 12:07:30.139059\n",
            "1614/1875 [========================>.....] - ETA: 39s - loss: 1.0368 - accuracy: 0.5961Treinamento: batch 1614 iniciou com 12:07:30.140483\n",
            "Treinamento: batch 1614 terminou com 12:07:30.274913\n",
            "1615/1875 [========================>.....] - ETA: 38s - loss: 1.0370 - accuracy: 0.5960Treinamento: batch 1615 iniciou com 12:07:30.276247\n",
            "Treinamento: batch 1615 terminou com 12:07:30.411777\n",
            "1616/1875 [========================>.....] - ETA: 38s - loss: 1.0367 - accuracy: 0.5961Treinamento: batch 1616 iniciou com 12:07:30.415426\n",
            "Treinamento: batch 1616 terminou com 12:07:30.551831\n",
            "1617/1875 [========================>.....] - ETA: 38s - loss: 1.0365 - accuracy: 0.5963Treinamento: batch 1617 iniciou com 12:07:30.553262\n",
            "Treinamento: batch 1617 terminou com 12:07:30.707882\n",
            "1618/1875 [========================>.....] - ETA: 38s - loss: 1.0364 - accuracy: 0.5963Treinamento: batch 1618 iniciou com 12:07:30.709426\n",
            "Treinamento: batch 1618 terminou com 12:07:30.841131\n",
            "1619/1875 [========================>.....] - ETA: 38s - loss: 1.0363 - accuracy: 0.5964Treinamento: batch 1619 iniciou com 12:07:30.842491\n",
            "Treinamento: batch 1619 terminou com 12:07:30.976119\n",
            "1620/1875 [========================>.....] - ETA: 38s - loss: 1.0364 - accuracy: 0.5964Treinamento: batch 1620 iniciou com 12:07:30.977479\n",
            "Treinamento: batch 1620 terminou com 12:07:31.114313\n",
            "1621/1875 [========================>.....] - ETA: 38s - loss: 1.0362 - accuracy: 0.5965Treinamento: batch 1621 iniciou com 12:07:31.115791\n",
            "Treinamento: batch 1621 terminou com 12:07:31.258681\n",
            "1622/1875 [========================>.....] - ETA: 37s - loss: 1.0360 - accuracy: 0.5966Treinamento: batch 1622 iniciou com 12:07:31.261294\n",
            "Treinamento: batch 1622 terminou com 12:07:31.394787\n",
            "1623/1875 [========================>.....] - ETA: 37s - loss: 1.0357 - accuracy: 0.5968Treinamento: batch 1623 iniciou com 12:07:31.396528\n",
            "Treinamento: batch 1623 terminou com 12:07:31.539480\n",
            "1624/1875 [========================>.....] - ETA: 37s - loss: 1.0354 - accuracy: 0.5969Treinamento: batch 1624 iniciou com 12:07:31.540906\n",
            "Treinamento: batch 1624 terminou com 12:07:31.681311\n",
            "1625/1875 [=========================>....] - ETA: 37s - loss: 1.0352 - accuracy: 0.5970Treinamento: batch 1625 iniciou com 12:07:31.683298\n",
            "Treinamento: batch 1625 terminou com 12:07:31.817551\n",
            "1626/1875 [=========================>....] - ETA: 37s - loss: 1.0351 - accuracy: 0.5970Treinamento: batch 1626 iniciou com 12:07:31.819365\n",
            "Treinamento: batch 1626 terminou com 12:07:31.951146\n",
            "1627/1875 [=========================>....] - ETA: 37s - loss: 1.0347 - accuracy: 0.5972Treinamento: batch 1627 iniciou com 12:07:31.952510\n",
            "Treinamento: batch 1627 terminou com 12:07:32.101995\n",
            "1628/1875 [=========================>....] - ETA: 37s - loss: 1.0343 - accuracy: 0.5973Treinamento: batch 1628 iniciou com 12:07:32.103620\n",
            "Treinamento: batch 1628 terminou com 12:07:32.255425\n",
            "1629/1875 [=========================>....] - ETA: 36s - loss: 1.0341 - accuracy: 0.5974Treinamento: batch 1629 iniciou com 12:07:32.257103\n",
            "Treinamento: batch 1629 terminou com 12:07:32.399843\n",
            "1630/1875 [=========================>....] - ETA: 36s - loss: 1.0339 - accuracy: 0.5975Treinamento: batch 1630 iniciou com 12:07:32.401581\n",
            "Treinamento: batch 1630 terminou com 12:07:32.546574\n",
            "1631/1875 [=========================>....] - ETA: 36s - loss: 1.0337 - accuracy: 0.5976Treinamento: batch 1631 iniciou com 12:07:32.548263\n",
            "Treinamento: batch 1631 terminou com 12:07:32.705524\n",
            "1632/1875 [=========================>....] - ETA: 36s - loss: 1.0336 - accuracy: 0.5977Treinamento: batch 1632 iniciou com 12:07:32.707413\n",
            "Treinamento: batch 1632 terminou com 12:07:32.847965\n",
            "1633/1875 [=========================>....] - ETA: 36s - loss: 1.0335 - accuracy: 0.5977Treinamento: batch 1633 iniciou com 12:07:32.849586\n",
            "Treinamento: batch 1633 terminou com 12:07:32.990974\n",
            "1634/1875 [=========================>....] - ETA: 36s - loss: 1.0334 - accuracy: 0.5978Treinamento: batch 1634 iniciou com 12:07:32.992688\n",
            "Treinamento: batch 1634 terminou com 12:07:33.147832\n",
            "1635/1875 [=========================>....] - ETA: 35s - loss: 1.0334 - accuracy: 0.5979Treinamento: batch 1635 iniciou com 12:07:33.149548\n",
            "Treinamento: batch 1635 terminou com 12:07:33.299918\n",
            "1636/1875 [=========================>....] - ETA: 35s - loss: 1.0332 - accuracy: 0.5979Treinamento: batch 1636 iniciou com 12:07:33.301596\n",
            "Treinamento: batch 1636 terminou com 12:07:33.445663\n",
            "1637/1875 [=========================>....] - ETA: 35s - loss: 1.0331 - accuracy: 0.5980Treinamento: batch 1637 iniciou com 12:07:33.447373\n",
            "Treinamento: batch 1637 terminou com 12:07:33.594434\n",
            "1638/1875 [=========================>....] - ETA: 35s - loss: 1.0329 - accuracy: 0.5981Treinamento: batch 1638 iniciou com 12:07:33.596139\n",
            "Treinamento: batch 1638 terminou com 12:07:33.746998\n",
            "1639/1875 [=========================>....] - ETA: 35s - loss: 1.0327 - accuracy: 0.5982Treinamento: batch 1639 iniciou com 12:07:33.748610\n",
            "Treinamento: batch 1639 terminou com 12:07:33.898816\n",
            "1640/1875 [=========================>....] - ETA: 35s - loss: 1.0324 - accuracy: 0.5983Treinamento: batch 1640 iniciou com 12:07:33.900438\n",
            "Treinamento: batch 1640 terminou com 12:07:34.047853\n",
            "1641/1875 [=========================>....] - ETA: 35s - loss: 1.0322 - accuracy: 0.5984Treinamento: batch 1641 iniciou com 12:07:34.049775\n",
            "Treinamento: batch 1641 terminou com 12:07:34.194620\n",
            "1642/1875 [=========================>....] - ETA: 34s - loss: 1.0320 - accuracy: 0.5984Treinamento: batch 1642 iniciou com 12:07:34.196348\n",
            "Treinamento: batch 1642 terminou com 12:07:34.347105\n",
            "1643/1875 [=========================>....] - ETA: 34s - loss: 1.0318 - accuracy: 0.5986Treinamento: batch 1643 iniciou com 12:07:34.349055\n",
            "Treinamento: batch 1643 terminou com 12:07:34.495073\n",
            "1644/1875 [=========================>....] - ETA: 34s - loss: 1.0315 - accuracy: 0.5987Treinamento: batch 1644 iniciou com 12:07:34.501219\n",
            "Treinamento: batch 1644 terminou com 12:07:34.653968\n",
            "1645/1875 [=========================>....] - ETA: 34s - loss: 1.0312 - accuracy: 0.5988Treinamento: batch 1645 iniciou com 12:07:34.655659\n",
            "Treinamento: batch 1645 terminou com 12:07:34.809506\n",
            "1646/1875 [=========================>....] - ETA: 34s - loss: 1.0310 - accuracy: 0.5989Treinamento: batch 1646 iniciou com 12:07:34.811860\n",
            "Treinamento: batch 1646 terminou com 12:07:34.954494\n",
            "1647/1875 [=========================>....] - ETA: 34s - loss: 1.0307 - accuracy: 0.5990Treinamento: batch 1647 iniciou com 12:07:34.957070\n",
            "Treinamento: batch 1647 terminou com 12:07:35.103361\n",
            "1648/1875 [=========================>....] - ETA: 34s - loss: 1.0305 - accuracy: 0.5991Treinamento: batch 1648 iniciou com 12:07:35.104965\n",
            "Treinamento: batch 1648 terminou com 12:07:35.250846\n",
            "1649/1875 [=========================>....] - ETA: 33s - loss: 1.0304 - accuracy: 0.5992Treinamento: batch 1649 iniciou com 12:07:35.252475\n",
            "Treinamento: batch 1649 terminou com 12:07:35.397770\n",
            "1650/1875 [=========================>....] - ETA: 33s - loss: 1.0301 - accuracy: 0.5993Treinamento: batch 1650 iniciou com 12:07:35.399739\n",
            "Treinamento: batch 1650 terminou com 12:07:35.542082\n",
            "1651/1875 [=========================>....] - ETA: 33s - loss: 1.0301 - accuracy: 0.5993Treinamento: batch 1651 iniciou com 12:07:35.543788\n",
            "Treinamento: batch 1651 terminou com 12:07:35.692551\n",
            "1652/1875 [=========================>....] - ETA: 33s - loss: 1.0300 - accuracy: 0.5993Treinamento: batch 1652 iniciou com 12:07:35.694571\n",
            "Treinamento: batch 1652 terminou com 12:07:35.849892\n",
            "1653/1875 [=========================>....] - ETA: 33s - loss: 1.0298 - accuracy: 0.5994Treinamento: batch 1653 iniciou com 12:07:35.852193\n",
            "Treinamento: batch 1653 terminou com 12:07:35.994861\n",
            "1654/1875 [=========================>....] - ETA: 33s - loss: 1.0297 - accuracy: 0.5995Treinamento: batch 1654 iniciou com 12:07:35.996524\n",
            "Treinamento: batch 1654 terminou com 12:07:36.143776\n",
            "1655/1875 [=========================>....] - ETA: 32s - loss: 1.0296 - accuracy: 0.5995Treinamento: batch 1655 iniciou com 12:07:36.145526\n",
            "Treinamento: batch 1655 terminou com 12:07:36.280097\n",
            "1656/1875 [=========================>....] - ETA: 32s - loss: 1.0293 - accuracy: 0.5996Treinamento: batch 1656 iniciou com 12:07:36.281452\n",
            "Treinamento: batch 1656 terminou com 12:07:36.413551\n",
            "1657/1875 [=========================>....] - ETA: 32s - loss: 1.0293 - accuracy: 0.5997Treinamento: batch 1657 iniciou com 12:07:36.415150\n",
            "Treinamento: batch 1657 terminou com 12:07:36.552746\n",
            "1658/1875 [=========================>....] - ETA: 32s - loss: 1.0291 - accuracy: 0.5998Treinamento: batch 1658 iniciou com 12:07:36.554228\n",
            "Treinamento: batch 1658 terminou com 12:07:36.688325\n",
            "1659/1875 [=========================>....] - ETA: 32s - loss: 1.0291 - accuracy: 0.5998Treinamento: batch 1659 iniciou com 12:07:36.689692\n",
            "Treinamento: batch 1659 terminou com 12:07:36.834753\n",
            "1660/1875 [=========================>....] - ETA: 32s - loss: 1.0289 - accuracy: 0.5998Treinamento: batch 1660 iniciou com 12:07:36.836219\n",
            "Treinamento: batch 1660 terminou com 12:07:36.967894\n",
            "1661/1875 [=========================>....] - ETA: 32s - loss: 1.0287 - accuracy: 0.5999Treinamento: batch 1661 iniciou com 12:07:36.969295\n",
            "Treinamento: batch 1661 terminou com 12:07:37.111813\n",
            "1662/1875 [=========================>....] - ETA: 31s - loss: 1.0287 - accuracy: 0.6000Treinamento: batch 1662 iniciou com 12:07:37.113463\n",
            "Treinamento: batch 1662 terminou com 12:07:37.256506\n",
            "1663/1875 [=========================>....] - ETA: 31s - loss: 1.0287 - accuracy: 0.6001Treinamento: batch 1663 iniciou com 12:07:37.258183\n",
            "Treinamento: batch 1663 terminou com 12:07:37.411370\n",
            "1664/1875 [=========================>....] - ETA: 31s - loss: 1.0286 - accuracy: 0.6001Treinamento: batch 1664 iniciou com 12:07:37.413080\n",
            "Treinamento: batch 1664 terminou com 12:07:37.557039\n",
            "1665/1875 [=========================>....] - ETA: 31s - loss: 1.0286 - accuracy: 0.6002Treinamento: batch 1665 iniciou com 12:07:37.558730\n",
            "Treinamento: batch 1665 terminou com 12:07:37.699621\n",
            "1666/1875 [=========================>....] - ETA: 31s - loss: 1.0282 - accuracy: 0.6003Treinamento: batch 1666 iniciou com 12:07:37.701336\n",
            "Treinamento: batch 1666 terminou com 12:07:37.854048\n",
            "1667/1875 [=========================>....] - ETA: 31s - loss: 1.0281 - accuracy: 0.6004Treinamento: batch 1667 iniciou com 12:07:37.855676\n",
            "Treinamento: batch 1667 terminou com 12:07:38.001326\n",
            "1668/1875 [=========================>....] - ETA: 30s - loss: 1.0279 - accuracy: 0.6004Treinamento: batch 1668 iniciou com 12:07:38.003001\n",
            "Treinamento: batch 1668 terminou com 12:07:38.142160\n",
            "1669/1875 [=========================>....] - ETA: 30s - loss: 1.0277 - accuracy: 0.6005Treinamento: batch 1669 iniciou com 12:07:38.143799\n",
            "Treinamento: batch 1669 terminou com 12:07:38.296007\n",
            "1670/1875 [=========================>....] - ETA: 30s - loss: 1.0274 - accuracy: 0.6006Treinamento: batch 1670 iniciou com 12:07:38.297612\n",
            "Treinamento: batch 1670 terminou com 12:07:38.441834\n",
            "1671/1875 [=========================>....] - ETA: 30s - loss: 1.0273 - accuracy: 0.6007Treinamento: batch 1671 iniciou com 12:07:38.443818\n",
            "Treinamento: batch 1671 terminou com 12:07:38.587805\n",
            "1672/1875 [=========================>....] - ETA: 30s - loss: 1.0270 - accuracy: 0.6008Treinamento: batch 1672 iniciou com 12:07:38.590361\n",
            "Treinamento: batch 1672 terminou com 12:07:38.741550\n",
            "1673/1875 [=========================>....] - ETA: 30s - loss: 1.0267 - accuracy: 0.6008Treinamento: batch 1673 iniciou com 12:07:38.743514\n",
            "Treinamento: batch 1673 terminou com 12:07:38.895579\n",
            "1674/1875 [=========================>....] - ETA: 30s - loss: 1.0266 - accuracy: 0.6009Treinamento: batch 1674 iniciou com 12:07:38.897283\n",
            "Treinamento: batch 1674 terminou com 12:07:39.044724\n",
            "1675/1875 [=========================>....] - ETA: 29s - loss: 1.0264 - accuracy: 0.6010Treinamento: batch 1675 iniciou com 12:07:39.046465\n",
            "Treinamento: batch 1675 terminou com 12:07:39.219956\n",
            "1676/1875 [=========================>....] - ETA: 29s - loss: 1.0261 - accuracy: 0.6011Treinamento: batch 1676 iniciou com 12:07:39.221869\n",
            "Treinamento: batch 1676 terminou com 12:07:39.367555\n",
            "1677/1875 [=========================>....] - ETA: 29s - loss: 1.0259 - accuracy: 0.6011Treinamento: batch 1677 iniciou com 12:07:39.369232\n",
            "Treinamento: batch 1677 terminou com 12:07:39.511929\n",
            "1678/1875 [=========================>....] - ETA: 29s - loss: 1.0259 - accuracy: 0.6011Treinamento: batch 1678 iniciou com 12:07:39.513580\n",
            "Treinamento: batch 1678 terminou com 12:07:39.656271\n",
            "1679/1875 [=========================>....] - ETA: 29s - loss: 1.0257 - accuracy: 0.6012Treinamento: batch 1679 iniciou com 12:07:39.657932\n",
            "Treinamento: batch 1679 terminou com 12:07:39.809673\n",
            "1680/1875 [=========================>....] - ETA: 29s - loss: 1.0254 - accuracy: 0.6013Treinamento: batch 1680 iniciou com 12:07:39.811331\n",
            "Treinamento: batch 1680 terminou com 12:07:39.956396\n",
            "1681/1875 [=========================>....] - ETA: 29s - loss: 1.0251 - accuracy: 0.6014Treinamento: batch 1681 iniciou com 12:07:39.957890\n",
            "Treinamento: batch 1681 terminou com 12:07:40.107844\n",
            "1682/1875 [=========================>....] - ETA: 28s - loss: 1.0250 - accuracy: 0.6015Treinamento: batch 1682 iniciou com 12:07:40.109727\n",
            "Treinamento: batch 1682 terminou com 12:07:40.255891\n",
            "1683/1875 [=========================>....] - ETA: 28s - loss: 1.0250 - accuracy: 0.6015Treinamento: batch 1683 iniciou com 12:07:40.257553\n",
            "Treinamento: batch 1683 terminou com 12:07:40.400288\n",
            "1684/1875 [=========================>....] - ETA: 28s - loss: 1.0248 - accuracy: 0.6016Treinamento: batch 1684 iniciou com 12:07:40.401901\n",
            "Treinamento: batch 1684 terminou com 12:07:40.543905\n",
            "1685/1875 [=========================>....] - ETA: 28s - loss: 1.0247 - accuracy: 0.6017Treinamento: batch 1685 iniciou com 12:07:40.545465\n",
            "Treinamento: batch 1685 terminou com 12:07:40.691015\n",
            "1686/1875 [=========================>....] - ETA: 28s - loss: 1.0245 - accuracy: 0.6017Treinamento: batch 1686 iniciou com 12:07:40.692777\n",
            "Treinamento: batch 1686 terminou com 12:07:40.851015\n",
            "1687/1875 [=========================>....] - ETA: 28s - loss: 1.0244 - accuracy: 0.6017Treinamento: batch 1687 iniciou com 12:07:40.852676\n",
            "Treinamento: batch 1687 terminou com 12:07:40.994620\n",
            "1688/1875 [==========================>...] - ETA: 28s - loss: 1.0242 - accuracy: 0.6018Treinamento: batch 1688 iniciou com 12:07:40.996731\n",
            "Treinamento: batch 1688 terminou com 12:07:41.146508\n",
            "1689/1875 [==========================>...] - ETA: 27s - loss: 1.0240 - accuracy: 0.6019Treinamento: batch 1689 iniciou com 12:07:41.148193\n",
            "Treinamento: batch 1689 terminou com 12:07:41.295347\n",
            "1690/1875 [==========================>...] - ETA: 27s - loss: 1.0238 - accuracy: 0.6019Treinamento: batch 1690 iniciou com 12:07:41.296995\n",
            "Treinamento: batch 1690 terminou com 12:07:41.447145\n",
            "1691/1875 [==========================>...] - ETA: 27s - loss: 1.0236 - accuracy: 0.6020Treinamento: batch 1691 iniciou com 12:07:41.448865\n",
            "Treinamento: batch 1691 terminou com 12:07:41.597052\n",
            "1692/1875 [==========================>...] - ETA: 27s - loss: 1.0235 - accuracy: 0.6020Treinamento: batch 1692 iniciou com 12:07:41.598697\n",
            "Treinamento: batch 1692 terminou com 12:07:41.738134\n",
            "1693/1875 [==========================>...] - ETA: 27s - loss: 1.0234 - accuracy: 0.6020Treinamento: batch 1693 iniciou com 12:07:41.739748\n",
            "Treinamento: batch 1693 terminou com 12:07:41.895738\n",
            "1694/1875 [==========================>...] - ETA: 27s - loss: 1.0231 - accuracy: 0.6022Treinamento: batch 1694 iniciou com 12:07:41.897371\n",
            "Treinamento: batch 1694 terminou com 12:07:42.039031\n",
            "1695/1875 [==========================>...] - ETA: 26s - loss: 1.0227 - accuracy: 0.6023Treinamento: batch 1695 iniciou com 12:07:42.040676\n",
            "Treinamento: batch 1695 terminou com 12:07:42.187838\n",
            "1696/1875 [==========================>...] - ETA: 26s - loss: 1.0226 - accuracy: 0.6024Treinamento: batch 1696 iniciou com 12:07:42.189927\n",
            "Treinamento: batch 1696 terminou com 12:07:42.345803\n",
            "1697/1875 [==========================>...] - ETA: 26s - loss: 1.0224 - accuracy: 0.6024Treinamento: batch 1697 iniciou com 12:07:42.347483\n",
            "Treinamento: batch 1697 terminou com 12:07:42.492009\n",
            "1698/1875 [==========================>...] - ETA: 26s - loss: 1.0222 - accuracy: 0.6025Treinamento: batch 1698 iniciou com 12:07:42.493667\n",
            "Treinamento: batch 1698 terminou com 12:07:42.639532\n",
            "1699/1875 [==========================>...] - ETA: 26s - loss: 1.0221 - accuracy: 0.6025Treinamento: batch 1699 iniciou com 12:07:42.641050\n",
            "Treinamento: batch 1699 terminou com 12:07:42.784393\n",
            "1700/1875 [==========================>...] - ETA: 26s - loss: 1.0219 - accuracy: 0.6026Treinamento: batch 1700 iniciou com 12:07:42.786001\n",
            "Treinamento: batch 1700 terminou com 12:07:42.943897\n",
            "1701/1875 [==========================>...] - ETA: 26s - loss: 1.0218 - accuracy: 0.6026Treinamento: batch 1701 iniciou com 12:07:42.945561\n",
            "Treinamento: batch 1701 terminou com 12:07:43.092371\n",
            "1702/1875 [==========================>...] - ETA: 25s - loss: 1.0217 - accuracy: 0.6027Treinamento: batch 1702 iniciou com 12:07:43.094034\n",
            "Treinamento: batch 1702 terminou com 12:07:43.241272\n",
            "1703/1875 [==========================>...] - ETA: 25s - loss: 1.0215 - accuracy: 0.6027Treinamento: batch 1703 iniciou com 12:07:43.243058\n",
            "Treinamento: batch 1703 terminou com 12:07:43.393054\n",
            "1704/1875 [==========================>...] - ETA: 25s - loss: 1.0214 - accuracy: 0.6028Treinamento: batch 1704 iniciou com 12:07:43.394744\n",
            "Treinamento: batch 1704 terminou com 12:07:43.538088\n",
            "1705/1875 [==========================>...] - ETA: 25s - loss: 1.0211 - accuracy: 0.6029Treinamento: batch 1705 iniciou com 12:07:43.539783\n",
            "Treinamento: batch 1705 terminou com 12:07:43.686849\n",
            "1706/1875 [==========================>...] - ETA: 25s - loss: 1.0208 - accuracy: 0.6030Treinamento: batch 1706 iniciou com 12:07:43.688461\n",
            "Treinamento: batch 1706 terminou com 12:07:43.833064\n",
            "1707/1875 [==========================>...] - ETA: 25s - loss: 1.0206 - accuracy: 0.6031Treinamento: batch 1707 iniciou com 12:07:43.834700\n",
            "Treinamento: batch 1707 terminou com 12:07:43.985900\n",
            "1708/1875 [==========================>...] - ETA: 25s - loss: 1.0203 - accuracy: 0.6032Treinamento: batch 1708 iniciou com 12:07:43.987962\n",
            "Treinamento: batch 1708 terminou com 12:07:44.132382\n",
            "1709/1875 [==========================>...] - ETA: 24s - loss: 1.0202 - accuracy: 0.6032Treinamento: batch 1709 iniciou com 12:07:44.134119\n",
            "Treinamento: batch 1709 terminou com 12:07:44.281279\n",
            "1710/1875 [==========================>...] - ETA: 24s - loss: 1.0200 - accuracy: 0.6033Treinamento: batch 1710 iniciou com 12:07:44.282977\n",
            "Treinamento: batch 1710 terminou com 12:07:44.438966\n",
            "1711/1875 [==========================>...] - ETA: 24s - loss: 1.0197 - accuracy: 0.6033Treinamento: batch 1711 iniciou com 12:07:44.440609\n",
            "Treinamento: batch 1711 terminou com 12:07:44.589815\n",
            "1712/1875 [==========================>...] - ETA: 24s - loss: 1.0196 - accuracy: 0.6034Treinamento: batch 1712 iniciou com 12:07:44.591443\n",
            "Treinamento: batch 1712 terminou com 12:07:44.737975\n",
            "1713/1875 [==========================>...] - ETA: 24s - loss: 1.0196 - accuracy: 0.6034Treinamento: batch 1713 iniciou com 12:07:44.739735\n",
            "Treinamento: batch 1713 terminou com 12:07:44.893757\n",
            "1714/1875 [==========================>...] - ETA: 24s - loss: 1.0195 - accuracy: 0.6035Treinamento: batch 1714 iniciou com 12:07:44.895427\n",
            "Treinamento: batch 1714 terminou com 12:07:45.040224\n",
            "1715/1875 [==========================>...] - ETA: 23s - loss: 1.0195 - accuracy: 0.6036Treinamento: batch 1715 iniciou com 12:07:45.041906\n",
            "Treinamento: batch 1715 terminou com 12:07:45.188169\n",
            "1716/1875 [==========================>...] - ETA: 23s - loss: 1.0194 - accuracy: 0.6037Treinamento: batch 1716 iniciou com 12:07:45.189894\n",
            "Treinamento: batch 1716 terminou com 12:07:45.340342\n",
            "1717/1875 [==========================>...] - ETA: 23s - loss: 1.0191 - accuracy: 0.6037Treinamento: batch 1717 iniciou com 12:07:45.341985\n",
            "Treinamento: batch 1717 terminou com 12:07:45.483254\n",
            "1718/1875 [==========================>...] - ETA: 23s - loss: 1.0190 - accuracy: 0.6038Treinamento: batch 1718 iniciou com 12:07:45.485017\n",
            "Treinamento: batch 1718 terminou com 12:07:45.636776\n",
            "1719/1875 [==========================>...] - ETA: 23s - loss: 1.0188 - accuracy: 0.6039Treinamento: batch 1719 iniciou com 12:07:45.638998\n",
            "Treinamento: batch 1719 terminou com 12:07:45.783753\n",
            "1720/1875 [==========================>...] - ETA: 23s - loss: 1.0188 - accuracy: 0.6039Treinamento: batch 1720 iniciou com 12:07:45.785445\n",
            "Treinamento: batch 1720 terminou com 12:07:45.944048\n",
            "1721/1875 [==========================>...] - ETA: 23s - loss: 1.0187 - accuracy: 0.6039Treinamento: batch 1721 iniciou com 12:07:45.945866\n",
            "Treinamento: batch 1721 terminou com 12:07:46.089692\n",
            "1722/1875 [==========================>...] - ETA: 22s - loss: 1.0186 - accuracy: 0.6039Treinamento: batch 1722 iniciou com 12:07:46.091341\n",
            "Treinamento: batch 1722 terminou com 12:07:46.240022\n",
            "1723/1875 [==========================>...] - ETA: 22s - loss: 1.0184 - accuracy: 0.6040Treinamento: batch 1723 iniciou com 12:07:46.241687\n",
            "Treinamento: batch 1723 terminou com 12:07:46.385722\n",
            "1724/1875 [==========================>...] - ETA: 22s - loss: 1.0182 - accuracy: 0.6041Treinamento: batch 1724 iniciou com 12:07:46.387364\n",
            "Treinamento: batch 1724 terminou com 12:07:46.529881\n",
            "1725/1875 [==========================>...] - ETA: 22s - loss: 1.0180 - accuracy: 0.6042Treinamento: batch 1725 iniciou com 12:07:46.531452\n",
            "Treinamento: batch 1725 terminou com 12:07:46.672557\n",
            "1726/1875 [==========================>...] - ETA: 22s - loss: 1.0178 - accuracy: 0.6042Treinamento: batch 1726 iniciou com 12:07:46.674107\n",
            "Treinamento: batch 1726 terminou com 12:07:46.815835\n",
            "1727/1875 [==========================>...] - ETA: 22s - loss: 1.0176 - accuracy: 0.6043Treinamento: batch 1727 iniciou com 12:07:46.817445\n",
            "Treinamento: batch 1727 terminou com 12:07:46.973108\n",
            "1728/1875 [==========================>...] - ETA: 22s - loss: 1.0174 - accuracy: 0.6045Treinamento: batch 1728 iniciou com 12:07:46.974788\n",
            "Treinamento: batch 1728 terminou com 12:07:47.123759\n",
            "1729/1875 [==========================>...] - ETA: 21s - loss: 1.0171 - accuracy: 0.6046Treinamento: batch 1729 iniciou com 12:07:47.126079\n",
            "Treinamento: batch 1729 terminou com 12:07:47.269765\n",
            "1730/1875 [==========================>...] - ETA: 21s - loss: 1.0169 - accuracy: 0.6047Treinamento: batch 1730 iniciou com 12:07:47.271420\n",
            "Treinamento: batch 1730 terminou com 12:07:47.419151\n",
            "1731/1875 [==========================>...] - ETA: 21s - loss: 1.0168 - accuracy: 0.6047Treinamento: batch 1731 iniciou com 12:07:47.420779\n",
            "Treinamento: batch 1731 terminou com 12:07:47.562114\n",
            "1732/1875 [==========================>...] - ETA: 21s - loss: 1.0167 - accuracy: 0.6048Treinamento: batch 1732 iniciou com 12:07:47.563730\n",
            "Treinamento: batch 1732 terminou com 12:07:47.703550\n",
            "1733/1875 [==========================>...] - ETA: 21s - loss: 1.0165 - accuracy: 0.6049Treinamento: batch 1733 iniciou com 12:07:47.705143\n",
            "Treinamento: batch 1733 terminou com 12:07:47.851764\n",
            "1734/1875 [==========================>...] - ETA: 21s - loss: 1.0163 - accuracy: 0.6049Treinamento: batch 1734 iniciou com 12:07:47.853367\n",
            "Treinamento: batch 1734 terminou com 12:07:48.002511\n",
            "1735/1875 [==========================>...] - ETA: 20s - loss: 1.0161 - accuracy: 0.6050Treinamento: batch 1735 iniciou com 12:07:48.004228\n",
            "Treinamento: batch 1735 terminou com 12:07:48.149357\n",
            "1736/1875 [==========================>...] - ETA: 20s - loss: 1.0158 - accuracy: 0.6051Treinamento: batch 1736 iniciou com 12:07:48.151055\n",
            "Treinamento: batch 1736 terminou com 12:07:48.296018\n",
            "1737/1875 [==========================>...] - ETA: 20s - loss: 1.0156 - accuracy: 0.6052Treinamento: batch 1737 iniciou com 12:07:48.297681\n",
            "Treinamento: batch 1737 terminou com 12:07:48.439282\n",
            "1738/1875 [==========================>...] - ETA: 20s - loss: 1.0154 - accuracy: 0.6053Treinamento: batch 1738 iniciou com 12:07:48.440906\n",
            "Treinamento: batch 1738 terminou com 12:07:48.579934\n",
            "1739/1875 [==========================>...] - ETA: 20s - loss: 1.0152 - accuracy: 0.6054Treinamento: batch 1739 iniciou com 12:07:48.581879\n",
            "Treinamento: batch 1739 terminou com 12:07:48.734313\n",
            "1740/1875 [==========================>...] - ETA: 20s - loss: 1.0150 - accuracy: 0.6054Treinamento: batch 1740 iniciou com 12:07:48.736036\n",
            "Treinamento: batch 1740 terminou com 12:07:48.883209\n",
            "1741/1875 [==========================>...] - ETA: 20s - loss: 1.0150 - accuracy: 0.6055Treinamento: batch 1741 iniciou com 12:07:48.885285\n",
            "Treinamento: batch 1741 terminou com 12:07:49.035146\n",
            "1742/1875 [==========================>...] - ETA: 19s - loss: 1.0150 - accuracy: 0.6055Treinamento: batch 1742 iniciou com 12:07:49.036877\n",
            "Treinamento: batch 1742 terminou com 12:07:49.183440\n",
            "1743/1875 [==========================>...] - ETA: 19s - loss: 1.0147 - accuracy: 0.6056Treinamento: batch 1743 iniciou com 12:07:49.185131\n",
            "Treinamento: batch 1743 terminou com 12:07:49.331449\n",
            "1744/1875 [==========================>...] - ETA: 19s - loss: 1.0146 - accuracy: 0.6056Treinamento: batch 1744 iniciou com 12:07:49.333773\n",
            "Treinamento: batch 1744 terminou com 12:07:49.483820\n",
            "1745/1875 [==========================>...] - ETA: 19s - loss: 1.0145 - accuracy: 0.6057Treinamento: batch 1745 iniciou com 12:07:49.486487\n",
            "Treinamento: batch 1745 terminou com 12:07:49.655601\n",
            "1746/1875 [==========================>...] - ETA: 19s - loss: 1.0144 - accuracy: 0.6058Treinamento: batch 1746 iniciou com 12:07:49.657291\n",
            "Treinamento: batch 1746 terminou com 12:07:49.797994\n",
            "1747/1875 [==========================>...] - ETA: 19s - loss: 1.0143 - accuracy: 0.6058Treinamento: batch 1747 iniciou com 12:07:49.799621\n",
            "Treinamento: batch 1747 terminou com 12:07:49.941411\n",
            "1748/1875 [==========================>...] - ETA: 19s - loss: 1.0141 - accuracy: 0.6059Treinamento: batch 1748 iniciou com 12:07:49.943015\n",
            "Treinamento: batch 1748 terminou com 12:07:50.099755\n",
            "1749/1875 [==========================>...] - ETA: 18s - loss: 1.0138 - accuracy: 0.6060Treinamento: batch 1749 iniciou com 12:07:50.101416\n",
            "Treinamento: batch 1749 terminou com 12:07:50.242212\n",
            "1750/1875 [===========================>..] - ETA: 18s - loss: 1.0137 - accuracy: 0.6061Treinamento: batch 1750 iniciou com 12:07:50.243867\n",
            "Treinamento: batch 1750 terminou com 12:07:50.387984\n",
            "1751/1875 [===========================>..] - ETA: 18s - loss: 1.0135 - accuracy: 0.6061Treinamento: batch 1751 iniciou com 12:07:50.389731\n",
            "Treinamento: batch 1751 terminou com 12:07:50.535264\n",
            "1752/1875 [===========================>..] - ETA: 18s - loss: 1.0133 - accuracy: 0.6062Treinamento: batch 1752 iniciou com 12:07:50.536916\n",
            "Treinamento: batch 1752 terminou com 12:07:50.678139\n",
            "1753/1875 [===========================>..] - ETA: 18s - loss: 1.0132 - accuracy: 0.6062Treinamento: batch 1753 iniciou com 12:07:50.679835\n",
            "Treinamento: batch 1753 terminou com 12:07:50.822329\n",
            "1754/1875 [===========================>..] - ETA: 18s - loss: 1.0129 - accuracy: 0.6064Treinamento: batch 1754 iniciou com 12:07:50.823947\n",
            "Treinamento: batch 1754 terminou com 12:07:50.962931\n",
            "1755/1875 [===========================>..] - ETA: 17s - loss: 1.0128 - accuracy: 0.6065Treinamento: batch 1755 iniciou com 12:07:50.964512\n",
            "Treinamento: batch 1755 terminou com 12:07:51.125425\n",
            "1756/1875 [===========================>..] - ETA: 17s - loss: 1.0128 - accuracy: 0.6065Treinamento: batch 1756 iniciou com 12:07:51.127042\n",
            "Treinamento: batch 1756 terminou com 12:07:51.274872\n",
            "1757/1875 [===========================>..] - ETA: 17s - loss: 1.0129 - accuracy: 0.6065Treinamento: batch 1757 iniciou com 12:07:51.276612\n",
            "Treinamento: batch 1757 terminou com 12:07:51.419416\n",
            "1758/1875 [===========================>..] - ETA: 17s - loss: 1.0125 - accuracy: 0.6066Treinamento: batch 1758 iniciou com 12:07:51.421088\n",
            "Treinamento: batch 1758 terminou com 12:07:51.564200\n",
            "1759/1875 [===========================>..] - ETA: 17s - loss: 1.0123 - accuracy: 0.6067Treinamento: batch 1759 iniciou com 12:07:51.565806\n",
            "Treinamento: batch 1759 terminou com 12:07:51.721035\n",
            "1760/1875 [===========================>..] - ETA: 17s - loss: 1.0122 - accuracy: 0.6067Treinamento: batch 1760 iniciou com 12:07:51.723516\n",
            "Treinamento: batch 1760 terminou com 12:07:51.866179\n",
            "1761/1875 [===========================>..] - ETA: 17s - loss: 1.0120 - accuracy: 0.6068Treinamento: batch 1761 iniciou com 12:07:51.870340\n",
            "Treinamento: batch 1761 terminou com 12:07:52.013144\n",
            "1762/1875 [===========================>..] - ETA: 16s - loss: 1.0118 - accuracy: 0.6069Treinamento: batch 1762 iniciou com 12:07:52.014920\n",
            "Treinamento: batch 1762 terminou com 12:07:52.170577\n",
            "1763/1875 [===========================>..] - ETA: 16s - loss: 1.0115 - accuracy: 0.6070Treinamento: batch 1763 iniciou com 12:07:52.172246\n",
            "Treinamento: batch 1763 terminou com 12:07:52.320377\n",
            "1764/1875 [===========================>..] - ETA: 16s - loss: 1.0115 - accuracy: 0.6070Treinamento: batch 1764 iniciou com 12:07:52.322520\n",
            "Treinamento: batch 1764 terminou com 12:07:52.476156\n",
            "1765/1875 [===========================>..] - ETA: 16s - loss: 1.0113 - accuracy: 0.6071Treinamento: batch 1765 iniciou com 12:07:52.477839\n",
            "Treinamento: batch 1765 terminou com 12:07:52.624247\n",
            "1766/1875 [===========================>..] - ETA: 16s - loss: 1.0113 - accuracy: 0.6072Treinamento: batch 1766 iniciou com 12:07:52.626674\n",
            "Treinamento: batch 1766 terminou com 12:07:52.766838\n",
            "1767/1875 [===========================>..] - ETA: 16s - loss: 1.0111 - accuracy: 0.6072Treinamento: batch 1767 iniciou com 12:07:52.768452\n",
            "Treinamento: batch 1767 terminou com 12:07:52.914200\n",
            "1768/1875 [===========================>..] - ETA: 16s - loss: 1.0110 - accuracy: 0.6073Treinamento: batch 1768 iniciou com 12:07:52.915767\n",
            "Treinamento: batch 1768 terminou com 12:07:53.072999\n",
            "1769/1875 [===========================>..] - ETA: 15s - loss: 1.0106 - accuracy: 0.6074Treinamento: batch 1769 iniciou com 12:07:53.074677\n",
            "Treinamento: batch 1769 terminou com 12:07:53.216153\n",
            "1770/1875 [===========================>..] - ETA: 15s - loss: 1.0105 - accuracy: 0.6075Treinamento: batch 1770 iniciou com 12:07:53.218961\n",
            "Treinamento: batch 1770 terminou com 12:07:53.366672\n",
            "1771/1875 [===========================>..] - ETA: 15s - loss: 1.0104 - accuracy: 0.6075Treinamento: batch 1771 iniciou com 12:07:53.368299\n",
            "Treinamento: batch 1771 terminou com 12:07:53.511703\n",
            "1772/1875 [===========================>..] - ETA: 15s - loss: 1.0102 - accuracy: 0.6076Treinamento: batch 1772 iniciou com 12:07:53.513314\n",
            "Treinamento: batch 1772 terminou com 12:07:53.655377\n",
            "1773/1875 [===========================>..] - ETA: 15s - loss: 1.0099 - accuracy: 0.6078Treinamento: batch 1773 iniciou com 12:07:53.657283\n",
            "Treinamento: batch 1773 terminou com 12:07:53.808729\n",
            "1774/1875 [===========================>..] - ETA: 15s - loss: 1.0098 - accuracy: 0.6078Treinamento: batch 1774 iniciou com 12:07:53.810318\n",
            "Treinamento: batch 1774 terminou com 12:07:53.954612\n",
            "1775/1875 [===========================>..] - ETA: 14s - loss: 1.0096 - accuracy: 0.6079Treinamento: batch 1775 iniciou com 12:07:53.956290\n",
            "Treinamento: batch 1775 terminou com 12:07:54.109282\n",
            "1776/1875 [===========================>..] - ETA: 14s - loss: 1.0094 - accuracy: 0.6079Treinamento: batch 1776 iniciou com 12:07:54.111315\n",
            "Treinamento: batch 1776 terminou com 12:07:54.257945\n",
            "1777/1875 [===========================>..] - ETA: 14s - loss: 1.0093 - accuracy: 0.6080Treinamento: batch 1777 iniciou com 12:07:54.259687\n",
            "Treinamento: batch 1777 terminou com 12:07:54.399238\n",
            "1778/1875 [===========================>..] - ETA: 14s - loss: 1.0092 - accuracy: 0.6080Treinamento: batch 1778 iniciou com 12:07:54.400890\n",
            "Treinamento: batch 1778 terminou com 12:07:54.542965\n",
            "1779/1875 [===========================>..] - ETA: 14s - loss: 1.0091 - accuracy: 0.6080Treinamento: batch 1779 iniciou com 12:07:54.544587\n",
            "Treinamento: batch 1779 terminou com 12:07:54.696348\n",
            "1780/1875 [===========================>..] - ETA: 14s - loss: 1.0089 - accuracy: 0.6081Treinamento: batch 1780 iniciou com 12:07:54.698016\n",
            "Treinamento: batch 1780 terminou com 12:07:54.853432\n",
            "1781/1875 [===========================>..] - ETA: 14s - loss: 1.0087 - accuracy: 0.6081Treinamento: batch 1781 iniciou com 12:07:54.855407\n",
            "Treinamento: batch 1781 terminou com 12:07:54.996172\n",
            "1782/1875 [===========================>..] - ETA: 13s - loss: 1.0085 - accuracy: 0.6083Treinamento: batch 1782 iniciou com 12:07:54.998000\n",
            "Treinamento: batch 1782 terminou com 12:07:55.153939\n",
            "1783/1875 [===========================>..] - ETA: 13s - loss: 1.0082 - accuracy: 0.6084Treinamento: batch 1783 iniciou com 12:07:55.155565\n",
            "Treinamento: batch 1783 terminou com 12:07:55.303177\n",
            "1784/1875 [===========================>..] - ETA: 13s - loss: 1.0081 - accuracy: 0.6084Treinamento: batch 1784 iniciou com 12:07:55.304835\n",
            "Treinamento: batch 1784 terminou com 12:07:55.447106\n",
            "1785/1875 [===========================>..] - ETA: 13s - loss: 1.0079 - accuracy: 0.6085Treinamento: batch 1785 iniciou com 12:07:55.449048\n",
            "Treinamento: batch 1785 terminou com 12:07:55.597257\n",
            "1786/1875 [===========================>..] - ETA: 13s - loss: 1.0077 - accuracy: 0.6085Treinamento: batch 1786 iniciou com 12:07:55.598890\n",
            "Treinamento: batch 1786 terminou com 12:07:55.740560\n",
            "1787/1875 [===========================>..] - ETA: 13s - loss: 1.0075 - accuracy: 0.6085Treinamento: batch 1787 iniciou com 12:07:55.742137\n",
            "Treinamento: batch 1787 terminou com 12:07:55.887741\n",
            "1788/1875 [===========================>..] - ETA: 13s - loss: 1.0075 - accuracy: 0.6086Treinamento: batch 1788 iniciou com 12:07:55.889366\n",
            "Treinamento: batch 1788 terminou com 12:07:56.044032\n",
            "1789/1875 [===========================>..] - ETA: 12s - loss: 1.0074 - accuracy: 0.6087Treinamento: batch 1789 iniciou com 12:07:56.046146\n",
            "Treinamento: batch 1789 terminou com 12:07:56.196242\n",
            "1790/1875 [===========================>..] - ETA: 12s - loss: 1.0073 - accuracy: 0.6087Treinamento: batch 1790 iniciou com 12:07:56.198126\n",
            "Treinamento: batch 1790 terminou com 12:07:56.345587\n",
            "1791/1875 [===========================>..] - ETA: 12s - loss: 1.0072 - accuracy: 0.6087Treinamento: batch 1791 iniciou com 12:07:56.347587\n",
            "Treinamento: batch 1791 terminou com 12:07:56.489725\n",
            "1792/1875 [===========================>..] - ETA: 12s - loss: 1.0072 - accuracy: 0.6088Treinamento: batch 1792 iniciou com 12:07:56.491387\n",
            "Treinamento: batch 1792 terminou com 12:07:56.631514\n",
            "1793/1875 [===========================>..] - ETA: 12s - loss: 1.0070 - accuracy: 0.6088Treinamento: batch 1793 iniciou com 12:07:56.633035\n",
            "Treinamento: batch 1793 terminou com 12:07:56.775745\n",
            "1794/1875 [===========================>..] - ETA: 12s - loss: 1.0068 - accuracy: 0.6090Treinamento: batch 1794 iniciou com 12:07:56.777341\n",
            "Treinamento: batch 1794 terminou com 12:07:56.921724\n",
            "1795/1875 [===========================>..] - ETA: 11s - loss: 1.0067 - accuracy: 0.6091Treinamento: batch 1795 iniciou com 12:07:56.923421\n",
            "Treinamento: batch 1795 terminou com 12:07:57.071583\n",
            "1796/1875 [===========================>..] - ETA: 11s - loss: 1.0065 - accuracy: 0.6092Treinamento: batch 1796 iniciou com 12:07:57.073660\n",
            "Treinamento: batch 1796 terminou com 12:07:57.231578\n",
            "1797/1875 [===========================>..] - ETA: 11s - loss: 1.0063 - accuracy: 0.6093Treinamento: batch 1797 iniciou com 12:07:57.234252\n",
            "Treinamento: batch 1797 terminou com 12:07:57.377604\n",
            "1798/1875 [===========================>..] - ETA: 11s - loss: 1.0060 - accuracy: 0.6093Treinamento: batch 1798 iniciou com 12:07:57.379252\n",
            "Treinamento: batch 1798 terminou com 12:07:57.528180\n",
            "1799/1875 [===========================>..] - ETA: 11s - loss: 1.0059 - accuracy: 0.6094Treinamento: batch 1799 iniciou com 12:07:57.529983\n",
            "Treinamento: batch 1799 terminou com 12:07:57.673529\n",
            "1800/1875 [===========================>..] - ETA: 11s - loss: 1.0056 - accuracy: 0.6095Treinamento: batch 1800 iniciou com 12:07:57.675339\n",
            "Treinamento: batch 1800 terminou com 12:07:57.818318\n",
            "1801/1875 [===========================>..] - ETA: 11s - loss: 1.0054 - accuracy: 0.6096Treinamento: batch 1801 iniciou com 12:07:57.820087\n",
            "Treinamento: batch 1801 terminou com 12:07:57.966013\n",
            "1802/1875 [===========================>..] - ETA: 10s - loss: 1.0051 - accuracy: 0.6097Treinamento: batch 1802 iniciou com 12:07:57.967758\n",
            "Treinamento: batch 1802 terminou com 12:07:58.118510\n",
            "1803/1875 [===========================>..] - ETA: 10s - loss: 1.0048 - accuracy: 0.6099Treinamento: batch 1803 iniciou com 12:07:58.120136\n",
            "Treinamento: batch 1803 terminou com 12:07:58.275001\n",
            "1804/1875 [===========================>..] - ETA: 10s - loss: 1.0046 - accuracy: 0.6099Treinamento: batch 1804 iniciou com 12:07:58.276810\n",
            "Treinamento: batch 1804 terminou com 12:07:58.416438\n",
            "1805/1875 [===========================>..] - ETA: 10s - loss: 1.0044 - accuracy: 0.6101Treinamento: batch 1805 iniciou com 12:07:58.418695\n",
            "Treinamento: batch 1805 terminou com 12:07:58.569451\n",
            "1806/1875 [===========================>..] - ETA: 10s - loss: 1.0042 - accuracy: 0.6101Treinamento: batch 1806 iniciou com 12:07:58.571404\n",
            "Treinamento: batch 1806 terminou com 12:07:58.713864\n",
            "1807/1875 [===========================>..] - ETA: 10s - loss: 1.0040 - accuracy: 0.6102Treinamento: batch 1807 iniciou com 12:07:58.718742\n",
            "Treinamento: batch 1807 terminou com 12:07:58.857617\n",
            "1808/1875 [===========================>..] - ETA: 10s - loss: 1.0040 - accuracy: 0.6102Treinamento: batch 1808 iniciou com 12:07:58.859659\n",
            "Treinamento: batch 1808 terminou com 12:07:59.002064\n",
            "1809/1875 [===========================>..] - ETA: 9s - loss: 1.0039 - accuracy: 0.6102 Treinamento: batch 1809 iniciou com 12:07:59.003810\n",
            "Treinamento: batch 1809 terminou com 12:07:59.145750\n",
            "1810/1875 [===========================>..] - ETA: 9s - loss: 1.0037 - accuracy: 0.6103Treinamento: batch 1810 iniciou com 12:07:59.147696\n",
            "Treinamento: batch 1810 terminou com 12:07:59.304159\n",
            "1811/1875 [===========================>..] - ETA: 9s - loss: 1.0034 - accuracy: 0.6104Treinamento: batch 1811 iniciou com 12:07:59.305773\n",
            "Treinamento: batch 1811 terminou com 12:07:59.447597\n",
            "1812/1875 [===========================>..] - ETA: 9s - loss: 1.0031 - accuracy: 0.6105Treinamento: batch 1812 iniciou com 12:07:59.449299\n",
            "Treinamento: batch 1812 terminou com 12:07:59.591740\n",
            "1813/1875 [============================>.] - ETA: 9s - loss: 1.0029 - accuracy: 0.6106Treinamento: batch 1813 iniciou com 12:07:59.593436\n",
            "Treinamento: batch 1813 terminou com 12:07:59.735972\n",
            "1814/1875 [============================>.] - ETA: 9s - loss: 1.0026 - accuracy: 0.6107Treinamento: batch 1814 iniciou com 12:07:59.737702\n",
            "Treinamento: batch 1814 terminou com 12:07:59.913528\n",
            "1815/1875 [============================>.] - ETA: 8s - loss: 1.0025 - accuracy: 0.6107Treinamento: batch 1815 iniciou com 12:07:59.915326\n",
            "Treinamento: batch 1815 terminou com 12:08:00.059091\n",
            "1816/1875 [============================>.] - ETA: 8s - loss: 1.0023 - accuracy: 0.6109Treinamento: batch 1816 iniciou com 12:08:00.060784\n",
            "Treinamento: batch 1816 terminou com 12:08:00.216490\n",
            "1817/1875 [============================>.] - ETA: 8s - loss: 1.0023 - accuracy: 0.6109Treinamento: batch 1817 iniciou com 12:08:00.218471\n",
            "Treinamento: batch 1817 terminou com 12:08:00.362910\n",
            "1818/1875 [============================>.] - ETA: 8s - loss: 1.0021 - accuracy: 0.6110Treinamento: batch 1818 iniciou com 12:08:00.364729\n",
            "Treinamento: batch 1818 terminou com 12:08:00.509076\n",
            "1819/1875 [============================>.] - ETA: 8s - loss: 1.0020 - accuracy: 0.6110Treinamento: batch 1819 iniciou com 12:08:00.511371\n",
            "Treinamento: batch 1819 terminou com 12:08:00.657672\n",
            "1820/1875 [============================>.] - ETA: 8s - loss: 1.0017 - accuracy: 0.6111Treinamento: batch 1820 iniciou com 12:08:00.659278\n",
            "Treinamento: batch 1820 terminou com 12:08:00.798309\n",
            "1821/1875 [============================>.] - ETA: 8s - loss: 1.0015 - accuracy: 0.6112Treinamento: batch 1821 iniciou com 12:08:00.799926\n",
            "Treinamento: batch 1821 terminou com 12:08:00.944276\n",
            "1822/1875 [============================>.] - ETA: 7s - loss: 1.0011 - accuracy: 0.6114Treinamento: batch 1822 iniciou com 12:08:00.945889\n",
            "Treinamento: batch 1822 terminou com 12:08:01.090201\n",
            "1823/1875 [============================>.] - ETA: 7s - loss: 1.0010 - accuracy: 0.6115Treinamento: batch 1823 iniciou com 12:08:01.091868\n",
            "Treinamento: batch 1823 terminou com 12:08:01.258124\n",
            "1824/1875 [============================>.] - ETA: 7s - loss: 1.0007 - accuracy: 0.6116Treinamento: batch 1824 iniciou com 12:08:01.260330\n",
            "Treinamento: batch 1824 terminou com 12:08:01.401873\n",
            "1825/1875 [============================>.] - ETA: 7s - loss: 1.0004 - accuracy: 0.6117Treinamento: batch 1825 iniciou com 12:08:01.405554\n",
            "Treinamento: batch 1825 terminou com 12:08:01.549404\n",
            "1826/1875 [============================>.] - ETA: 7s - loss: 1.0002 - accuracy: 0.6118Treinamento: batch 1826 iniciou com 12:08:01.551172\n",
            "Treinamento: batch 1826 terminou com 12:08:01.691399\n",
            "1827/1875 [============================>.] - ETA: 7s - loss: 1.0001 - accuracy: 0.6119Treinamento: batch 1827 iniciou com 12:08:01.693059\n",
            "Treinamento: batch 1827 terminou com 12:08:01.838007\n",
            "1828/1875 [============================>.] - ETA: 7s - loss: 1.0000 - accuracy: 0.6120Treinamento: batch 1828 iniciou com 12:08:01.839553\n",
            "Treinamento: batch 1828 terminou com 12:08:01.982758\n",
            "1829/1875 [============================>.] - ETA: 6s - loss: 0.9998 - accuracy: 0.6120Treinamento: batch 1829 iniciou com 12:08:01.984325\n",
            "Treinamento: batch 1829 terminou com 12:08:02.127149\n",
            "1830/1875 [============================>.] - ETA: 6s - loss: 0.9996 - accuracy: 0.6121Treinamento: batch 1830 iniciou com 12:08:02.128872\n",
            "Treinamento: batch 1830 terminou com 12:08:02.289784\n",
            "1831/1875 [============================>.] - ETA: 6s - loss: 0.9994 - accuracy: 0.6122Treinamento: batch 1831 iniciou com 12:08:02.291481\n",
            "Treinamento: batch 1831 terminou com 12:08:02.439270\n",
            "1832/1875 [============================>.] - ETA: 6s - loss: 0.9991 - accuracy: 0.6123Treinamento: batch 1832 iniciou com 12:08:02.441170\n",
            "Treinamento: batch 1832 terminou com 12:08:02.584113\n",
            "1833/1875 [============================>.] - ETA: 6s - loss: 0.9990 - accuracy: 0.6124Treinamento: batch 1833 iniciou com 12:08:02.585847\n",
            "Treinamento: batch 1833 terminou com 12:08:02.729471\n",
            "1834/1875 [============================>.] - ETA: 6s - loss: 0.9989 - accuracy: 0.6124Treinamento: batch 1834 iniciou com 12:08:02.731140\n",
            "Treinamento: batch 1834 terminou com 12:08:02.874497\n",
            "1835/1875 [============================>.] - ETA: 5s - loss: 0.9988 - accuracy: 0.6125Treinamento: batch 1835 iniciou com 12:08:02.877025\n",
            "Treinamento: batch 1835 terminou com 12:08:03.022696\n",
            "1836/1875 [============================>.] - ETA: 5s - loss: 0.9987 - accuracy: 0.6126Treinamento: batch 1836 iniciou com 12:08:03.024422\n",
            "Treinamento: batch 1836 terminou com 12:08:03.174729\n",
            "1837/1875 [============================>.] - ETA: 5s - loss: 0.9986 - accuracy: 0.6126Treinamento: batch 1837 iniciou com 12:08:03.176366\n",
            "Treinamento: batch 1837 terminou com 12:08:03.328805\n",
            "1838/1875 [============================>.] - ETA: 5s - loss: 0.9985 - accuracy: 0.6127Treinamento: batch 1838 iniciou com 12:08:03.330429\n",
            "Treinamento: batch 1838 terminou com 12:08:03.473481\n",
            "1839/1875 [============================>.] - ETA: 5s - loss: 0.9983 - accuracy: 0.6127Treinamento: batch 1839 iniciou com 12:08:03.475469\n",
            "Treinamento: batch 1839 terminou com 12:08:03.616128\n",
            "1840/1875 [============================>.] - ETA: 5s - loss: 0.9980 - accuracy: 0.6128Treinamento: batch 1840 iniciou com 12:08:03.617976\n",
            "Treinamento: batch 1840 terminou com 12:08:03.762087\n",
            "1841/1875 [============================>.] - ETA: 5s - loss: 0.9978 - accuracy: 0.6130Treinamento: batch 1841 iniciou com 12:08:03.763767\n",
            "Treinamento: batch 1841 terminou com 12:08:03.912524\n",
            "1842/1875 [============================>.] - ETA: 4s - loss: 0.9978 - accuracy: 0.6130Treinamento: batch 1842 iniciou com 12:08:03.914073\n",
            "Treinamento: batch 1842 terminou com 12:08:04.059004\n",
            "1843/1875 [============================>.] - ETA: 4s - loss: 0.9977 - accuracy: 0.6130Treinamento: batch 1843 iniciou com 12:08:04.060665\n",
            "Treinamento: batch 1843 terminou com 12:08:04.209443\n",
            "1844/1875 [============================>.] - ETA: 4s - loss: 0.9975 - accuracy: 0.6131Treinamento: batch 1844 iniciou com 12:08:04.211072\n",
            "Treinamento: batch 1844 terminou com 12:08:04.366563\n",
            "1845/1875 [============================>.] - ETA: 4s - loss: 0.9974 - accuracy: 0.6131Treinamento: batch 1845 iniciou com 12:08:04.368170\n",
            "Treinamento: batch 1845 terminou com 12:08:04.511986\n",
            "1846/1875 [============================>.] - ETA: 4s - loss: 0.9972 - accuracy: 0.6132Treinamento: batch 1846 iniciou com 12:08:04.513676\n",
            "Treinamento: batch 1846 terminou com 12:08:04.659034\n",
            "1847/1875 [============================>.] - ETA: 4s - loss: 0.9970 - accuracy: 0.6133Treinamento: batch 1847 iniciou com 12:08:04.660647\n",
            "Treinamento: batch 1847 terminou com 12:08:04.806592\n",
            "1848/1875 [============================>.] - ETA: 4s - loss: 0.9970 - accuracy: 0.6134Treinamento: batch 1848 iniciou com 12:08:04.808205\n",
            "Treinamento: batch 1848 terminou com 12:08:04.960691\n",
            "1849/1875 [============================>.] - ETA: 3s - loss: 0.9967 - accuracy: 0.6135Treinamento: batch 1849 iniciou com 12:08:04.962840\n",
            "Treinamento: batch 1849 terminou com 12:08:05.110477\n",
            "1850/1875 [============================>.] - ETA: 3s - loss: 0.9969 - accuracy: 0.6135Treinamento: batch 1850 iniciou com 12:08:05.112238\n",
            "Treinamento: batch 1850 terminou com 12:08:05.262011\n",
            "1851/1875 [============================>.] - ETA: 3s - loss: 0.9968 - accuracy: 0.6136Treinamento: batch 1851 iniciou com 12:08:05.264462\n",
            "Treinamento: batch 1851 terminou com 12:08:05.420250\n",
            "1852/1875 [============================>.] - ETA: 3s - loss: 0.9968 - accuracy: 0.6135Treinamento: batch 1852 iniciou com 12:08:05.421879\n",
            "Treinamento: batch 1852 terminou com 12:08:05.561481\n",
            "1853/1875 [============================>.] - ETA: 3s - loss: 0.9966 - accuracy: 0.6136Treinamento: batch 1853 iniciou com 12:08:05.563147\n",
            "Treinamento: batch 1853 terminou com 12:08:05.703582\n",
            "1854/1875 [============================>.] - ETA: 3s - loss: 0.9965 - accuracy: 0.6136Treinamento: batch 1854 iniciou com 12:08:05.705460\n",
            "Treinamento: batch 1854 terminou com 12:08:05.847492\n",
            "1855/1875 [============================>.] - ETA: 2s - loss: 0.9962 - accuracy: 0.6137Treinamento: batch 1855 iniciou com 12:08:05.849164\n",
            "Treinamento: batch 1855 terminou com 12:08:05.990744\n",
            "1856/1875 [============================>.] - ETA: 2s - loss: 0.9961 - accuracy: 0.6137Treinamento: batch 1856 iniciou com 12:08:05.992429\n",
            "Treinamento: batch 1856 terminou com 12:08:06.136538\n",
            "1857/1875 [============================>.] - ETA: 2s - loss: 0.9958 - accuracy: 0.6138Treinamento: batch 1857 iniciou com 12:08:06.138198\n",
            "Treinamento: batch 1857 terminou com 12:08:06.288490\n",
            "1858/1875 [============================>.] - ETA: 2s - loss: 0.9958 - accuracy: 0.6138Treinamento: batch 1858 iniciou com 12:08:06.290128\n",
            "Treinamento: batch 1858 terminou com 12:08:06.445837\n",
            "1859/1875 [============================>.] - ETA: 2s - loss: 0.9956 - accuracy: 0.6139Treinamento: batch 1859 iniciou com 12:08:06.447583\n",
            "Treinamento: batch 1859 terminou com 12:08:06.590062\n",
            "1860/1875 [============================>.] - ETA: 2s - loss: 0.9956 - accuracy: 0.6139Treinamento: batch 1860 iniciou com 12:08:06.591731\n",
            "Treinamento: batch 1860 terminou com 12:08:06.736456\n",
            "1861/1875 [============================>.] - ETA: 2s - loss: 0.9954 - accuracy: 0.6140Treinamento: batch 1861 iniciou com 12:08:06.738214\n",
            "Treinamento: batch 1861 terminou com 12:08:06.883099\n",
            "1862/1875 [============================>.] - ETA: 1s - loss: 0.9952 - accuracy: 0.6141Treinamento: batch 1862 iniciou com 12:08:06.884676\n",
            "Treinamento: batch 1862 terminou com 12:08:07.029275\n",
            "1863/1875 [============================>.] - ETA: 1s - loss: 0.9952 - accuracy: 0.6141Treinamento: batch 1863 iniciou com 12:08:07.031054\n",
            "Treinamento: batch 1863 terminou com 12:08:07.176109\n",
            "1864/1875 [============================>.] - ETA: 1s - loss: 0.9952 - accuracy: 0.6142Treinamento: batch 1864 iniciou com 12:08:07.177812\n",
            "Treinamento: batch 1864 terminou com 12:08:07.321458\n",
            "1865/1875 [============================>.] - ETA: 1s - loss: 0.9952 - accuracy: 0.6142Treinamento: batch 1865 iniciou com 12:08:07.323454\n",
            "Treinamento: batch 1865 terminou com 12:08:07.477800\n",
            "1866/1875 [============================>.] - ETA: 1s - loss: 0.9950 - accuracy: 0.6142Treinamento: batch 1866 iniciou com 12:08:07.479307\n",
            "Treinamento: batch 1866 terminou com 12:08:07.622673\n",
            "1867/1875 [============================>.] - ETA: 1s - loss: 0.9948 - accuracy: 0.6143Treinamento: batch 1867 iniciou com 12:08:07.624373\n",
            "Treinamento: batch 1867 terminou com 12:08:07.770765\n",
            "1868/1875 [============================>.] - ETA: 1s - loss: 0.9948 - accuracy: 0.6143Treinamento: batch 1868 iniciou com 12:08:07.772348\n",
            "Treinamento: batch 1868 terminou com 12:08:07.915304\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.9946 - accuracy: 0.6144Treinamento: batch 1869 iniciou com 12:08:07.916964\n",
            "Treinamento: batch 1869 terminou com 12:08:08.061473\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.9946 - accuracy: 0.6145Treinamento: batch 1870 iniciou com 12:08:08.063090\n",
            "Treinamento: batch 1870 terminou com 12:08:08.212581\n",
            "1871/1875 [============================>.] - ETA: 0s - loss: 0.9944 - accuracy: 0.6145Treinamento: batch 1871 iniciou com 12:08:08.214319\n",
            "Treinamento: batch 1871 terminou com 12:08:08.370230\n",
            "1872/1875 [============================>.] - ETA: 0s - loss: 0.9942 - accuracy: 0.6146Treinamento: batch 1872 iniciou com 12:08:08.375621\n",
            "Treinamento: batch 1872 terminou com 12:08:08.516749\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.9941 - accuracy: 0.6146Treinamento: batch 1873 iniciou com 12:08:08.518723\n",
            "Treinamento: batch 1873 terminou com 12:08:08.660203\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.9939 - accuracy: 0.6147Treinamento: batch 1874 iniciou com 12:08:08.661819\n",
            "Treinamento: batch 1874 terminou com 12:08:08.805915\n",
            "1875/1875 [==============================] - 290s 155ms/step - loss: 0.9936 - accuracy: 0.6148 - val_loss: 0.7129 - val_accuracy: 0.7435\n",
            "Treino finalizado no tempo de 12:08:18.299964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f43233bc3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBvLhoeT2bZk",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "### 2. EarlyStopping Callback \n",
        "\n",
        "<br>\n",
        "\n",
        "EarlyStopping é uma forma de regularização usada para evitar ajustes excessivos(Overfitting) ao treinar uma rede com um método iterativo, como descida em gradiente. \n",
        "\n",
        "Esses métodos atualizam a rede para melhor ajustá-lo aos dados de treinamento a cada iteração. Até certo ponto, isso melhora o desempenho da rede em dados fora do conjunto de treinamento. \n",
        "<br>\n",
        "\n",
        "Depois desse ponto, no entanto, melhorar o ajuste da rede aos dados de treinamento custa às custas do aumento do erro de generalização. As regras de EarlyStopping fornecem orientações sobre quantas iterações podem ser executadas antes que a rede comece a se ajustar demais. As regras de EarlyStopping foram empregadas em muitos métodos diferentes de ML, com quantidades variadas de fundamentação teórica.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "Parâmetros: \n",
        "\n",
        "* \"monitor\", você pode passar a perda ou a métrica. Geralmente, passamos por val_loss e monitoramos.\n",
        "\n",
        "* \"min_delta\" você pode passar um número inteiro nesse argumento. Em palavras simples, você está dizendo ao retorno de chamada que o modelo não está melhorando se não estiver diminuindo mais / menos que a perda / métrica.\n",
        "\n",
        "* \"pacient\", significa quantas épocas esperar. E depois disso, se não houver melhoria observada no desempenho do modelo de acordo com o valor de “delta mínimo”, pare o treinamento.\n",
        "\n",
        "* \"mode\" Por padrão, é definido como \"automático\", o que é útil quando você lida com a perda / métrica personalizada. Portanto, você pode dizer ao retorno de chamada se o modelo está melhorando quando sua perda / métrica personalizada está diminuindo e, em seguida, defina-o como \"min\" ou aumentando e defina-o como \"max\".\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vg4zKWA2YCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yAdZWMZ2d-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c04f148d-c900-4b17-e2d4-d532d32e74c7"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               mode='min',\n",
        "                               min_delta=0.001,\n",
        "                               patience=5\n",
        "                               )\n",
        "\n",
        "model = get_model()\n",
        "history = model.fit(X_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=64,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[early_stopping])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "938/938 [==============================] - 272s 290ms/step - loss: 0.7941 - accuracy: 0.6962 - val_loss: 0.4980 - val_accuracy: 0.8182\n",
            "Epoch 2/20\n",
            "938/938 [==============================] - 272s 290ms/step - loss: 0.5040 - accuracy: 0.8116 - val_loss: 0.4967 - val_accuracy: 0.8150\n",
            "Epoch 3/20\n",
            "938/938 [==============================] - 272s 290ms/step - loss: 0.4770 - accuracy: 0.8234 - val_loss: 0.4459 - val_accuracy: 0.8312\n",
            "Epoch 4/20\n",
            "938/938 [==============================] - 271s 289ms/step - loss: 0.4609 - accuracy: 0.8272 - val_loss: 0.4380 - val_accuracy: 0.8337\n",
            "Epoch 5/20\n",
            "938/938 [==============================] - 266s 284ms/step - loss: 0.4556 - accuracy: 0.8297 - val_loss: 0.4224 - val_accuracy: 0.8401\n",
            "Epoch 6/20\n",
            "938/938 [==============================] - 258s 275ms/step - loss: 0.4468 - accuracy: 0.8331 - val_loss: 0.4199 - val_accuracy: 0.8421\n",
            "Epoch 7/20\n",
            "938/938 [==============================] - 262s 280ms/step - loss: 0.4395 - accuracy: 0.8371 - val_loss: 0.4142 - val_accuracy: 0.8436\n",
            "Epoch 8/20\n",
            "938/938 [==============================] - 266s 284ms/step - loss: 0.4384 - accuracy: 0.8358 - val_loss: 0.4320 - val_accuracy: 0.8429\n",
            "Epoch 9/20\n",
            "938/938 [==============================] - 267s 285ms/step - loss: 0.4318 - accuracy: 0.8386 - val_loss: 0.4111 - val_accuracy: 0.8422\n",
            "Epoch 10/20\n",
            "938/938 [==============================] - 267s 284ms/step - loss: 0.4359 - accuracy: 0.8384 - val_loss: 0.3914 - val_accuracy: 0.8567\n",
            "Epoch 11/20\n",
            "938/938 [==============================] - 267s 285ms/step - loss: 0.4246 - accuracy: 0.8399 - val_loss: 0.4142 - val_accuracy: 0.8443\n",
            "Epoch 12/20\n",
            "938/938 [==============================] - 269s 287ms/step - loss: 0.4284 - accuracy: 0.8409 - val_loss: 0.3972 - val_accuracy: 0.8503\n",
            "Epoch 13/20\n",
            "938/938 [==============================] - 267s 285ms/step - loss: 0.4273 - accuracy: 0.8405 - val_loss: 0.3991 - val_accuracy: 0.8522\n",
            "Epoch 14/20\n",
            "938/938 [==============================] - 263s 280ms/step - loss: 0.4209 - accuracy: 0.8450 - val_loss: 0.4014 - val_accuracy: 0.8525\n",
            "Epoch 15/20\n",
            "938/938 [==============================] - 270s 288ms/step - loss: 0.4202 - accuracy: 0.8442 - val_loss: 0.4220 - val_accuracy: 0.8441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuiamRPy2hkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6250e8a-294f-4cff-ae21-b7add90ffb82"
      },
      "source": [
        "# epochs executadas \n",
        "len(history.history['loss'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AVUQUBG2i8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e40e5dcd-4806-4523-e8ce-692e7c1ba3a7"
      },
      "source": [
        "# métricas \n",
        "pred = model.predict(X_test)\n",
        "y_pred = np.argmax(pred, axis=1)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.77      1000\n",
            "           1       0.98      0.98      0.98      1000\n",
            "           2       0.83      0.64      0.72      1000\n",
            "           3       0.82      0.92      0.86      1000\n",
            "           4       0.74      0.72      0.73      1000\n",
            "           5       0.99      0.90      0.94      1000\n",
            "           6       0.54      0.76      0.63      1000\n",
            "           7       0.85      0.99      0.92      1000\n",
            "           8       0.98      0.96      0.97      1000\n",
            "           9       0.98      0.90      0.94      1000\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.86      0.84      0.85     10000\n",
            "weighted avg       0.86      0.84      0.85     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEgfyDz6_8hB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "a1a1d62c-4dde-4445-dc78-7db5f98cfc4c"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Loss Curve')\n",
        "plt.plot(history.history['loss'], color='blue')\n",
        "plt.plot(history.history['val_loss'], color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Acurácia')\n",
        "plt.plot(history.history['accuracy'], color='blue')\n",
        "plt.plot(history.history['val_accuracy'], color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Acurácia')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Acurácia')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lloduydb1udJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "fd617021-e17d-415e-dabe-0105aebc69c4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_2 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 74,602\n",
            "Trainable params: 74,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9NxOzGZ_eG9",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "### ReduceLROnPlateau.\n",
        "\n",
        "<br>\n",
        "\n",
        "Esse retorno de chamada é usado para reduzir a taxa de aprendizado se não houver nenhuma melhoria na perda / métrica.\n",
        "\n",
        "Os argumentos são:\n",
        "\n",
        "* \"monitor\" está definido para essa perda / métrica como uma sequência da qual estamos reduzindo o aprendizado, caso não melhore.\n",
        "\n",
        "* “factor” Você pode passar um número inteiro nesse argumento e dizer que sua taxa de aprendizado atual é LR; se não houver nenhuma melhoria observada na perda / métrica monitorada, o aprendizado diminuirá com esse “fator”. ou seja, nova taxa de aprendizado = fator lr *\n",
        "\n",
        "* \"Verbose\" Você pode definir detalhado = 1 para ver a taxa de aprendizado em todas as épocas. Ou detalhado = 0 para desativá-lo.\n",
        "\n",
        "\n",
        "* O argumento min_delta e mode são os mesmos que os explicados nos argumentos do EarlyStopping Callback.\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPdIe0qjCLK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi3bsJw9CY3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "e5ba5ad6-35af-4647-f6c8-9b4f14fabbea"
      },
      "source": [
        "%%time \n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.1,\n",
        "                              patience=10,\n",
        "                              min_delta=0.001)\n",
        "\n",
        "model = get_model()\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[reduce_lr])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 278s 148ms/step - loss: 0.7454 - accuracy: 0.7210 - val_loss: 0.4958 - val_accuracy: 0.8185 - lr: 0.0100\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 279s 149ms/step - loss: 0.5364 - accuracy: 0.8033 - val_loss: 0.5306 - val_accuracy: 0.7991 - lr: 0.0100\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 279s 149ms/step - loss: 0.5126 - accuracy: 0.8106 - val_loss: 0.4462 - val_accuracy: 0.8295 - lr: 0.0100\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 278s 148ms/step - loss: 0.5053 - accuracy: 0.8153 - val_loss: 0.4605 - val_accuracy: 0.8208 - lr: 0.0100\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 278s 149ms/step - loss: 0.4933 - accuracy: 0.8176 - val_loss: 0.4647 - val_accuracy: 0.8302 - lr: 0.0100\n",
            "CPU times: user 41min 37s, sys: 1min 7s, total: 42min 45s\n",
            "Wall time: 23min 15s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjdo4qaEDoc8",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "### ModelCheckPoint \n",
        "\n",
        "<br> \n",
        "\n",
        "Vamos imaginar que você está treinando um modelo pesado como BERT no colab, e isso exige muito tempo para o treinamento. Então, você começou o treinamento do modelo e foi dormir. E então, na manhã seguinte, você acorda e abre sua colab. Mas você verá a mensagem \"Runtime Disconnect\" na tela. Para esse problema, o ModelCheckpoint vem como um salvador em nossa vida. Podemos salvar os pontos de verificação no final de cada época. Para que possamos carregar os pesos ou retomar o treinamento se algo terrível acontecer durante o treinamento.\n",
        "\n",
        "<br>\n",
        "\n",
        "Então, vamos ver como podemos usar esse retorno de chamada. Podemos salvar o ponto de verificação do modelo no formato Keras h5 / hd5 ou no formato TensorFlow pb. Se você passar o argumento \"filepath = model.h5\" (extensão. H5), ele será salvo no formato Keras ou \"filepath = model.pb\" (extensão .pb) para salvar no formato do modelo TensorFlow.\n",
        "\n",
        "<br>\n",
        "\n",
        "Além disso, existem duas opções para salvar o ponto de verificação: você pode salvar toda a arquitetura + pesos ou apenas os pesos. Você pode fazer isso definindo \"save_only_weights = True\" ou \"save_only_weights = False\"\n",
        "\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB9m-1ZKD105",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cOl8cJgFCAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "5357b091-43db-449e-99b7-e5d89e7ce1b9"
      },
      "source": [
        "model_checkpoint = ModelCheckpoint(filepath='cnn.pb',\n",
        "                                   monitor='val_loss',\n",
        "                                   save_freq='epoch')\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[model_checkpoint])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 2.3042 - accuracy: 0.1000WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: cnn.pb/assets\n",
            "1875/1875 [==============================] - 278s 148ms/step - loss: 2.3042 - accuracy: 0.1000 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 2.3041 - accuracy: 0.0992INFO:tensorflow:Assets written to: cnn.pb/assets\n",
            "1875/1875 [==============================] - 277s 148ms/step - loss: 2.3041 - accuracy: 0.0992 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 2.3038 - accuracy: 0.1001INFO:tensorflow:Assets written to: cnn.pb/assets\n",
            "1875/1875 [==============================] - 272s 145ms/step - loss: 2.3038 - accuracy: 0.1001 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 2.3039 - accuracy: 0.0994INFO:tensorflow:Assets written to: cnn.pb/assets\n",
            "1875/1875 [==============================] - 272s 145ms/step - loss: 2.3039 - accuracy: 0.0994 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 2.3041 - accuracy: 0.0982INFO:tensorflow:Assets written to: cnn.pb/assets\n",
            "1875/1875 [==============================] - 272s 145ms/step - loss: 2.3041 - accuracy: 0.0982 - val_loss: 2.3034 - val_accuracy: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f431e1f10b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk4V0QbHOrJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2223d337-2744-4cc1-f672-88e843eb9152"
      },
      "source": [
        "# salvando modelo \n",
        "model.save('cnn', save_format='tf')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cnn/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZMEwbegFVcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# carregando modelo no disco \n",
        "reconstructed_model = load_model('cnn')"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}